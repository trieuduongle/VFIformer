{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "Wro-793iCP7w",
        "outputId": "9e9689d7-4956-436a-9282-9ab1f675312b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat /content/drive/My\\ Drive/foo.txt"
      ],
      "metadata": {
        "id": "A12QcaVOCZjl",
        "outputId": "d8c68f8a-2135-49b6-f9cb-752b45b95ed0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Google Drive!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/trieuduongle/VFIformer.git '/content/drive/My Drive/Duong/VFIformer/code'"
      ],
      "metadata": {
        "id": "M9Vb0A2-EcvB",
        "outputId": "5c37c39b-40cf-4965-db7a-dedaeef10dfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path '/content/drive/My Drive/Duong/VFIformer/code' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/My Drive/Duong/VFIformer/code'"
      ],
      "metadata": {
        "id": "cO72eFDsHF-3",
        "outputId": "78190a45-bf44-4e9b-9423-4ec8d73d542a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Duong/VFIformer/code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y && sudo apt-get install python3.9"
      ],
      "metadata": {
        "id": "CiLkHhCBKexJ",
        "outputId": "ad690d17-ba8e-49e9-d93e-5132b3abd3a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3.9 is already the newest version (3.9.15-1+bionic1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9* 3\n",
        "!sudo update-alternatives --config python3"
      ],
      "metadata": {
        "id": "_vCsDMK586ib",
        "outputId": "f9acd0be-1fa4-4b70-b81d-a2c3dfbfd1d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 4 choices for the alternative python3 (providing /usr/bin/python3).\n",
            "\n",
            "  Selection    Path                 Priority   Status\n",
            "------------------------------------------------------------\n",
            "  0            /usr/bin/python3.9    3         auto mode\n",
            "* 1            /usr/bin/python3.10   1         manual mode\n",
            "  2            /usr/bin/python3.6    1         manual mode\n",
            "  3            /usr/bin/python3.7    2         manual mode\n",
            "  4            /usr/bin/python3.9    3         manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 0\n",
            "update-alternatives: using /usr/bin/python3.9 to provide /usr/bin/python3 (python3) in auto mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install python3.9-distutils\n",
        "!python -m pip install --upgrade pip"
      ],
      "metadata": {
        "id": "X5-6wEGr9OSn",
        "outputId": "bfa65b58-70c5-431f-bc9b-893681cff13e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3.9-distutils is already the newest version (3.9.15-1+bionic1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n",
            "Collecting pip\n",
            "  Downloading https://files.pythonhosted.org/packages/09/bd/2410905c76ee14c62baf69e3f4aa780226c1bbfc9485731ad018e35b0cb5/pip-22.3.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.1MB 745kB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 9.0.1\n",
            "    Not uninstalling pip at /usr/lib/python3/dist-packages, outside environment /usr\n",
            "Successfully installed pip-22.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1\n",
        "!sudo update-alternatives --config python3\n",
        "!python --version\n",
        "!sudo apt install python3-pip"
      ],
      "metadata": {
        "id": "y9A_FHz3L3Xo",
        "outputId": "128481c6-8369-4f8f-f03e-d3043658745a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 4 choices for the alternative python3 (providing /usr/bin/python3).\n",
            "\n",
            "  Selection    Path                 Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/bin/python3.9    3         auto mode\n",
            "  1            /usr/bin/python3.10   1         manual mode\n",
            "  2            /usr/bin/python3.6    1         manual mode\n",
            "  3            /usr/bin/python3.7    2         manual mode\n",
            "  4            /usr/bin/python3.9    3         manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 0\n",
            "Python 3.9.15\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-pip is already the newest version (9.0.1-2.3~ubuntu1.18.04.5).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout colab && git pull && pip install -r requirements.txt -f https://download.pytorch.org/whl/cu111/torch_stable.html"
      ],
      "metadata": {
        "id": "IbMoodR4I95F",
        "outputId": "a45db7bd-d3bd-4cf4-bbf0-0badcd0d039c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already on 'colab'\n",
            "Your branch is up to date with 'origin/colab'.\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), done.\n",
            "From https://github.com/trieuduongle/VFIformer\n",
            "   cbefb01..12bed71  colab      -> origin/colab\n",
            "Updating cbefb01..12bed71\n",
            "Fast-forward\n",
            " VFIFormer.ipynb | 1289 \u001b[32m+++++++++++++++++++++\u001b[m\u001b[31m----------------------------------\u001b[m\n",
            " 1 file changed, 501 insertions(+), 788 deletions(-)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/cu111/torch_stable.html\n",
            "Collecting matplotlib==3.5.3\n",
            "  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy==1.21.6 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.21.6)\n",
            "Requirement already satisfied: opencv_python==4.6.0.66 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (4.6.0.66)\n",
            "Collecting Pillow==9.2.0\n",
            "  Downloading Pillow-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 77.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.7.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.7.3)\n",
            "Collecting tensorboardX==2.5.1\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 92.1 MB/s \n",
            "\u001b[?25hCollecting timm==0.6.7\n",
            "  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
            "\u001b[K     |████████████████████████████████| 509 kB 88.2 MB/s \n",
            "\u001b[?25hCollecting torch==1.8.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2 MB)\n",
            "\u001b[K     |█████████████▌                  | 834.1 MB 1.3 MB/s eta 0:14:57tcmalloc: large alloc 1147494400 bytes == 0x393e8000 @  0x7f718bed4615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |█████████████████               | 1055.7 MB 1.2 MB/s eta 0:13:11tcmalloc: large alloc 1434370048 bytes == 0x7da3e000 @  0x7f718bed4615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |█████████████████████▋          | 1336.2 MB 120.0 MB/s eta 0:00:06tcmalloc: large alloc 1792966656 bytes == 0x2870000 @  0x7f718bed4615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |███████████████████████████▎    | 1691.1 MB 1.1 MB/s eta 0:04:18tcmalloc: large alloc 2241208320 bytes == 0x6d658000 @  0x7f718bed4615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 1.1 MB/s eta 0:00:01tcmalloc: large alloc 1982177280 bytes == 0xf2fba000 @  0x7f718bed31e7 0x4b2590 0x4b261c 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6\n",
            "tcmalloc: large alloc 2477727744 bytes == 0x169214000 @  0x7f718bed4615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x4bad99 0x4d3249\n",
            "\u001b[K     |████████████████████████████████| 1982.2 MB 3.3 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.9.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (17.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.6 MB 30.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 1)) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 1)) (3.0.9)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 82.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 1)) (0.11.0)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.5.1->-r requirements.txt (line 6)) (3.19.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cu111->-r requirements.txt (line 8)) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.3->-r requirements.txt (line 1)) (1.15.0)\n",
            "Installing collected packages: torch, Pillow, torchvision, fonttools, timm, tensorboardX, matplotlib\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.1+cu113\n",
            "    Uninstalling torchvision-0.13.1+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.1+cu113\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.8.1+cu111 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.8.1+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed Pillow-9.2.0 fonttools-4.38.0 matplotlib-3.5.3 tensorboardX-2.5.1 timm-0.6.7 torch-1.8.1+cu111 torchvision-0.9.1+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -r requirements.txt"
      ],
      "metadata": {
        "id": "syd88yVWQA28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.device_count()\n",
        "torch.cuda.is_available()\n",
        "torch.cuda.current_device()\n",
        "torch.cuda.device(0)\n",
        "torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "id": "xgg3AuNpCryO",
        "outputId": "429ca830-681e-4d16-9c36-5ac84b2bb450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-65426ad33994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mcurrent_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;34mr\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python compute_flow_vimeo.py"
      ],
      "metadata": {
        "id": "7Dgb-CEeSynO",
        "outputId": "7d024c2e-6f39-4d49-a4ec-1e82c6672e2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"compute_flow_vimeo.py\", line 16, in <module>\n",
            "    from .correlation import correlation # the custom cost volume layer\n",
            "ImportError: attempted relative import with no known parent package\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/cupy/__init__.py\", line 18, in <module>\n",
            "    from cupy import _core  # NOQA\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/cupy/_core/__init__.py\", line 3, in <module>\n",
            "    from cupy._core import core  # NOQA\n",
            "  File \"cupy/_core/core.pyx\", line 1, in init cupy._core.core\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/cupy/cuda/__init__.py\", line 8, in <module>\n",
            "    from cupy.cuda import compiler  # NOQA\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/cupy/cuda/compiler.py\", line 13, in <module>\n",
            "    from cupy.cuda import device\n",
            "  File \"cupy/cuda/device.pyx\", line 1, in init cupy.cuda.device\n",
            "ImportError: /usr/local/lib/python3.7/dist-packages/cupy_backends/cuda/api/runtime.cpython-37m-x86_64-linux-gnu.so: symbol cudaMemPoolTrimTo version libcudart.so.11.0 not defined in file libcudart.so.11.0 with link time reference\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"compute_flow_vimeo.py\", line 18, in <module>\n",
            "    sys.path.insert(0, './correlation'); import correlation # you should consider upgrading python\n",
            "  File \"./correlation/correlation.py\", line 3, in <module>\n",
            "    import cupy\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/cupy/__init__.py\", line 27, in <module>\n",
            "    ''') from exc\n",
            "ImportError: \n",
            "================================================================\n",
            "Failed to import CuPy.\n",
            "\n",
            "If you installed CuPy via wheels (cupy-cudaXXX or cupy-rocm-X-X), make sure that the package matches with the version of CUDA or ROCm installed.\n",
            "\n",
            "On Linux, you may need to set LD_LIBRARY_PATH environment variable depending on how you installed CUDA/ROCm.\n",
            "On Windows, try setting CUDA_PATH environment variable.\n",
            "\n",
            "Check the Installation Guide for details:\n",
            "  https://docs.cupy.dev/en/latest/install.html\n",
            "\n",
            "Original error:\n",
            "  ImportError: /usr/local/lib/python3.7/dist-packages/cupy_backends/cuda/api/runtime.cpython-37m-x86_64-linux-gnu.so: symbol cudaMemPoolTrimTo version libcudart.so.11.0 not defined in file libcudart.so.11.0 with link time reference\n",
            "================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch --nproc_per_node=4 --master_port=4174 train.py --launcher pytorch --gpu_ids 0 \\\n",
        "            --loss_flow --use_tb_logger --batch_size 48 --net_name IFNet --name train_IFNet --max_iter 300 --crop_size 192 --save_epoch_freq 5 \\\n",
        "            --data_root \"/content/drive/My Drive/Duong/datasets/vimeo-small\""
      ],
      "metadata": {
        "id": "dWAMQQfeKyzJ",
        "outputId": "c57e9988-750c-4c04-8af4-65cc82c3157c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use_env is set by default in torchrun.\n",
            "If your script expects `--local_rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  FutureWarning,\n",
            "WARNING:torch.distributed.run:\n",
            "*****************************************\n",
            "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "*****************************************\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 16, in <module>\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 16, in <module>\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 16, in <module>\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 16, in <module>\n",
            "        from models import Trainerfrom models import Trainer\n",
            "\n",
            "      File \"/content/drive/My Drive/Duong/VFIformer/code/models/__init__.py\", line 1, in <module>\n",
            "from models import Trainer  File \"/content/drive/My Drive/Duong/VFIformer/code/models/__init__.py\", line 1, in <module>\n",
            "\n",
            "  File \"/content/drive/My Drive/Duong/VFIformer/code/models/__init__.py\", line 1, in <module>\n",
            "    from models import Trainer\n",
            "  File \"/content/drive/My Drive/Duong/VFIformer/code/models/__init__.py\", line 1, in <module>\n",
            "    from .trainer import Trainer\n",
            "    from .trainer import Trainer  File \"/content/drive/My Drive/Duong/VFIformer/code/models/trainer.py\", line 10, in <module>\n",
            "\n",
            "  File \"/content/drive/My Drive/Duong/VFIformer/code/models/trainer.py\", line 10, in <module>\n",
            "    from .trainer import Trainer\n",
            "  File \"/content/drive/My Drive/Duong/VFIformer/code/models/trainer.py\", line 10, in <module>\n",
            "    from .trainer import Trainer\n",
            "  File \"/content/drive/My Drive/Duong/VFIformer/code/models/trainer.py\", line 10, in <module>\n",
            "    from tensorboardX import SummaryWriter    \n",
            "from tensorboardX import SummaryWriter    \n",
            "ModuleNotFoundError    from tensorboardX import SummaryWriter: ModuleNotFoundError\n",
            "from tensorboardX import SummaryWriterNo module named 'tensorboardX': \n",
            "\n",
            "ModuleNotFoundErrorNo module named 'tensorboardX': \n",
            "No module named 'tensorboardX'ModuleNotFoundError\n",
            ": No module named 'tensorboardX'\n",
            "WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 401 closing signal SIGINT\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 402 closing signal SIGINT\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 403 closing signal SIGINT\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 404 closing signal SIGINT\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 193, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 189, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 174, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/run.py\", line 755, in run\n",
            "    )(*cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n",
            "    result = agent.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n",
            "    result = self._invoke_run(role)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 850, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 60, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 45, in __init__\n",
            "    def __init__(self, msg: str, sigval: signal.Signals) -> None:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 60, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 377 got signal: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch --nproc_per_node=4 --master_port=4174 train.py \\\n",
        "  --gpu_ids -1 --loss_flow --use_tb_logger --batch_size 24 --net_name IFNet \\\n",
        "  --name train_IFNet --max_iter 300 --crop_size 192 --save_epoch_freq 5 \\\n",
        "  --data_root \"/content/drive/My Drive/Duong/datasets/vimeo-samples\""
      ],
      "metadata": {
        "id": "f50QnvxmBFxX",
        "outputId": "cef6ad4f-1f39-4211-cf6b-72d148bcb6ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****************************************\n",
            "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "*****************************************\n",
            "Disabled distributed training.\n",
            "2022-11-20 09:51:18,866 [INFO ]  Logging file is ./weights/train_IFNet/20221120_095118.log\n",
            "2022-11-20 09:51:18,867 [INFO ]  random_seed:0\n",
            "2022-11-20 09:51:18,867 [INFO ]  name:train_IFNet\n",
            "2022-11-20 09:51:18,868 [INFO ]  phase:train\n",
            "2022-11-20 09:51:18,868 [INFO ]  gpu_ids:[]\n",
            "2022-11-20 09:51:18,868 [INFO ]  launcher:none\n",
            "2022-11-20 09:51:18,868 [INFO ]  local_rank:1\n",
            "2022-11-20 09:51:18,868 [INFO ]  net_name:IFNet\n",
            "2022-11-20 09:51:18,869 [INFO ]  window_size:8\n",
            "2022-11-20 09:51:18,869 [INFO ]  data_root:/content/drive/My Drive/Duong/datasets/vimeo-samples\n",
            "2022-11-20 09:51:18,869 [INFO ]  trainset:VimeoDataset\n",
            "2022-11-20 09:51:18,869 [INFO ]  testset:VimeoDataset\n",
            "2022-11-20 09:51:18,869 [INFO ]  save_test_root:generated\n",
            "2022-11-20 09:51:18,870 [INFO ]  crop_size:192\n",
            "2022-11-20 09:51:18,870 [INFO ]  batch_size:24\n",
            "2022-11-20 09:51:18,870 [INFO ]  num_workers:4\n",
            "2022-11-20 09:51:18,871 [INFO ]  multi_scale:False\n",
            "2022-11-20 09:51:18,871 [INFO ]  data_augmentation:False\n",
            "2022-11-20 09:51:18,871 [INFO ]  lr:0.0001\n",
            "2022-11-20 09:51:18,871 [INFO ]  lr_D:0.0001\n",
            "2022-11-20 09:51:18,871 [INFO ]  weight_decay:0.0001\n",
            "2022-11-20 09:51:18,872 [INFO ]  start_iter:0\n",
            "2022-11-20 09:51:18,872 [INFO ]  max_iter:300\n",
            "2022-11-20 09:51:18,872 [INFO ]  loss_l1:False\n",
            "2022-11-20 09:51:18,872 [INFO ]  loss_ter:False\n",
            "2022-11-20 09:51:18,873 [INFO ]  loss_flow:True\n",
            "2022-11-20 09:51:18,873 [INFO ]  loss_perceptual:False\n",
            "2022-11-20 09:51:18,873 [INFO ]  loss_adv:False\n",
            "2022-11-20 09:51:18,873 [INFO ]  gan_type:WGAN_GP\n",
            "2022-11-20 09:51:18,874 [INFO ]  lambda_l1:1\n",
            "2022-11-20 09:51:18,874 [INFO ]  lambda_ter:1\n",
            "2022-11-20 09:51:18,874 [INFO ]  lambda_flow:0.01\n",
            "2022-11-20 09:51:18,877 [INFO ]  lambda_perceptual:1\n",
            "2022-11-20 09:51:18,877 [INFO ]  lambda_adv:0.005\n",
            "2022-11-20 09:51:18,877 [INFO ]  resume:\n",
            "2022-11-20 09:51:18,877 [INFO ]  resume_optim:\n",
            "2022-11-20 09:51:18,878 [INFO ]  resume_scheduler:\n",
            "2022-11-20 09:51:18,878 [INFO ]  resume_flownet:\n",
            "2022-11-20 09:51:18,878 [INFO ]  log_freq:10\n",
            "2022-11-20 09:51:18,878 [INFO ]  vis_freq:50000\n",
            "2022-11-20 09:51:18,878 [INFO ]  save_epoch_freq:5\n",
            "2022-11-20 09:51:18,879 [INFO ]  test_freq:100\n",
            "2022-11-20 09:51:18,879 [INFO ]  save_folder:./weights/train_IFNet\n",
            "2022-11-20 09:51:18,879 [INFO ]  vis_step_freq:100\n",
            "2022-11-20 09:51:18,879 [INFO ]  use_tb_logger:True\n",
            "2022-11-20 09:51:18,880 [INFO ]  save_test_results:False\n",
            "2022-11-20 09:51:18,883 [INFO ]  ref_level:1\n",
            "2022-11-20 09:51:18,883 [INFO ]  dist:False\n",
            "2022-11-20 09:51:18,884 [INFO ]  rank:-1\n",
            "2022-11-20 09:51:18,884 [INFO ]  vis_save_dir:./weights/train_IFNet/vis\n",
            "2022-11-20 09:51:18,884 [INFO ]  snapshot_save_dir:./weights/train_IFNet/snapshot\n",
            "Disabled distributed training.\n",
            "2022-11-20 09:51:18,909 [INFO ]  Logging file is ./weights/train_IFNet/20221120_095118.log\n",
            "2022-11-20 09:51:18,910 [INFO ]  random_seed:0\n",
            "2022-11-20 09:51:18,910 [INFO ]  name:train_IFNet\n",
            "2022-11-20 09:51:18,910 [INFO ]  phase:train\n",
            "2022-11-20 09:51:18,910 [INFO ]  gpu_ids:[]\n",
            "2022-11-20 09:51:18,911 [INFO ]  launcher:none\n",
            "2022-11-20 09:51:18,911 [INFO ]  local_rank:2\n",
            "2022-11-20 09:51:18,911 [INFO ]  net_name:IFNet\n",
            "2022-11-20 09:51:18,911 [INFO ]  window_size:8\n",
            "2022-11-20 09:51:18,911 [INFO ]  data_root:/content/drive/My Drive/Duong/datasets/vimeo-samples\n",
            "2022-11-20 09:51:18,912 [INFO ]  trainset:VimeoDataset\n",
            "2022-11-20 09:51:18,912 [INFO ]  testset:VimeoDataset\n",
            "2022-11-20 09:51:18,912 [INFO ]  save_test_root:generated\n",
            "2022-11-20 09:51:18,912 [INFO ]  crop_size:192\n",
            "2022-11-20 09:51:18,912 [INFO ]  batch_size:24\n",
            "2022-11-20 09:51:18,912 [INFO ]  num_workers:4\n",
            "2022-11-20 09:51:18,913 [INFO ]  multi_scale:False\n",
            "2022-11-20 09:51:18,913 [INFO ]  data_augmentation:False\n",
            "2022-11-20 09:51:18,913 [INFO ]  lr:0.0001\n",
            "2022-11-20 09:51:18,913 [INFO ]  lr_D:0.0001\n",
            "2022-11-20 09:51:18,913 [INFO ]  weight_decay:0.0001\n",
            "2022-11-20 09:51:18,914 [INFO ]  start_iter:0\n",
            "2022-11-20 09:51:18,914 [INFO ]  max_iter:300\n",
            "2022-11-20 09:51:18,914 [INFO ]  loss_l1:False\n",
            "2022-11-20 09:51:18,914 [INFO ]  loss_ter:False\n",
            "2022-11-20 09:51:18,914 [INFO ]  loss_flow:True\n",
            "2022-11-20 09:51:18,915 [INFO ]  loss_perceptual:False\n",
            "2022-11-20 09:51:18,915 [INFO ]  loss_adv:False\n",
            "2022-11-20 09:51:18,915 [INFO ]  gan_type:WGAN_GP\n",
            "2022-11-20 09:51:18,915 [INFO ]  lambda_l1:1\n",
            "2022-11-20 09:51:18,915 [INFO ]  lambda_ter:1\n",
            "2022-11-20 09:51:18,916 [INFO ]  lambda_flow:0.01\n",
            "2022-11-20 09:51:18,916 [INFO ]  lambda_perceptual:1\n",
            "2022-11-20 09:51:18,916 [INFO ]  lambda_adv:0.005\n",
            "2022-11-20 09:51:18,916 [INFO ]  resume:\n",
            "2022-11-20 09:51:18,916 [INFO ]  resume_optim:\n",
            "2022-11-20 09:51:18,916 [INFO ]  resume_scheduler:\n",
            "2022-11-20 09:51:18,917 [INFO ]  resume_flownet:\n",
            "2022-11-20 09:51:18,917 [INFO ]  log_freq:10\n",
            "2022-11-20 09:51:18,917 [INFO ]  vis_freq:50000\n",
            "2022-11-20 09:51:18,917 [INFO ]  save_epoch_freq:5\n",
            "2022-11-20 09:51:18,917 [INFO ]  test_freq:100\n",
            "2022-11-20 09:51:18,917 [INFO ]  save_folder:./weights/train_IFNet\n",
            "2022-11-20 09:51:18,918 [INFO ]  vis_step_freq:100\n",
            "2022-11-20 09:51:18,918 [INFO ]  use_tb_logger:True\n",
            "2022-11-20 09:51:18,918 [INFO ]  save_test_results:False\n",
            "2022-11-20 09:51:18,923 [INFO ]  ref_level:1\n",
            "2022-11-20 09:51:18,923 [INFO ]  dist:False\n",
            "2022-11-20 09:51:18,923 [INFO ]  rank:-1\n",
            "2022-11-20 09:51:18,924 [INFO ]  vis_save_dir:./weights/train_IFNet/vis\n",
            "2022-11-20 09:51:18,924 [INFO ]  snapshot_save_dir:./weights/train_IFNet/snapshot\n",
            "Disabled distributed training.\n",
            "2022-11-20 09:51:18,944 [INFO ]  Logging file is ./weights/train_IFNet/20221120_095118.log\n",
            "2022-11-20 09:51:18,944 [INFO ]  random_seed:0\n",
            "2022-11-20 09:51:18,944 [INFO ]  name:train_IFNet\n",
            "2022-11-20 09:51:18,944 [INFO ]  phase:train\n",
            "2022-11-20 09:51:18,945 [INFO ]  gpu_ids:[]\n",
            "2022-11-20 09:51:18,945 [INFO ]  launcher:none\n",
            "2022-11-20 09:51:18,945 [INFO ]  local_rank:3\n",
            "2022-11-20 09:51:18,945 [INFO ]  net_name:IFNet\n",
            "2022-11-20 09:51:18,945 [INFO ]  window_size:8\n",
            "2022-11-20 09:51:18,946 [INFO ]  data_root:/content/drive/My Drive/Duong/datasets/vimeo-samples\n",
            "2022-11-20 09:51:18,946 [INFO ]  trainset:VimeoDataset\n",
            "2022-11-20 09:51:18,946 [INFO ]  testset:VimeoDataset\n",
            "2022-11-20 09:51:18,946 [INFO ]  save_test_root:generated\n",
            "2022-11-20 09:51:18,946 [INFO ]  crop_size:192\n",
            "2022-11-20 09:51:18,946 [INFO ]  batch_size:24\n",
            "2022-11-20 09:51:18,947 [INFO ]  num_workers:4\n",
            "2022-11-20 09:51:18,947 [INFO ]  multi_scale:False\n",
            "2022-11-20 09:51:18,947 [INFO ]  data_augmentation:False\n",
            "2022-11-20 09:51:18,947 [INFO ]  lr:0.0001\n",
            "2022-11-20 09:51:18,947 [INFO ]  lr_D:0.0001\n",
            "2022-11-20 09:51:18,947 [INFO ]  weight_decay:0.0001\n",
            "2022-11-20 09:51:18,948 [INFO ]  start_iter:0\n",
            "2022-11-20 09:51:18,948 [INFO ]  max_iter:300\n",
            "2022-11-20 09:51:18,948 [INFO ]  loss_l1:False\n",
            "2022-11-20 09:51:18,948 [INFO ]  loss_ter:False\n",
            "2022-11-20 09:51:18,948 [INFO ]  loss_flow:True\n",
            "2022-11-20 09:51:18,948 [INFO ]  loss_perceptual:False\n",
            "2022-11-20 09:51:18,948 [INFO ]  loss_adv:False\n",
            "2022-11-20 09:51:18,949 [INFO ]  gan_type:WGAN_GP\n",
            "2022-11-20 09:51:18,949 [INFO ]  lambda_l1:1\n",
            "2022-11-20 09:51:18,949 [INFO ]  lambda_ter:1\n",
            "2022-11-20 09:51:18,949 [INFO ]  lambda_flow:0.01\n",
            "2022-11-20 09:51:18,949 [INFO ]  lambda_perceptual:1\n",
            "2022-11-20 09:51:18,949 [INFO ]  lambda_adv:0.005\n",
            "2022-11-20 09:51:18,950 [INFO ]  resume:\n",
            "2022-11-20 09:51:18,950 [INFO ]  resume_optim:\n",
            "2022-11-20 09:51:18,950 [INFO ]  resume_scheduler:\n",
            "2022-11-20 09:51:18,950 [INFO ]  resume_flownet:\n",
            "2022-11-20 09:51:18,950 [INFO ]  log_freq:10\n",
            "2022-11-20 09:51:18,950 [INFO ]  vis_freq:50000\n",
            "2022-11-20 09:51:18,950 [INFO ]  save_epoch_freq:5\n",
            "2022-11-20 09:51:18,951 [INFO ]  test_freq:100\n",
            "2022-11-20 09:51:18,951 [INFO ]  save_folder:./weights/train_IFNet\n",
            "2022-11-20 09:51:18,951 [INFO ]  vis_step_freq:100\n",
            "2022-11-20 09:51:18,951 [INFO ]  use_tb_logger:True\n",
            "2022-11-20 09:51:18,951 [INFO ]  save_test_results:False\n",
            "2022-11-20 09:51:18,951 [INFO ]  ref_level:1\n",
            "2022-11-20 09:51:18,952 [INFO ]  dist:False\n",
            "2022-11-20 09:51:18,952 [INFO ]  rank:-1\n",
            "2022-11-20 09:51:18,952 [INFO ]  vis_save_dir:./weights/train_IFNet/vis\n",
            "2022-11-20 09:51:18,952 [INFO ]  snapshot_save_dir:./weights/train_IFNet/snapshot\n",
            "Disabled distributed training.\n",
            "2022-11-20 09:51:18,983 [INFO ]  Logging file is ./weights/train_IFNet/20221120_095118.log\n",
            "2022-11-20 09:51:18,984 [INFO ]  random_seed:0\n",
            "2022-11-20 09:51:18,984 [INFO ]  name:train_IFNet\n",
            "2022-11-20 09:51:18,984 [INFO ]  phase:train\n",
            "2022-11-20 09:51:18,984 [INFO ]  gpu_ids:[]\n",
            "2022-11-20 09:51:18,984 [INFO ]  launcher:none\n",
            "2022-11-20 09:51:18,985 [INFO ]  local_rank:0\n",
            "2022-11-20 09:51:18,985 [INFO ]  net_name:IFNet\n",
            "2022-11-20 09:51:18,985 [INFO ]  window_size:8\n",
            "2022-11-20 09:51:18,985 [INFO ]  data_root:/content/drive/My Drive/Duong/datasets/vimeo-samples\n",
            "2022-11-20 09:51:18,986 [INFO ]  trainset:VimeoDataset\n",
            "2022-11-20 09:51:18,986 [INFO ]  testset:VimeoDataset\n",
            "2022-11-20 09:51:18,986 [INFO ]  save_test_root:generated\n",
            "2022-11-20 09:51:18,986 [INFO ]  crop_size:192\n",
            "2022-11-20 09:51:18,986 [INFO ]  batch_size:24\n",
            "2022-11-20 09:51:18,987 [INFO ]  num_workers:4\n",
            "2022-11-20 09:51:18,987 [INFO ]  multi_scale:False\n",
            "2022-11-20 09:51:18,987 [INFO ]  data_augmentation:False\n",
            "2022-11-20 09:51:18,987 [INFO ]  lr:0.0001\n",
            "2022-11-20 09:51:18,987 [INFO ]  lr_D:0.0001\n",
            "2022-11-20 09:51:18,988 [INFO ]  weight_decay:0.0001\n",
            "2022-11-20 09:51:18,988 [INFO ]  start_iter:0\n",
            "2022-11-20 09:51:18,988 [INFO ]  max_iter:300\n",
            "2022-11-20 09:51:18,988 [INFO ]  loss_l1:False\n",
            "2022-11-20 09:51:18,988 [INFO ]  loss_ter:False\n",
            "2022-11-20 09:51:18,989 [INFO ]  loss_flow:True\n",
            "2022-11-20 09:51:18,989 [INFO ]  loss_perceptual:False\n",
            "2022-11-20 09:51:18,989 [INFO ]  loss_adv:False\n",
            "2022-11-20 09:51:18,989 [INFO ]  gan_type:WGAN_GP\n",
            "2022-11-20 09:51:18,989 [INFO ]  lambda_l1:1\n",
            "2022-11-20 09:51:18,990 [INFO ]  lambda_ter:1\n",
            "2022-11-20 09:51:18,990 [INFO ]  lambda_flow:0.01\n",
            "2022-11-20 09:51:18,990 [INFO ]  lambda_perceptual:1\n",
            "2022-11-20 09:51:18,990 [INFO ]  lambda_adv:0.005\n",
            "2022-11-20 09:51:18,990 [INFO ]  resume:\n",
            "2022-11-20 09:51:18,991 [INFO ]  resume_optim:\n",
            "2022-11-20 09:51:18,991 [INFO ]  resume_scheduler:\n",
            "2022-11-20 09:51:18,991 [INFO ]  resume_flownet:\n",
            "2022-11-20 09:51:18,991 [INFO ]  log_freq:10\n",
            "2022-11-20 09:51:18,991 [INFO ]  vis_freq:50000\n",
            "2022-11-20 09:51:18,992 [INFO ]  save_epoch_freq:5\n",
            "2022-11-20 09:51:18,992 [INFO ]  test_freq:100\n",
            "2022-11-20 09:51:18,992 [INFO ]  save_folder:./weights/train_IFNet\n",
            "2022-11-20 09:51:18,992 [INFO ]  vis_step_freq:100\n",
            "2022-11-20 09:51:18,992 [INFO ]  use_tb_logger:True\n",
            "2022-11-20 09:51:18,992 [INFO ]  save_test_results:False\n",
            "2022-11-20 09:51:18,993 [INFO ]  ref_level:1\n",
            "2022-11-20 09:51:18,993 [INFO ]  dist:False\n",
            "2022-11-20 09:51:18,993 [INFO ]  rank:-1\n",
            "2022-11-20 09:51:18,993 [INFO ]  vis_save_dir:./weights/train_IFNet/vis\n",
            "2022-11-20 09:51:18,993 [INFO ]  snapshot_save_dir:./weights/train_IFNet/snapshot\n",
            "Killing subprocess 714\n",
            "Killing subprocess 715\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 340, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 332, in main\n",
            "    time.sleep(1)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 295, in sigkill_handler\n",
            "    print(f\"Killing subprocess {process.pid}\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 295, in sigkill_handler\n",
            "    print(f\"Killing subprocess {process.pid}\")\n",
            "RuntimeError: reentrant call inside <_io.BufferedWriter name='<stdout>'>\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 152, in <module>\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 152, in <module>\n",
            "  File \"train.py\", line 152, in <module>\n",
            "    main()\n",
            "  File \"train.py\", line 148, in main\n",
            "    main()\n",
            "  File \"train.py\", line 148, in main\n",
            "    main()\n",
            "  File \"train.py\", line 148, in main\n",
            "    trainer = Trainer(args)\n",
            "  File \"/content/drive/MyDrive/Duong/VFIformer/code/models/trainer.py\", line 43, in __init__\n",
            "    trainer = Trainer(args)\n",
            "  File \"/content/drive/MyDrive/Duong/VFIformer/code/models/trainer.py\", line 43, in __init__\n",
            "    trainer = Trainer(args)\n",
            "  File \"/content/drive/MyDrive/Duong/VFIformer/code/models/trainer.py\", line 43, in __init__\n",
            "    self.train_dataset = trainset_(self.args)\n",
            "  File \"/content/drive/MyDrive/Duong/VFIformer/code/dataloader/dataset.py\", line 22, in __init__\n",
            "    self.train_dataset = trainset_(self.args)\n",
            "  File \"/content/drive/MyDrive/Duong/VFIformer/code/dataloader/dataset.py\", line 22, in __init__\n",
            "    self.train_dataset = trainset_(self.args)\n",
            "  File \"/content/drive/MyDrive/Duong/VFIformer/code/dataloader/dataset.py\", line 22, in __init__\n",
            "    self.load_data()\n",
            "  File \"/content/drive/MyDrive/Duong/VFIformer/code/dataloader/dataset.py\", line 40, in load_data\n",
            "        self.load_data()\n",
            "self.load_data()  File \"/content/drive/MyDrive/Duong/VFIformer/code/dataloader/dataset.py\", line 40, in load_data\n",
            "\n",
            "  File \"/content/drive/MyDrive/Duong/VFIformer/code/dataloader/dataset.py\", line 40, in load_data\n",
            "    pair = sorted(glob.glob(os.path.join(self.data_root, 'sequences', name, '*.png')))    \n",
            "  File \"/usr/lib/python3.7/glob.py\", line 20, in glob\n",
            "pair = sorted(glob.glob(os.path.join(self.data_root, 'sequences', name, '*.png')))\n",
            "  File \"/usr/lib/python3.7/glob.py\", line 20, in glob\n",
            "    pair = sorted(glob.glob(os.path.join(self.data_root, 'sequences', name, '*.png')))\n",
            "  File \"/usr/lib/python3.7/glob.py\", line 20, in glob\n",
            "        return list(iglob(pathname, recursive=recursive))return list(iglob(pathname, recursive=recursive))\n",
            "\n",
            "  File \"/usr/lib/python3.7/glob.py\", line 72, in _iglob\n",
            "  File \"/usr/lib/python3.7/glob.py\", line 72, in _iglob\n",
            "            return list(iglob(pathname, recursive=recursive))\n",
            "  File \"/usr/lib/python3.7/glob.py\", line 72, in _iglob\n",
            "    for name in glob_in_dir(dirname, basename, dironly):\n",
            "  File \"/usr/lib/python3.7/glob.py\", line 80, in _glob1\n",
            "    for name in glob_in_dir(dirname, basename, dironly):\n",
            "  File \"/usr/lib/python3.7/glob.py\", line 80, in _glob1\n",
            "    for name in glob_in_dir(dirname, basename, dironly):\n",
            "  File \"/usr/lib/python3.7/glob.py\", line 80, in _glob1\n",
            "    names = list(_iterdir(dirname, dironly))\n",
            "  File \"/usr/lib/python3.7/glob.py\", line 121, in _iterdir\n",
            "    names = list(_iterdir(dirname, dironly))names = list(_iterdir(dirname, dironly))\n",
            "  File \"/usr/lib/python3.7/glob.py\", line 121, in _iterdir\n",
            "\n",
            "with os.scandir(dirname) as it:  File \"/usr/lib/python3.7/glob.py\", line 121, in _iterdir\n",
            "\n",
            "    KeyboardInterrupt    \n",
            "with os.scandir(dirname) as it:with os.scandir(dirname) as it:\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch --nproc_per_node=1 --master_port=4175 train.py --launcher pytorch --gpu_ids 0 \\\n",
        "            --loss_l1 --loss_ter --loss_flow --use_tb_logger --batch_size 8 --net_name VFIformer --name train_VFIformerFull --max_iter 300 \\\n",
        "            --crop_size 192 --save_epoch_freq 20 --data_root \"/content/drive/My Drive/Duong/datasets/smallest-2\" \\\n",
        "            --num_workers 2"
      ],
      "metadata": {
        "id": "zEzvnwE0HbE5",
        "outputId": "f377cb25-836d-4b90-fff0-362f2029377f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Logging file is ./weights/train_VFIformerFull/20221120_123346.log\n",
            "2022-11-20 12:33:46,067 [INFO ]  Logging file is ./weights/train_VFIformerFull/20221120_123346.log\n",
            "INFO:root:random_seed:0\n",
            "2022-11-20 12:33:46,068 [INFO ]  random_seed:0\n",
            "INFO:root:name:train_VFIformerFull\n",
            "2022-11-20 12:33:46,068 [INFO ]  name:train_VFIformerFull\n",
            "INFO:root:phase:train\n",
            "2022-11-20 12:33:46,068 [INFO ]  phase:train\n",
            "INFO:root:gpu_ids:[0]\n",
            "2022-11-20 12:33:46,068 [INFO ]  gpu_ids:[0]\n",
            "INFO:root:launcher:pytorch\n",
            "2022-11-20 12:33:46,068 [INFO ]  launcher:pytorch\n",
            "INFO:root:local_rank:0\n",
            "2022-11-20 12:33:46,068 [INFO ]  local_rank:0\n",
            "INFO:root:net_name:VFIformer\n",
            "2022-11-20 12:33:46,068 [INFO ]  net_name:VFIformer\n",
            "INFO:root:window_size:8\n",
            "2022-11-20 12:33:46,068 [INFO ]  window_size:8\n",
            "INFO:root:data_root:/content/drive/My Drive/Duong/datasets/smallest-2\n",
            "2022-11-20 12:33:46,068 [INFO ]  data_root:/content/drive/My Drive/Duong/datasets/smallest-2\n",
            "INFO:root:trainset:VimeoDataset\n",
            "2022-11-20 12:33:46,069 [INFO ]  trainset:VimeoDataset\n",
            "INFO:root:testset:VimeoDataset\n",
            "2022-11-20 12:33:46,069 [INFO ]  testset:VimeoDataset\n",
            "INFO:root:save_test_root:generated\n",
            "2022-11-20 12:33:46,069 [INFO ]  save_test_root:generated\n",
            "INFO:root:crop_size:192\n",
            "2022-11-20 12:33:46,069 [INFO ]  crop_size:192\n",
            "INFO:root:batch_size:8\n",
            "2022-11-20 12:33:46,069 [INFO ]  batch_size:8\n",
            "INFO:root:num_workers:2\n",
            "2022-11-20 12:33:46,069 [INFO ]  num_workers:2\n",
            "INFO:root:multi_scale:False\n",
            "2022-11-20 12:33:46,069 [INFO ]  multi_scale:False\n",
            "INFO:root:data_augmentation:False\n",
            "2022-11-20 12:33:46,069 [INFO ]  data_augmentation:False\n",
            "INFO:root:lr:0.0001\n",
            "2022-11-20 12:33:46,069 [INFO ]  lr:0.0001\n",
            "INFO:root:lr_D:0.0001\n",
            "2022-11-20 12:33:46,069 [INFO ]  lr_D:0.0001\n",
            "INFO:root:weight_decay:0.0001\n",
            "2022-11-20 12:33:46,070 [INFO ]  weight_decay:0.0001\n",
            "INFO:root:start_iter:0\n",
            "2022-11-20 12:33:46,070 [INFO ]  start_iter:0\n",
            "INFO:root:max_iter:300\n",
            "2022-11-20 12:33:46,070 [INFO ]  max_iter:300\n",
            "INFO:root:loss_l1:True\n",
            "2022-11-20 12:33:46,070 [INFO ]  loss_l1:True\n",
            "INFO:root:loss_ter:True\n",
            "2022-11-20 12:33:46,070 [INFO ]  loss_ter:True\n",
            "INFO:root:loss_flow:True\n",
            "2022-11-20 12:33:46,070 [INFO ]  loss_flow:True\n",
            "INFO:root:loss_perceptual:False\n",
            "2022-11-20 12:33:46,070 [INFO ]  loss_perceptual:False\n",
            "INFO:root:loss_adv:False\n",
            "2022-11-20 12:33:46,070 [INFO ]  loss_adv:False\n",
            "INFO:root:gan_type:WGAN_GP\n",
            "2022-11-20 12:33:46,070 [INFO ]  gan_type:WGAN_GP\n",
            "INFO:root:lambda_l1:1\n",
            "2022-11-20 12:33:46,070 [INFO ]  lambda_l1:1\n",
            "INFO:root:lambda_ter:1\n",
            "2022-11-20 12:33:46,071 [INFO ]  lambda_ter:1\n",
            "INFO:root:lambda_flow:0.01\n",
            "2022-11-20 12:33:46,071 [INFO ]  lambda_flow:0.01\n",
            "INFO:root:lambda_perceptual:1\n",
            "2022-11-20 12:33:46,071 [INFO ]  lambda_perceptual:1\n",
            "INFO:root:lambda_adv:0.005\n",
            "2022-11-20 12:33:46,071 [INFO ]  lambda_adv:0.005\n",
            "INFO:root:resume:\n",
            "2022-11-20 12:33:46,071 [INFO ]  resume:\n",
            "INFO:root:resume_optim:\n",
            "2022-11-20 12:33:46,071 [INFO ]  resume_optim:\n",
            "INFO:root:resume_scheduler:\n",
            "2022-11-20 12:33:46,071 [INFO ]  resume_scheduler:\n",
            "INFO:root:resume_flownet:\n",
            "2022-11-20 12:33:46,071 [INFO ]  resume_flownet:\n",
            "INFO:root:log_freq:10\n",
            "2022-11-20 12:33:46,071 [INFO ]  log_freq:10\n",
            "INFO:root:vis_freq:50000\n",
            "2022-11-20 12:33:46,071 [INFO ]  vis_freq:50000\n",
            "INFO:root:save_epoch_freq:20\n",
            "2022-11-20 12:33:46,071 [INFO ]  save_epoch_freq:20\n",
            "INFO:root:test_freq:100\n",
            "2022-11-20 12:33:46,072 [INFO ]  test_freq:100\n",
            "INFO:root:save_folder:./weights/train_VFIformerFull\n",
            "2022-11-20 12:33:46,072 [INFO ]  save_folder:./weights/train_VFIformerFull\n",
            "INFO:root:vis_step_freq:100\n",
            "2022-11-20 12:33:46,072 [INFO ]  vis_step_freq:100\n",
            "INFO:root:use_tb_logger:True\n",
            "2022-11-20 12:33:46,072 [INFO ]  use_tb_logger:True\n",
            "INFO:root:save_test_results:False\n",
            "2022-11-20 12:33:46,072 [INFO ]  save_test_results:False\n",
            "INFO:root:ref_level:1\n",
            "2022-11-20 12:33:46,072 [INFO ]  ref_level:1\n",
            "INFO:root:dist:True\n",
            "2022-11-20 12:33:46,072 [INFO ]  dist:True\n",
            "INFO:root:world_size:1\n",
            "2022-11-20 12:33:46,072 [INFO ]  world_size:1\n",
            "INFO:root:rank:0\n",
            "2022-11-20 12:33:46,072 [INFO ]  rank:0\n",
            "INFO:root:vis_save_dir:./weights/train_VFIformerFull/vis\n",
            "2022-11-20 12:33:46,072 [INFO ]  vis_save_dir:./weights/train_VFIformerFull/vis\n",
            "INFO:root:snapshot_save_dir:./weights/train_VFIformerFull/snapshot\n",
            "2022-11-20 12:33:46,073 [INFO ]  snapshot_save_dir:./weights/train_VFIformerFull/snapshot\n",
            "INFO:root:----- generator parameters: 24.166930 -----\n",
            "2022-11-20 12:33:50,360 [INFO ]  ----- generator parameters: 24.166930 -----\n",
            "INFO:root:init criterion and optimizer...\n",
            "2022-11-20 12:33:50,360 [INFO ]  init criterion and optimizer...\n",
            "INFO:root:  using l1 loss...\n",
            "2022-11-20 12:33:50,362 [INFO ]    using l1 loss...\n",
            "INFO:root:  using flow loss...\n",
            "2022-11-20 12:33:50,363 [INFO ]    using flow loss...\n",
            "INFO:root:  using ter loss...\n",
            "2022-11-20 12:33:50,363 [INFO ]    using ter loss...\n",
            "INFO:root:training on  ...VimeoDataset\n",
            "2022-11-20 12:33:50,363 [INFO ]  training on  ...VimeoDataset\n",
            "INFO:root:80 training samples\n",
            "2022-11-20 12:33:50,363 [INFO ]  80 training samples\n",
            "INFO:root:the init lr: 0.000100\n",
            "2022-11-20 12:33:50,364 [INFO ]  the init lr: 0.000100\n",
            "INFO:root:epoch:000 step:0000  current_lr: 0.000000  l1_loss:0.042022 flow_loss:0.039800 ter_loss:0.025174 loss_sum:0.106996 0.567739s/batch\n",
            "2022-11-20 12:33:56,093 [INFO ]  epoch:000 step:0000  current_lr: 0.000000  l1_loss:0.042022 flow_loss:0.039800 ter_loss:0.025174 loss_sum:0.106996 0.567739s/batch\n",
            "INFO:root:Reducer buckets have been rebuilt in this iteration.\n",
            "2022-11-20 12:33:56,144 [INFO ]  Reducer buckets have been rebuilt in this iteration.\n",
            "INFO:root:Saving state, epoch: 0 iter:0\n",
            "2022-11-20 12:34:06,154 [INFO ]  Saving state, epoch: 0 iter:0\n",
            "INFO:root:epoch:001 step:0000  current_lr: 0.000001  l1_loss:0.040879 flow_loss:0.032022 ter_loss:0.026169 loss_sum:0.099070 0.281658s/batch\n",
            "2022-11-20 12:34:10,209 [INFO ]  epoch:001 step:0000  current_lr: 0.000001  l1_loss:0.040879 flow_loss:0.032022 ter_loss:0.026169 loss_sum:0.099070 0.281658s/batch\n",
            "INFO:root:epoch:002 step:0000  current_lr: 0.000001  l1_loss:0.036054 flow_loss:0.033323 ter_loss:0.027906 loss_sum:0.097283 0.280040s/batch\n",
            "2022-11-20 12:34:24,033 [INFO ]  epoch:002 step:0000  current_lr: 0.000001  l1_loss:0.036054 flow_loss:0.033323 ter_loss:0.027906 loss_sum:0.097283 0.280040s/batch\n",
            "INFO:root:epoch:003 step:0000  current_lr: 0.000002  l1_loss:0.028138 flow_loss:0.036328 ter_loss:0.026159 loss_sum:0.090626 0.279230s/batch\n",
            "2022-11-20 12:34:36,774 [INFO ]  epoch:003 step:0000  current_lr: 0.000002  l1_loss:0.028138 flow_loss:0.036328 ter_loss:0.026159 loss_sum:0.090626 0.279230s/batch\n",
            "INFO:root:epoch:004 step:0000  current_lr: 0.000002  l1_loss:0.027220 flow_loss:0.038278 ter_loss:0.026309 loss_sum:0.091807 0.282727s/batch\n",
            "2022-11-20 12:34:49,557 [INFO ]  epoch:004 step:0000  current_lr: 0.000002  l1_loss:0.027220 flow_loss:0.038278 ter_loss:0.026309 loss_sum:0.091807 0.282727s/batch\n",
            "INFO:root:epoch:005 step:0000  current_lr: 0.000003  l1_loss:0.026023 flow_loss:0.032983 ter_loss:0.029046 loss_sum:0.088051 0.281762s/batch\n",
            "2022-11-20 12:35:02,368 [INFO ]  epoch:005 step:0000  current_lr: 0.000003  l1_loss:0.026023 flow_loss:0.032983 ter_loss:0.029046 loss_sum:0.088051 0.281762s/batch\n",
            "INFO:root:epoch:006 step:0000  current_lr: 0.000003  l1_loss:0.024937 flow_loss:0.037059 ter_loss:0.026103 loss_sum:0.088099 0.284474s/batch\n",
            "2022-11-20 12:35:15,176 [INFO ]  epoch:006 step:0000  current_lr: 0.000003  l1_loss:0.024937 flow_loss:0.037059 ter_loss:0.026103 loss_sum:0.088099 0.284474s/batch\n",
            "INFO:root:epoch:007 step:0000  current_lr: 0.000004  l1_loss:0.021407 flow_loss:0.029369 ter_loss:0.022703 loss_sum:0.073479 0.285787s/batch\n",
            "2022-11-20 12:35:27,995 [INFO ]  epoch:007 step:0000  current_lr: 0.000004  l1_loss:0.021407 flow_loss:0.029369 ter_loss:0.022703 loss_sum:0.073479 0.285787s/batch\n",
            "INFO:root:epoch:008 step:0000  current_lr: 0.000004  l1_loss:0.020837 flow_loss:0.035503 ter_loss:0.022108 loss_sum:0.078449 0.284520s/batch\n",
            "2022-11-20 12:35:40,820 [INFO ]  epoch:008 step:0000  current_lr: 0.000004  l1_loss:0.020837 flow_loss:0.035503 ter_loss:0.022108 loss_sum:0.078449 0.284520s/batch\n",
            "INFO:root:epoch:009 step:0000  current_lr: 0.000005  l1_loss:0.022867 flow_loss:0.031957 ter_loss:0.024470 loss_sum:0.079293 0.283874s/batch\n",
            "2022-11-20 12:35:53,629 [INFO ]  epoch:009 step:0000  current_lr: 0.000005  l1_loss:0.022867 flow_loss:0.031957 ter_loss:0.024470 loss_sum:0.079293 0.283874s/batch\n",
            "INFO:root:epoch:010 step:0000  current_lr: 0.000005  l1_loss:0.022194 flow_loss:0.035556 ter_loss:0.022872 loss_sum:0.080621 0.280953s/batch\n",
            "2022-11-20 12:36:06,412 [INFO ]  epoch:010 step:0000  current_lr: 0.000005  l1_loss:0.022194 flow_loss:0.035556 ter_loss:0.022872 loss_sum:0.080621 0.280953s/batch\n",
            "INFO:root:epoch:011 step:0000  current_lr: 0.000006  l1_loss:0.020429 flow_loss:0.041393 ter_loss:0.021563 loss_sum:0.083384 0.286402s/batch\n",
            "2022-11-20 12:36:19,290 [INFO ]  epoch:011 step:0000  current_lr: 0.000006  l1_loss:0.020429 flow_loss:0.041393 ter_loss:0.021563 loss_sum:0.083384 0.286402s/batch\n",
            "INFO:root:epoch:012 step:0000  current_lr: 0.000006  l1_loss:0.026738 flow_loss:0.038400 ter_loss:0.029241 loss_sum:0.094378 0.284717s/batch\n",
            "2022-11-20 12:36:32,102 [INFO ]  epoch:012 step:0000  current_lr: 0.000006  l1_loss:0.026738 flow_loss:0.038400 ter_loss:0.029241 loss_sum:0.094378 0.284717s/batch\n",
            "INFO:root:epoch:013 step:0000  current_lr: 0.000007  l1_loss:0.024130 flow_loss:0.036358 ter_loss:0.027160 loss_sum:0.087648 0.281416s/batch\n",
            "2022-11-20 12:36:44,884 [INFO ]  epoch:013 step:0000  current_lr: 0.000007  l1_loss:0.024130 flow_loss:0.036358 ter_loss:0.027160 loss_sum:0.087648 0.281416s/batch\n",
            "INFO:root:epoch:014 step:0000  current_lr: 0.000007  l1_loss:0.023467 flow_loss:0.032419 ter_loss:0.026241 loss_sum:0.082127 0.281209s/batch\n",
            "2022-11-20 12:36:57,666 [INFO ]  epoch:014 step:0000  current_lr: 0.000007  l1_loss:0.023467 flow_loss:0.032419 ter_loss:0.026241 loss_sum:0.082127 0.281209s/batch\n",
            "INFO:root:epoch:015 step:0000  current_lr: 0.000008  l1_loss:0.023645 flow_loss:0.034807 ter_loss:0.026693 loss_sum:0.085145 0.282573s/batch\n",
            "2022-11-20 12:37:10,476 [INFO ]  epoch:015 step:0000  current_lr: 0.000008  l1_loss:0.023645 flow_loss:0.034807 ter_loss:0.026693 loss_sum:0.085145 0.282573s/batch\n",
            "INFO:root:epoch:016 step:0000  current_lr: 0.000008  l1_loss:0.020022 flow_loss:0.032601 ter_loss:0.021838 loss_sum:0.074461 0.288964s/batch\n",
            "2022-11-20 12:37:23,342 [INFO ]  epoch:016 step:0000  current_lr: 0.000008  l1_loss:0.020022 flow_loss:0.032601 ter_loss:0.021838 loss_sum:0.074461 0.288964s/batch\n",
            "INFO:root:epoch:017 step:0000  current_lr: 0.000009  l1_loss:0.024241 flow_loss:0.030968 ter_loss:0.027878 loss_sum:0.083088 0.277746s/batch\n",
            "2022-11-20 12:37:36,091 [INFO ]  epoch:017 step:0000  current_lr: 0.000009  l1_loss:0.024241 flow_loss:0.030968 ter_loss:0.027878 loss_sum:0.083088 0.277746s/batch\n",
            "INFO:root:epoch:018 step:0000  current_lr: 0.000009  l1_loss:0.021610 flow_loss:0.042496 ter_loss:0.023323 loss_sum:0.087429 0.281149s/batch\n",
            "2022-11-20 12:37:48,921 [INFO ]  epoch:018 step:0000  current_lr: 0.000009  l1_loss:0.021610 flow_loss:0.042496 ter_loss:0.023323 loss_sum:0.087429 0.281149s/batch\n",
            "INFO:root:epoch:019 step:0000  current_lr: 0.000010  l1_loss:0.023542 flow_loss:0.033471 ter_loss:0.025534 loss_sum:0.082547 0.284284s/batch\n",
            "2022-11-20 12:38:01,757 [INFO ]  epoch:019 step:0000  current_lr: 0.000010  l1_loss:0.023542 flow_loss:0.033471 ter_loss:0.025534 loss_sum:0.082547 0.284284s/batch\n",
            "INFO:root:epoch:020 step:0000  current_lr: 0.000010  l1_loss:0.020030 flow_loss:0.034641 ter_loss:0.022019 loss_sum:0.076690 0.281006s/batch\n",
            "2022-11-20 12:38:14,546 [INFO ]  epoch:020 step:0000  current_lr: 0.000010  l1_loss:0.020030 flow_loss:0.034641 ter_loss:0.022019 loss_sum:0.076690 0.281006s/batch\n",
            "INFO:root:Saving state, epoch: 20 iter:0\n",
            "2022-11-20 12:38:24,525 [INFO ]  Saving state, epoch: 20 iter:0\n",
            "INFO:root:epoch:021 step:0000  current_lr: 0.000010  l1_loss:0.020140 flow_loss:0.032110 ter_loss:0.022524 loss_sum:0.074775 0.280248s/batch\n",
            "2022-11-20 12:38:28,434 [INFO ]  epoch:021 step:0000  current_lr: 0.000010  l1_loss:0.020140 flow_loss:0.032110 ter_loss:0.022524 loss_sum:0.074775 0.280248s/batch\n",
            "INFO:root:epoch:022 step:0000  current_lr: 0.000011  l1_loss:0.024297 flow_loss:0.036354 ter_loss:0.027061 loss_sum:0.087712 0.279829s/batch\n",
            "2022-11-20 12:38:41,300 [INFO ]  epoch:022 step:0000  current_lr: 0.000011  l1_loss:0.024297 flow_loss:0.036354 ter_loss:0.027061 loss_sum:0.087712 0.279829s/batch\n",
            "INFO:root:epoch:023 step:0000  current_lr: 0.000012  l1_loss:0.024962 flow_loss:0.038245 ter_loss:0.026835 loss_sum:0.090042 0.281548s/batch\n",
            "2022-11-20 12:38:54,100 [INFO ]  epoch:023 step:0000  current_lr: 0.000012  l1_loss:0.024962 flow_loss:0.038245 ter_loss:0.026835 loss_sum:0.090042 0.281548s/batch\n",
            "INFO:root:epoch:024 step:0000  current_lr: 0.000012  l1_loss:0.021837 flow_loss:0.034044 ter_loss:0.023460 loss_sum:0.079342 0.281441s/batch\n",
            "2022-11-20 12:39:06,897 [INFO ]  epoch:024 step:0000  current_lr: 0.000012  l1_loss:0.021837 flow_loss:0.034044 ter_loss:0.023460 loss_sum:0.079342 0.281441s/batch\n",
            "INFO:root:epoch:025 step:0000  current_lr: 0.000013  l1_loss:0.022747 flow_loss:0.030882 ter_loss:0.023477 loss_sum:0.077106 0.288112s/batch\n",
            "2022-11-20 12:39:19,760 [INFO ]  epoch:025 step:0000  current_lr: 0.000013  l1_loss:0.022747 flow_loss:0.030882 ter_loss:0.023477 loss_sum:0.077106 0.288112s/batch\n",
            "INFO:root:epoch:026 step:0000  current_lr: 0.000013  l1_loss:0.022776 flow_loss:0.031574 ter_loss:0.025287 loss_sum:0.079637 0.284557s/batch\n",
            "2022-11-20 12:39:32,583 [INFO ]  epoch:026 step:0000  current_lr: 0.000013  l1_loss:0.022776 flow_loss:0.031574 ter_loss:0.025287 loss_sum:0.079637 0.284557s/batch\n",
            "INFO:root:epoch:027 step:0000  current_lr: 0.000014  l1_loss:0.023330 flow_loss:0.032163 ter_loss:0.024221 loss_sum:0.079714 0.279326s/batch\n",
            "2022-11-20 12:39:45,348 [INFO ]  epoch:027 step:0000  current_lr: 0.000014  l1_loss:0.023330 flow_loss:0.032163 ter_loss:0.024221 loss_sum:0.079714 0.279326s/batch\n",
            "INFO:root:epoch:028 step:0000  current_lr: 0.000014  l1_loss:0.024314 flow_loss:0.028556 ter_loss:0.026913 loss_sum:0.079783 0.281443s/batch\n",
            "2022-11-20 12:39:58,151 [INFO ]  epoch:028 step:0000  current_lr: 0.000014  l1_loss:0.024314 flow_loss:0.028556 ter_loss:0.026913 loss_sum:0.079783 0.281443s/batch\n",
            "INFO:root:epoch:029 step:0000  current_lr: 0.000015  l1_loss:0.023892 flow_loss:0.030604 ter_loss:0.024529 loss_sum:0.079024 0.282224s/batch\n",
            "2022-11-20 12:40:10,939 [INFO ]  epoch:029 step:0000  current_lr: 0.000015  l1_loss:0.023892 flow_loss:0.030604 ter_loss:0.024529 loss_sum:0.079024 0.282224s/batch\n",
            "INFO:root:epoch:030 step:0000  current_lr: 0.000015  l1_loss:0.024050 flow_loss:0.033901 ter_loss:0.025083 loss_sum:0.083034 0.279729s/batch\n",
            "2022-11-20 12:40:23,717 [INFO ]  epoch:030 step:0000  current_lr: 0.000015  l1_loss:0.024050 flow_loss:0.033901 ter_loss:0.025083 loss_sum:0.083034 0.279729s/batch\n",
            "INFO:root:epoch:031 step:0000  current_lr: 0.000016  l1_loss:0.023541 flow_loss:0.034017 ter_loss:0.024839 loss_sum:0.082396 0.281201s/batch\n",
            "2022-11-20 12:40:36,518 [INFO ]  epoch:031 step:0000  current_lr: 0.000016  l1_loss:0.023541 flow_loss:0.034017 ter_loss:0.024839 loss_sum:0.082396 0.281201s/batch\n",
            "INFO:root:epoch:032 step:0000  current_lr: 0.000016  l1_loss:0.026053 flow_loss:0.031929 ter_loss:0.027833 loss_sum:0.085814 0.281077s/batch\n",
            "2022-11-20 12:40:49,295 [INFO ]  epoch:032 step:0000  current_lr: 0.000016  l1_loss:0.026053 flow_loss:0.031929 ter_loss:0.027833 loss_sum:0.085814 0.281077s/batch\n",
            "INFO:root:epoch:033 step:0000  current_lr: 0.000017  l1_loss:0.024539 flow_loss:0.041193 ter_loss:0.024337 loss_sum:0.090070 0.282688s/batch\n",
            "2022-11-20 12:41:02,109 [INFO ]  epoch:033 step:0000  current_lr: 0.000017  l1_loss:0.024539 flow_loss:0.041193 ter_loss:0.024337 loss_sum:0.090070 0.282688s/batch\n",
            "INFO:root:epoch:034 step:0000  current_lr: 0.000017  l1_loss:0.024402 flow_loss:0.040510 ter_loss:0.023578 loss_sum:0.088491 0.283384s/batch\n",
            "2022-11-20 12:41:14,927 [INFO ]  epoch:034 step:0000  current_lr: 0.000017  l1_loss:0.024402 flow_loss:0.040510 ter_loss:0.023578 loss_sum:0.088491 0.283384s/batch\n",
            "INFO:root:epoch:035 step:0000  current_lr: 0.000017  l1_loss:0.024032 flow_loss:0.032860 ter_loss:0.025040 loss_sum:0.081932 0.284409s/batch\n",
            "2022-11-20 12:41:27,783 [INFO ]  epoch:035 step:0000  current_lr: 0.000017  l1_loss:0.024032 flow_loss:0.032860 ter_loss:0.025040 loss_sum:0.081932 0.284409s/batch\n",
            "INFO:root:epoch:036 step:0000  current_lr: 0.000018  l1_loss:0.023184 flow_loss:0.037864 ter_loss:0.022845 loss_sum:0.083893 0.278239s/batch\n",
            "2022-11-20 12:41:40,542 [INFO ]  epoch:036 step:0000  current_lr: 0.000018  l1_loss:0.023184 flow_loss:0.037864 ter_loss:0.022845 loss_sum:0.083893 0.278239s/batch\n",
            "INFO:root:epoch:037 step:0000  current_lr: 0.000018  l1_loss:0.023080 flow_loss:0.032857 ter_loss:0.023431 loss_sum:0.079367 0.280574s/batch\n",
            "2022-11-20 12:41:53,331 [INFO ]  epoch:037 step:0000  current_lr: 0.000018  l1_loss:0.023080 flow_loss:0.032857 ter_loss:0.023431 loss_sum:0.079367 0.280574s/batch\n",
            "INFO:root:epoch:038 step:0000  current_lr: 0.000019  l1_loss:0.024114 flow_loss:0.036891 ter_loss:0.024857 loss_sum:0.085861 0.280806s/batch\n",
            "2022-11-20 12:42:06,110 [INFO ]  epoch:038 step:0000  current_lr: 0.000019  l1_loss:0.024114 flow_loss:0.036891 ter_loss:0.024857 loss_sum:0.085861 0.280806s/batch\n",
            "INFO:root:epoch:039 step:0000  current_lr: 0.000020  l1_loss:0.023007 flow_loss:0.032691 ter_loss:0.022928 loss_sum:0.078626 0.281914s/batch\n",
            "2022-11-20 12:42:18,901 [INFO ]  epoch:039 step:0000  current_lr: 0.000020  l1_loss:0.023007 flow_loss:0.032691 ter_loss:0.022928 loss_sum:0.078626 0.281914s/batch\n",
            "INFO:root:epoch:040 step:0000  current_lr: 0.000020  l1_loss:0.023031 flow_loss:0.032501 ter_loss:0.023346 loss_sum:0.078877 0.286273s/batch\n",
            "2022-11-20 12:42:31,743 [INFO ]  epoch:040 step:0000  current_lr: 0.000020  l1_loss:0.023031 flow_loss:0.032501 ter_loss:0.023346 loss_sum:0.078877 0.286273s/batch\n",
            "INFO:root:Saving state, epoch: 40 iter:0\n",
            "2022-11-20 12:42:41,721 [INFO ]  Saving state, epoch: 40 iter:0\n",
            "INFO:root:epoch:041 step:0000  current_lr: 0.000021  l1_loss:0.021722 flow_loss:0.032087 ter_loss:0.021773 loss_sum:0.075583 0.279522s/batch\n",
            "2022-11-20 12:42:45,643 [INFO ]  epoch:041 step:0000  current_lr: 0.000021  l1_loss:0.021722 flow_loss:0.032087 ter_loss:0.021773 loss_sum:0.075583 0.279522s/batch\n",
            "INFO:root:epoch:042 step:0000  current_lr: 0.000021  l1_loss:0.023515 flow_loss:0.025676 ter_loss:0.023810 loss_sum:0.073001 0.289019s/batch\n",
            "2022-11-20 12:42:58,532 [INFO ]  epoch:042 step:0000  current_lr: 0.000021  l1_loss:0.023515 flow_loss:0.025676 ter_loss:0.023810 loss_sum:0.073001 0.289019s/batch\n",
            "INFO:root:epoch:043 step:0000  current_lr: 0.000022  l1_loss:0.021879 flow_loss:0.037787 ter_loss:0.022000 loss_sum:0.081666 0.281934s/batch\n",
            "2022-11-20 12:43:11,333 [INFO ]  epoch:043 step:0000  current_lr: 0.000022  l1_loss:0.021879 flow_loss:0.037787 ter_loss:0.022000 loss_sum:0.081666 0.281934s/batch\n",
            "INFO:root:epoch:044 step:0000  current_lr: 0.000022  l1_loss:0.025162 flow_loss:0.041239 ter_loss:0.023499 loss_sum:0.089900 0.281205s/batch\n",
            "2022-11-20 12:43:24,110 [INFO ]  epoch:044 step:0000  current_lr: 0.000022  l1_loss:0.025162 flow_loss:0.041239 ter_loss:0.023499 loss_sum:0.089900 0.281205s/batch\n",
            "INFO:root:epoch:045 step:0000  current_lr: 0.000023  l1_loss:0.023636 flow_loss:0.032409 ter_loss:0.023064 loss_sum:0.079108 0.284476s/batch\n",
            "2022-11-20 12:43:36,949 [INFO ]  epoch:045 step:0000  current_lr: 0.000023  l1_loss:0.023636 flow_loss:0.032409 ter_loss:0.023064 loss_sum:0.079108 0.284476s/batch\n",
            "INFO:root:epoch:046 step:0000  current_lr: 0.000023  l1_loss:0.020872 flow_loss:0.043876 ter_loss:0.019642 loss_sum:0.084390 0.283024s/batch\n",
            "2022-11-20 12:43:49,742 [INFO ]  epoch:046 step:0000  current_lr: 0.000023  l1_loss:0.020872 flow_loss:0.043876 ter_loss:0.019642 loss_sum:0.084390 0.283024s/batch\n",
            "INFO:root:epoch:047 step:0000  current_lr: 0.000023  l1_loss:0.021982 flow_loss:0.034793 ter_loss:0.021090 loss_sum:0.077865 0.280690s/batch\n",
            "2022-11-20 12:44:02,518 [INFO ]  epoch:047 step:0000  current_lr: 0.000023  l1_loss:0.021982 flow_loss:0.034793 ter_loss:0.021090 loss_sum:0.077865 0.280690s/batch\n",
            "INFO:root:epoch:048 step:0000  current_lr: 0.000024  l1_loss:0.019885 flow_loss:0.041498 ter_loss:0.018590 loss_sum:0.079973 0.282388s/batch\n",
            "2022-11-20 12:44:15,302 [INFO ]  epoch:048 step:0000  current_lr: 0.000024  l1_loss:0.019885 flow_loss:0.041498 ter_loss:0.018590 loss_sum:0.079973 0.282388s/batch\n",
            "INFO:root:epoch:049 step:0000  current_lr: 0.000024  l1_loss:0.021306 flow_loss:0.041317 ter_loss:0.020482 loss_sum:0.083105 0.280946s/batch\n",
            "2022-11-20 12:44:28,069 [INFO ]  epoch:049 step:0000  current_lr: 0.000024  l1_loss:0.021306 flow_loss:0.041317 ter_loss:0.020482 loss_sum:0.083105 0.280946s/batch\n",
            "INFO:root:epoch:050 step:0000  current_lr: 0.000025  l1_loss:0.022143 flow_loss:0.040524 ter_loss:0.019485 loss_sum:0.082152 0.280675s/batch\n",
            "2022-11-20 12:44:40,841 [INFO ]  epoch:050 step:0000  current_lr: 0.000025  l1_loss:0.022143 flow_loss:0.040524 ter_loss:0.019485 loss_sum:0.082152 0.280675s/batch\n",
            "INFO:root:epoch:051 step:0000  current_lr: 0.000026  l1_loss:0.021988 flow_loss:0.036769 ter_loss:0.020781 loss_sum:0.079538 0.286768s/batch\n",
            "2022-11-20 12:44:53,680 [INFO ]  epoch:051 step:0000  current_lr: 0.000026  l1_loss:0.021988 flow_loss:0.036769 ter_loss:0.020781 loss_sum:0.079538 0.286768s/batch\n",
            "INFO:root:epoch:052 step:0000  current_lr: 0.000026  l1_loss:0.021615 flow_loss:0.038823 ter_loss:0.020362 loss_sum:0.080800 0.282662s/batch\n",
            "2022-11-20 12:45:06,487 [INFO ]  epoch:052 step:0000  current_lr: 0.000026  l1_loss:0.021615 flow_loss:0.038823 ter_loss:0.020362 loss_sum:0.080800 0.282662s/batch\n",
            "INFO:root:epoch:053 step:0000  current_lr: 0.000027  l1_loss:0.022114 flow_loss:0.039081 ter_loss:0.019580 loss_sum:0.080775 0.284007s/batch\n",
            "2022-11-20 12:45:19,297 [INFO ]  epoch:053 step:0000  current_lr: 0.000027  l1_loss:0.022114 flow_loss:0.039081 ter_loss:0.019580 loss_sum:0.080775 0.284007s/batch\n",
            "INFO:root:epoch:054 step:0000  current_lr: 0.000027  l1_loss:0.022827 flow_loss:0.035322 ter_loss:0.021822 loss_sum:0.079970 0.281723s/batch\n",
            "2022-11-20 12:45:32,075 [INFO ]  epoch:054 step:0000  current_lr: 0.000027  l1_loss:0.022827 flow_loss:0.035322 ter_loss:0.021822 loss_sum:0.079970 0.281723s/batch\n",
            "INFO:root:epoch:055 step:0000  current_lr: 0.000028  l1_loss:0.021590 flow_loss:0.030625 ter_loss:0.020138 loss_sum:0.072353 0.281018s/batch\n",
            "2022-11-20 12:45:44,880 [INFO ]  epoch:055 step:0000  current_lr: 0.000028  l1_loss:0.021590 flow_loss:0.030625 ter_loss:0.020138 loss_sum:0.072353 0.281018s/batch\n",
            "INFO:root:epoch:056 step:0000  current_lr: 0.000028  l1_loss:0.023105 flow_loss:0.033888 ter_loss:0.022654 loss_sum:0.079648 0.291425s/batch\n",
            "2022-11-20 12:45:57,770 [INFO ]  epoch:056 step:0000  current_lr: 0.000028  l1_loss:0.023105 flow_loss:0.033888 ter_loss:0.022654 loss_sum:0.079648 0.291425s/batch\n",
            "INFO:root:epoch:057 step:0000  current_lr: 0.000028  l1_loss:0.021922 flow_loss:0.038736 ter_loss:0.020450 loss_sum:0.081109 0.288054s/batch\n",
            "2022-11-20 12:46:10,620 [INFO ]  epoch:057 step:0000  current_lr: 0.000028  l1_loss:0.021922 flow_loss:0.038736 ter_loss:0.020450 loss_sum:0.081109 0.288054s/batch\n",
            "INFO:root:epoch:058 step:0000  current_lr: 0.000029  l1_loss:0.021615 flow_loss:0.028376 ter_loss:0.021862 loss_sum:0.071852 0.282772s/batch\n",
            "2022-11-20 12:46:23,435 [INFO ]  epoch:058 step:0000  current_lr: 0.000029  l1_loss:0.021615 flow_loss:0.028376 ter_loss:0.021862 loss_sum:0.071852 0.282772s/batch\n",
            "INFO:root:epoch:059 step:0000  current_lr: 0.000029  l1_loss:0.020004 flow_loss:0.028674 ter_loss:0.019176 loss_sum:0.067854 0.278172s/batch\n",
            "2022-11-20 12:46:36,186 [INFO ]  epoch:059 step:0000  current_lr: 0.000029  l1_loss:0.020004 flow_loss:0.028674 ter_loss:0.019176 loss_sum:0.067854 0.278172s/batch\n",
            "INFO:root:epoch:060 step:0000  current_lr: 0.000030  l1_loss:0.021614 flow_loss:0.030851 ter_loss:0.019415 loss_sum:0.071880 0.283302s/batch\n",
            "2022-11-20 12:46:48,989 [INFO ]  epoch:060 step:0000  current_lr: 0.000030  l1_loss:0.021614 flow_loss:0.030851 ter_loss:0.019415 loss_sum:0.071880 0.283302s/batch\n",
            "INFO:root:Saving state, epoch: 60 iter:0\n",
            "2022-11-20 12:46:58,965 [INFO ]  Saving state, epoch: 60 iter:0\n",
            "INFO:root:epoch:061 step:0000  current_lr: 0.000030  l1_loss:0.020014 flow_loss:0.034099 ter_loss:0.018793 loss_sum:0.072905 0.278045s/batch\n",
            "2022-11-20 12:47:02,870 [INFO ]  epoch:061 step:0000  current_lr: 0.000030  l1_loss:0.020014 flow_loss:0.034099 ter_loss:0.018793 loss_sum:0.072905 0.278045s/batch\n",
            "INFO:root:epoch:062 step:0000  current_lr: 0.000031  l1_loss:0.021142 flow_loss:0.032222 ter_loss:0.019569 loss_sum:0.072932 0.280966s/batch\n",
            "2022-11-20 12:47:15,662 [INFO ]  epoch:062 step:0000  current_lr: 0.000031  l1_loss:0.021142 flow_loss:0.032222 ter_loss:0.019569 loss_sum:0.072932 0.280966s/batch\n",
            "INFO:root:epoch:063 step:0000  current_lr: 0.000031  l1_loss:0.019949 flow_loss:0.035175 ter_loss:0.018886 loss_sum:0.074009 0.282027s/batch\n",
            "2022-11-20 12:47:28,433 [INFO ]  epoch:063 step:0000  current_lr: 0.000031  l1_loss:0.019949 flow_loss:0.035175 ter_loss:0.018886 loss_sum:0.074009 0.282027s/batch\n",
            "INFO:root:epoch:064 step:0000  current_lr: 0.000032  l1_loss:0.019515 flow_loss:0.028239 ter_loss:0.019227 loss_sum:0.066981 0.281872s/batch\n",
            "2022-11-20 12:47:41,239 [INFO ]  epoch:064 step:0000  current_lr: 0.000032  l1_loss:0.019515 flow_loss:0.028239 ter_loss:0.019227 loss_sum:0.066981 0.281872s/batch\n",
            "INFO:root:epoch:065 step:0000  current_lr: 0.000033  l1_loss:0.021819 flow_loss:0.028445 ter_loss:0.020646 loss_sum:0.070909 0.283746s/batch\n",
            "2022-11-20 12:47:54,038 [INFO ]  epoch:065 step:0000  current_lr: 0.000033  l1_loss:0.021819 flow_loss:0.028445 ter_loss:0.020646 loss_sum:0.070909 0.283746s/batch\n",
            "INFO:root:epoch:066 step:0000  current_lr: 0.000033  l1_loss:0.020256 flow_loss:0.042256 ter_loss:0.017739 loss_sum:0.080251 0.281877s/batch\n",
            "2022-11-20 12:48:06,833 [INFO ]  epoch:066 step:0000  current_lr: 0.000033  l1_loss:0.020256 flow_loss:0.042256 ter_loss:0.017739 loss_sum:0.080251 0.281877s/batch\n",
            "INFO:root:epoch:067 step:0000  current_lr: 0.000034  l1_loss:0.017220 flow_loss:0.035774 ter_loss:0.015401 loss_sum:0.068395 0.278333s/batch\n",
            "2022-11-20 12:48:19,606 [INFO ]  epoch:067 step:0000  current_lr: 0.000034  l1_loss:0.017220 flow_loss:0.035774 ter_loss:0.015401 loss_sum:0.068395 0.278333s/batch\n",
            "INFO:root:epoch:068 step:0000  current_lr: 0.000034  l1_loss:0.018755 flow_loss:0.030421 ter_loss:0.017221 loss_sum:0.066397 0.285624s/batch\n",
            "2022-11-20 12:48:32,431 [INFO ]  epoch:068 step:0000  current_lr: 0.000034  l1_loss:0.018755 flow_loss:0.030421 ter_loss:0.017221 loss_sum:0.066397 0.285624s/batch\n",
            "INFO:root:epoch:069 step:0000  current_lr: 0.000034  l1_loss:0.019685 flow_loss:0.042170 ter_loss:0.017232 loss_sum:0.079087 0.281589s/batch\n",
            "2022-11-20 12:48:45,198 [INFO ]  epoch:069 step:0000  current_lr: 0.000034  l1_loss:0.019685 flow_loss:0.042170 ter_loss:0.017232 loss_sum:0.079087 0.281589s/batch\n",
            "INFO:root:epoch:070 step:0000  current_lr: 0.000035  l1_loss:0.019134 flow_loss:0.034660 ter_loss:0.018518 loss_sum:0.072312 0.284577s/batch\n",
            "2022-11-20 12:48:58,023 [INFO ]  epoch:070 step:0000  current_lr: 0.000035  l1_loss:0.019134 flow_loss:0.034660 ter_loss:0.018518 loss_sum:0.072312 0.284577s/batch\n",
            "INFO:root:epoch:071 step:0000  current_lr: 0.000036  l1_loss:0.018592 flow_loss:0.033847 ter_loss:0.017593 loss_sum:0.070032 0.290626s/batch\n",
            "2022-11-20 12:49:10,902 [INFO ]  epoch:071 step:0000  current_lr: 0.000036  l1_loss:0.018592 flow_loss:0.033847 ter_loss:0.017593 loss_sum:0.070032 0.290626s/batch\n",
            "INFO:root:epoch:072 step:0000  current_lr: 0.000036  l1_loss:0.018133 flow_loss:0.035527 ter_loss:0.015491 loss_sum:0.069151 0.280479s/batch\n",
            "2022-11-20 12:49:23,664 [INFO ]  epoch:072 step:0000  current_lr: 0.000036  l1_loss:0.018133 flow_loss:0.035527 ter_loss:0.015491 loss_sum:0.069151 0.280479s/batch\n",
            "INFO:root:epoch:073 step:0000  current_lr: 0.000036  l1_loss:0.019337 flow_loss:0.036792 ter_loss:0.016321 loss_sum:0.072449 0.281923s/batch\n",
            "2022-11-20 12:49:36,469 [INFO ]  epoch:073 step:0000  current_lr: 0.000036  l1_loss:0.019337 flow_loss:0.036792 ter_loss:0.016321 loss_sum:0.072449 0.281923s/batch\n",
            "INFO:root:epoch:074 step:0000  current_lr: 0.000037  l1_loss:0.018265 flow_loss:0.026480 ter_loss:0.016978 loss_sum:0.061722 0.285138s/batch\n",
            "2022-11-20 12:49:49,291 [INFO ]  epoch:074 step:0000  current_lr: 0.000037  l1_loss:0.018265 flow_loss:0.026480 ter_loss:0.016978 loss_sum:0.061722 0.285138s/batch\n",
            "INFO:root:epoch:075 step:0000  current_lr: 0.000038  l1_loss:0.017505 flow_loss:0.025410 ter_loss:0.016365 loss_sum:0.059279 0.282775s/batch\n",
            "2022-11-20 12:50:02,082 [INFO ]  epoch:075 step:0000  current_lr: 0.000038  l1_loss:0.017505 flow_loss:0.025410 ter_loss:0.016365 loss_sum:0.059279 0.282775s/batch\n",
            "INFO:root:epoch:076 step:0000  current_lr: 0.000038  l1_loss:0.019788 flow_loss:0.043989 ter_loss:0.016920 loss_sum:0.080697 0.278302s/batch\n",
            "2022-11-20 12:50:14,862 [INFO ]  epoch:076 step:0000  current_lr: 0.000038  l1_loss:0.019788 flow_loss:0.043989 ter_loss:0.016920 loss_sum:0.080697 0.278302s/batch\n",
            "INFO:root:epoch:077 step:0000  current_lr: 0.000039  l1_loss:0.019818 flow_loss:0.042254 ter_loss:0.017000 loss_sum:0.079071 0.285556s/batch\n",
            "2022-11-20 12:50:27,684 [INFO ]  epoch:077 step:0000  current_lr: 0.000039  l1_loss:0.019818 flow_loss:0.042254 ter_loss:0.017000 loss_sum:0.079071 0.285556s/batch\n",
            "INFO:root:epoch:078 step:0000  current_lr: 0.000039  l1_loss:0.017272 flow_loss:0.036172 ter_loss:0.015170 loss_sum:0.068615 0.282539s/batch\n",
            "2022-11-20 12:50:40,485 [INFO ]  epoch:078 step:0000  current_lr: 0.000039  l1_loss:0.017272 flow_loss:0.036172 ter_loss:0.015170 loss_sum:0.068615 0.282539s/batch\n",
            "INFO:root:epoch:079 step:0000  current_lr: 0.000040  l1_loss:0.019514 flow_loss:0.037481 ter_loss:0.016512 loss_sum:0.073506 0.286003s/batch\n",
            "2022-11-20 12:50:53,314 [INFO ]  epoch:079 step:0000  current_lr: 0.000040  l1_loss:0.019514 flow_loss:0.037481 ter_loss:0.016512 loss_sum:0.073506 0.286003s/batch\n",
            "INFO:root:epoch:080 step:0000  current_lr: 0.000040  l1_loss:0.018916 flow_loss:0.038933 ter_loss:0.016947 loss_sum:0.074797 0.281721s/batch\n",
            "2022-11-20 12:51:06,098 [INFO ]  epoch:080 step:0000  current_lr: 0.000040  l1_loss:0.018916 flow_loss:0.038933 ter_loss:0.016947 loss_sum:0.074797 0.281721s/batch\n",
            "INFO:root:Saving state, epoch: 80 iter:0\n",
            "2022-11-20 12:51:16,062 [INFO ]  Saving state, epoch: 80 iter:0\n",
            "INFO:root:epoch:081 step:0000  current_lr: 0.000041  l1_loss:0.018529 flow_loss:0.033291 ter_loss:0.016199 loss_sum:0.068020 0.283086s/batch\n",
            "2022-11-20 12:51:20,002 [INFO ]  epoch:081 step:0000  current_lr: 0.000041  l1_loss:0.018529 flow_loss:0.033291 ter_loss:0.016199 loss_sum:0.068020 0.283086s/batch\n",
            "INFO:root:epoch:082 step:0000  current_lr: 0.000041  l1_loss:0.016480 flow_loss:0.032352 ter_loss:0.014398 loss_sum:0.063230 0.281025s/batch\n",
            "2022-11-20 12:51:33,344 [INFO ]  epoch:082 step:0000  current_lr: 0.000041  l1_loss:0.016480 flow_loss:0.032352 ter_loss:0.014398 loss_sum:0.063230 0.281025s/batch\n",
            "INFO:root:epoch:083 step:0000  current_lr: 0.000041  l1_loss:0.019463 flow_loss:0.035076 ter_loss:0.016487 loss_sum:0.071027 0.283248s/batch\n",
            "2022-11-20 12:51:46,142 [INFO ]  epoch:083 step:0000  current_lr: 0.000041  l1_loss:0.019463 flow_loss:0.035076 ter_loss:0.016487 loss_sum:0.071027 0.283248s/batch\n",
            "INFO:root:epoch:084 step:0000  current_lr: 0.000042  l1_loss:0.018999 flow_loss:0.032790 ter_loss:0.017191 loss_sum:0.068980 0.278726s/batch\n",
            "2022-11-20 12:51:58,876 [INFO ]  epoch:084 step:0000  current_lr: 0.000042  l1_loss:0.018999 flow_loss:0.032790 ter_loss:0.017191 loss_sum:0.068980 0.278726s/batch\n",
            "INFO:root:epoch:085 step:0000  current_lr: 0.000043  l1_loss:0.017748 flow_loss:0.029879 ter_loss:0.016940 loss_sum:0.064567 0.280042s/batch\n",
            "2022-11-20 12:52:11,668 [INFO ]  epoch:085 step:0000  current_lr: 0.000043  l1_loss:0.017748 flow_loss:0.029879 ter_loss:0.016940 loss_sum:0.064567 0.280042s/batch\n",
            "INFO:root:epoch:086 step:0000  current_lr: 0.000043  l1_loss:0.017264 flow_loss:0.033458 ter_loss:0.014429 loss_sum:0.065151 0.280845s/batch\n",
            "2022-11-20 12:52:24,453 [INFO ]  epoch:086 step:0000  current_lr: 0.000043  l1_loss:0.017264 flow_loss:0.033458 ter_loss:0.014429 loss_sum:0.065151 0.280845s/batch\n",
            "INFO:root:epoch:087 step:0000  current_lr: 0.000044  l1_loss:0.017091 flow_loss:0.039504 ter_loss:0.014500 loss_sum:0.071095 0.284509s/batch\n",
            "2022-11-20 12:52:37,258 [INFO ]  epoch:087 step:0000  current_lr: 0.000044  l1_loss:0.017091 flow_loss:0.039504 ter_loss:0.014500 loss_sum:0.071095 0.284509s/batch\n",
            "INFO:root:epoch:088 step:0000  current_lr: 0.000044  l1_loss:0.017773 flow_loss:0.035211 ter_loss:0.015366 loss_sum:0.068350 0.282404s/batch\n",
            "2022-11-20 12:52:50,049 [INFO ]  epoch:088 step:0000  current_lr: 0.000044  l1_loss:0.017773 flow_loss:0.035211 ter_loss:0.015366 loss_sum:0.068350 0.282404s/batch\n",
            "INFO:root:epoch:089 step:0000  current_lr: 0.000045  l1_loss:0.016941 flow_loss:0.028424 ter_loss:0.015324 loss_sum:0.060689 0.280508s/batch\n",
            "2022-11-20 12:53:02,810 [INFO ]  epoch:089 step:0000  current_lr: 0.000045  l1_loss:0.016941 flow_loss:0.028424 ter_loss:0.015324 loss_sum:0.060689 0.280508s/batch\n",
            "INFO:root:epoch:090 step:0000  current_lr: 0.000045  l1_loss:0.016353 flow_loss:0.031712 ter_loss:0.014465 loss_sum:0.062530 0.279510s/batch\n",
            "2022-11-20 12:53:15,583 [INFO ]  epoch:090 step:0000  current_lr: 0.000045  l1_loss:0.016353 flow_loss:0.031712 ter_loss:0.014465 loss_sum:0.062530 0.279510s/batch\n",
            "INFO:root:epoch:091 step:0000  current_lr: 0.000046  l1_loss:0.017651 flow_loss:0.031924 ter_loss:0.015461 loss_sum:0.065036 0.281104s/batch\n",
            "2022-11-20 12:53:28,363 [INFO ]  epoch:091 step:0000  current_lr: 0.000046  l1_loss:0.017651 flow_loss:0.031924 ter_loss:0.015461 loss_sum:0.065036 0.281104s/batch\n",
            "INFO:root:epoch:092 step:0000  current_lr: 0.000046  l1_loss:0.017319 flow_loss:0.033154 ter_loss:0.014729 loss_sum:0.065202 0.279267s/batch\n",
            "2022-11-20 12:53:41,115 [INFO ]  epoch:092 step:0000  current_lr: 0.000046  l1_loss:0.017319 flow_loss:0.033154 ter_loss:0.014729 loss_sum:0.065202 0.279267s/batch\n",
            "INFO:root:epoch:093 step:0000  current_lr: 0.000047  l1_loss:0.017156 flow_loss:0.030471 ter_loss:0.016507 loss_sum:0.064133 0.278972s/batch\n",
            "2022-11-20 12:53:53,867 [INFO ]  epoch:093 step:0000  current_lr: 0.000047  l1_loss:0.017156 flow_loss:0.030471 ter_loss:0.016507 loss_sum:0.064133 0.278972s/batch\n",
            "INFO:root:epoch:094 step:0000  current_lr: 0.000047  l1_loss:0.017326 flow_loss:0.032319 ter_loss:0.015569 loss_sum:0.065214 0.279226s/batch\n",
            "2022-11-20 12:54:06,625 [INFO ]  epoch:094 step:0000  current_lr: 0.000047  l1_loss:0.017326 flow_loss:0.032319 ter_loss:0.015569 loss_sum:0.065214 0.279226s/batch\n",
            "INFO:root:epoch:095 step:0000  current_lr: 0.000048  l1_loss:0.016434 flow_loss:0.032899 ter_loss:0.013836 loss_sum:0.063169 0.278482s/batch\n",
            "2022-11-20 12:54:19,383 [INFO ]  epoch:095 step:0000  current_lr: 0.000048  l1_loss:0.016434 flow_loss:0.032899 ter_loss:0.013836 loss_sum:0.063169 0.278482s/batch\n",
            "INFO:root:epoch:096 step:0000  current_lr: 0.000048  l1_loss:0.017234 flow_loss:0.039104 ter_loss:0.014950 loss_sum:0.071287 0.280439s/batch\n",
            "2022-11-20 12:54:32,151 [INFO ]  epoch:096 step:0000  current_lr: 0.000048  l1_loss:0.017234 flow_loss:0.039104 ter_loss:0.014950 loss_sum:0.071287 0.280439s/batch\n",
            "INFO:root:epoch:097 step:0000  current_lr: 0.000048  l1_loss:0.017096 flow_loss:0.037524 ter_loss:0.015294 loss_sum:0.069914 0.280906s/batch\n",
            "2022-11-20 12:54:44,934 [INFO ]  epoch:097 step:0000  current_lr: 0.000048  l1_loss:0.017096 flow_loss:0.037524 ter_loss:0.015294 loss_sum:0.069914 0.280906s/batch\n",
            "INFO:root:epoch:098 step:0000  current_lr: 0.000049  l1_loss:0.018977 flow_loss:0.046646 ter_loss:0.015645 loss_sum:0.081268 0.283131s/batch\n",
            "2022-11-20 12:54:57,752 [INFO ]  epoch:098 step:0000  current_lr: 0.000049  l1_loss:0.018977 flow_loss:0.046646 ter_loss:0.015645 loss_sum:0.081268 0.283131s/batch\n",
            "INFO:root:epoch:099 step:0000  current_lr: 0.000050  l1_loss:0.017570 flow_loss:0.032600 ter_loss:0.015902 loss_sum:0.066071 0.282120s/batch\n",
            "2022-11-20 12:55:10,545 [INFO ]  epoch:099 step:0000  current_lr: 0.000050  l1_loss:0.017570 flow_loss:0.032600 ter_loss:0.015902 loss_sum:0.066071 0.282120s/batch\n",
            "INFO:root:epoch:100 step:0000  current_lr: 0.000050  l1_loss:0.018058 flow_loss:0.033687 ter_loss:0.015505 loss_sum:0.067250 0.278351s/batch\n",
            "2022-11-20 12:55:23,291 [INFO ]  epoch:100 step:0000  current_lr: 0.000050  l1_loss:0.018058 flow_loss:0.033687 ter_loss:0.015505 loss_sum:0.067250 0.278351s/batch\n",
            "INFO:root:Saving state, epoch: 100 iter:0\n",
            "2022-11-20 12:55:33,278 [INFO ]  Saving state, epoch: 100 iter:0\n",
            "INFO:root:epoch:101 step:0000  current_lr: 0.000051  l1_loss:0.016527 flow_loss:0.030053 ter_loss:0.014410 loss_sum:0.060990 0.283230s/batch\n",
            "2022-11-20 12:55:37,225 [INFO ]  epoch:101 step:0000  current_lr: 0.000051  l1_loss:0.016527 flow_loss:0.030053 ter_loss:0.014410 loss_sum:0.060990 0.283230s/batch\n",
            "INFO:root:epoch:102 step:0000  current_lr: 0.000051  l1_loss:0.016627 flow_loss:0.039669 ter_loss:0.014247 loss_sum:0.070542 0.281642s/batch\n",
            "2022-11-20 12:55:50,030 [INFO ]  epoch:102 step:0000  current_lr: 0.000051  l1_loss:0.016627 flow_loss:0.039669 ter_loss:0.014247 loss_sum:0.070542 0.281642s/batch\n",
            "INFO:root:epoch:103 step:0000  current_lr: 0.000052  l1_loss:0.015730 flow_loss:0.031137 ter_loss:0.013388 loss_sum:0.060255 0.283116s/batch\n",
            "2022-11-20 12:56:03,903 [INFO ]  epoch:103 step:0000  current_lr: 0.000052  l1_loss:0.015730 flow_loss:0.031137 ter_loss:0.013388 loss_sum:0.060255 0.283116s/batch\n",
            "INFO:root:epoch:104 step:0000  current_lr: 0.000052  l1_loss:0.017272 flow_loss:0.033098 ter_loss:0.015242 loss_sum:0.065612 0.282298s/batch\n",
            "2022-11-20 12:56:16,689 [INFO ]  epoch:104 step:0000  current_lr: 0.000052  l1_loss:0.017272 flow_loss:0.033098 ter_loss:0.015242 loss_sum:0.065612 0.282298s/batch\n",
            "INFO:root:epoch:105 step:0000  current_lr: 0.000053  l1_loss:0.014954 flow_loss:0.035128 ter_loss:0.012990 loss_sum:0.063072 0.280951s/batch\n",
            "2022-11-20 12:56:29,479 [INFO ]  epoch:105 step:0000  current_lr: 0.000053  l1_loss:0.014954 flow_loss:0.035128 ter_loss:0.012990 loss_sum:0.063072 0.280951s/batch\n",
            "INFO:root:epoch:106 step:0000  current_lr: 0.000053  l1_loss:0.015396 flow_loss:0.029354 ter_loss:0.013448 loss_sum:0.058197 0.279305s/batch\n",
            "2022-11-20 12:56:42,235 [INFO ]  epoch:106 step:0000  current_lr: 0.000053  l1_loss:0.015396 flow_loss:0.029354 ter_loss:0.013448 loss_sum:0.058197 0.279305s/batch\n",
            "INFO:root:epoch:107 step:0000  current_lr: 0.000054  l1_loss:0.016562 flow_loss:0.033359 ter_loss:0.014104 loss_sum:0.064025 0.281290s/batch\n",
            "2022-11-20 12:56:55,013 [INFO ]  epoch:107 step:0000  current_lr: 0.000054  l1_loss:0.016562 flow_loss:0.033359 ter_loss:0.014104 loss_sum:0.064025 0.281290s/batch\n",
            "INFO:root:epoch:108 step:0000  current_lr: 0.000054  l1_loss:0.015938 flow_loss:0.031126 ter_loss:0.013908 loss_sum:0.060972 0.285277s/batch\n",
            "2022-11-20 12:57:07,834 [INFO ]  epoch:108 step:0000  current_lr: 0.000054  l1_loss:0.015938 flow_loss:0.031126 ter_loss:0.013908 loss_sum:0.060972 0.285277s/batch\n",
            "INFO:root:epoch:109 step:0000  current_lr: 0.000055  l1_loss:0.016324 flow_loss:0.041081 ter_loss:0.013780 loss_sum:0.071185 0.285407s/batch\n",
            "2022-11-20 12:57:20,642 [INFO ]  epoch:109 step:0000  current_lr: 0.000055  l1_loss:0.016324 flow_loss:0.041081 ter_loss:0.013780 loss_sum:0.071185 0.285407s/batch\n",
            "INFO:root:epoch:110 step:0000  current_lr: 0.000055  l1_loss:0.016973 flow_loss:0.031422 ter_loss:0.014617 loss_sum:0.063012 0.278611s/batch\n",
            "2022-11-20 12:57:33,400 [INFO ]  epoch:110 step:0000  current_lr: 0.000055  l1_loss:0.016973 flow_loss:0.031422 ter_loss:0.014617 loss_sum:0.063012 0.278611s/batch\n",
            "INFO:root:epoch:111 step:0000  current_lr: 0.000056  l1_loss:0.017413 flow_loss:0.042510 ter_loss:0.015149 loss_sum:0.075072 0.281619s/batch\n",
            "2022-11-20 12:57:46,186 [INFO ]  epoch:111 step:0000  current_lr: 0.000056  l1_loss:0.017413 flow_loss:0.042510 ter_loss:0.015149 loss_sum:0.075072 0.281619s/batch\n",
            "INFO:root:epoch:112 step:0000  current_lr: 0.000056  l1_loss:0.016736 flow_loss:0.041997 ter_loss:0.013531 loss_sum:0.072264 0.281839s/batch\n",
            "2022-11-20 12:57:58,975 [INFO ]  epoch:112 step:0000  current_lr: 0.000056  l1_loss:0.016736 flow_loss:0.041997 ter_loss:0.013531 loss_sum:0.072264 0.281839s/batch\n",
            "INFO:root:epoch:113 step:0000  current_lr: 0.000056  l1_loss:0.017588 flow_loss:0.041549 ter_loss:0.015602 loss_sum:0.074738 0.279249s/batch\n",
            "2022-11-20 12:58:11,759 [INFO ]  epoch:113 step:0000  current_lr: 0.000056  l1_loss:0.017588 flow_loss:0.041549 ter_loss:0.015602 loss_sum:0.074738 0.279249s/batch\n",
            "INFO:root:epoch:114 step:0000  current_lr: 0.000057  l1_loss:0.016137 flow_loss:0.033030 ter_loss:0.013782 loss_sum:0.062949 0.280197s/batch\n",
            "2022-11-20 12:58:24,524 [INFO ]  epoch:114 step:0000  current_lr: 0.000057  l1_loss:0.016137 flow_loss:0.033030 ter_loss:0.013782 loss_sum:0.062949 0.280197s/batch\n",
            "INFO:root:epoch:115 step:0000  current_lr: 0.000057  l1_loss:0.015475 flow_loss:0.033033 ter_loss:0.012734 loss_sum:0.061242 0.285850s/batch\n",
            "2022-11-20 12:58:37,358 [INFO ]  epoch:115 step:0000  current_lr: 0.000057  l1_loss:0.015475 flow_loss:0.033033 ter_loss:0.012734 loss_sum:0.061242 0.285850s/batch\n",
            "INFO:root:epoch:116 step:0000  current_lr: 0.000058  l1_loss:0.016452 flow_loss:0.037420 ter_loss:0.013990 loss_sum:0.067862 0.282211s/batch\n",
            "2022-11-20 12:58:50,188 [INFO ]  epoch:116 step:0000  current_lr: 0.000058  l1_loss:0.016452 flow_loss:0.037420 ter_loss:0.013990 loss_sum:0.067862 0.282211s/batch\n",
            "INFO:root:epoch:117 step:0000  current_lr: 0.000058  l1_loss:0.016866 flow_loss:0.040434 ter_loss:0.013981 loss_sum:0.071281 0.285518s/batch\n",
            "2022-11-20 12:59:03,011 [INFO ]  epoch:117 step:0000  current_lr: 0.000058  l1_loss:0.016866 flow_loss:0.040434 ter_loss:0.013981 loss_sum:0.071281 0.285518s/batch\n",
            "INFO:root:epoch:118 step:0000  current_lr: 0.000059  l1_loss:0.017188 flow_loss:0.035714 ter_loss:0.014151 loss_sum:0.067053 0.283074s/batch\n",
            "2022-11-20 12:59:15,806 [INFO ]  epoch:118 step:0000  current_lr: 0.000059  l1_loss:0.017188 flow_loss:0.035714 ter_loss:0.014151 loss_sum:0.067053 0.283074s/batch\n",
            "INFO:root:epoch:119 step:0000  current_lr: 0.000060  l1_loss:0.016017 flow_loss:0.032748 ter_loss:0.013645 loss_sum:0.062409 0.282789s/batch\n",
            "2022-11-20 12:59:28,630 [INFO ]  epoch:119 step:0000  current_lr: 0.000060  l1_loss:0.016017 flow_loss:0.032748 ter_loss:0.013645 loss_sum:0.062409 0.282789s/batch\n",
            "INFO:root:epoch:120 step:0000  current_lr: 0.000060  l1_loss:0.014439 flow_loss:0.032401 ter_loss:0.011751 loss_sum:0.058591 0.285026s/batch\n",
            "2022-11-20 12:59:41,473 [INFO ]  epoch:120 step:0000  current_lr: 0.000060  l1_loss:0.014439 flow_loss:0.032401 ter_loss:0.011751 loss_sum:0.058591 0.285026s/batch\n",
            "INFO:root:Saving state, epoch: 120 iter:0\n",
            "2022-11-20 12:59:51,461 [INFO ]  Saving state, epoch: 120 iter:0\n",
            "INFO:root:epoch:121 step:0000  current_lr: 0.000061  l1_loss:0.016252 flow_loss:0.036839 ter_loss:0.014007 loss_sum:0.067098 0.282072s/batch\n",
            "2022-11-20 12:59:55,407 [INFO ]  epoch:121 step:0000  current_lr: 0.000061  l1_loss:0.016252 flow_loss:0.036839 ter_loss:0.014007 loss_sum:0.067098 0.282072s/batch\n",
            "INFO:root:epoch:122 step:0000  current_lr: 0.000061  l1_loss:0.016379 flow_loss:0.031550 ter_loss:0.014300 loss_sum:0.062228 0.280278s/batch\n",
            "2022-11-20 13:00:08,201 [INFO ]  epoch:122 step:0000  current_lr: 0.000061  l1_loss:0.016379 flow_loss:0.031550 ter_loss:0.014300 loss_sum:0.062228 0.280278s/batch\n",
            "INFO:root:epoch:123 step:0000  current_lr: 0.000062  l1_loss:0.015623 flow_loss:0.033757 ter_loss:0.013005 loss_sum:0.062384 0.280926s/batch\n",
            "2022-11-20 13:00:20,990 [INFO ]  epoch:123 step:0000  current_lr: 0.000062  l1_loss:0.015623 flow_loss:0.033757 ter_loss:0.013005 loss_sum:0.062384 0.280926s/batch\n",
            "INFO:root:epoch:124 step:0000  current_lr: 0.000062  l1_loss:0.013250 flow_loss:0.034776 ter_loss:0.010382 loss_sum:0.058408 0.281388s/batch\n",
            "2022-11-20 13:00:33,764 [INFO ]  epoch:124 step:0000  current_lr: 0.000062  l1_loss:0.013250 flow_loss:0.034776 ter_loss:0.010382 loss_sum:0.058408 0.281388s/batch\n",
            "INFO:root:epoch:125 step:0000  current_lr: 0.000063  l1_loss:0.015781 flow_loss:0.035028 ter_loss:0.012795 loss_sum:0.063603 0.282313s/batch\n",
            "2022-11-20 13:00:46,586 [INFO ]  epoch:125 step:0000  current_lr: 0.000063  l1_loss:0.015781 flow_loss:0.035028 ter_loss:0.012795 loss_sum:0.063603 0.282313s/batch\n",
            "INFO:root:epoch:126 step:0000  current_lr: 0.000063  l1_loss:0.017474 flow_loss:0.043497 ter_loss:0.014014 loss_sum:0.074986 0.283229s/batch\n",
            "2022-11-20 13:00:59,399 [INFO ]  epoch:126 step:0000  current_lr: 0.000063  l1_loss:0.017474 flow_loss:0.043497 ter_loss:0.014014 loss_sum:0.074986 0.283229s/batch\n",
            "INFO:root:epoch:127 step:0000  current_lr: 0.000063  l1_loss:0.015447 flow_loss:0.035655 ter_loss:0.012055 loss_sum:0.063157 0.280670s/batch\n",
            "2022-11-20 13:01:12,192 [INFO ]  epoch:127 step:0000  current_lr: 0.000063  l1_loss:0.015447 flow_loss:0.035655 ter_loss:0.012055 loss_sum:0.063157 0.280670s/batch\n",
            "INFO:root:epoch:128 step:0000  current_lr: 0.000064  l1_loss:0.014449 flow_loss:0.028245 ter_loss:0.011749 loss_sum:0.054443 0.278544s/batch\n",
            "2022-11-20 13:01:24,953 [INFO ]  epoch:128 step:0000  current_lr: 0.000064  l1_loss:0.014449 flow_loss:0.028245 ter_loss:0.011749 loss_sum:0.054443 0.278544s/batch\n",
            "INFO:root:epoch:129 step:0000  current_lr: 0.000065  l1_loss:0.015115 flow_loss:0.033581 ter_loss:0.012664 loss_sum:0.061359 0.282289s/batch\n",
            "2022-11-20 13:01:37,755 [INFO ]  epoch:129 step:0000  current_lr: 0.000065  l1_loss:0.015115 flow_loss:0.033581 ter_loss:0.012664 loss_sum:0.061359 0.282289s/batch\n",
            "INFO:root:epoch:130 step:0000  current_lr: 0.000065  l1_loss:0.015059 flow_loss:0.030581 ter_loss:0.012527 loss_sum:0.058167 0.282441s/batch\n",
            "2022-11-20 13:01:50,548 [INFO ]  epoch:130 step:0000  current_lr: 0.000065  l1_loss:0.015059 flow_loss:0.030581 ter_loss:0.012527 loss_sum:0.058167 0.282441s/batch\n",
            "INFO:root:epoch:131 step:0000  current_lr: 0.000066  l1_loss:0.015115 flow_loss:0.038801 ter_loss:0.012026 loss_sum:0.065943 0.285238s/batch\n",
            "2022-11-20 13:02:03,389 [INFO ]  epoch:131 step:0000  current_lr: 0.000066  l1_loss:0.015115 flow_loss:0.038801 ter_loss:0.012026 loss_sum:0.065943 0.285238s/batch\n",
            "INFO:root:epoch:132 step:0000  current_lr: 0.000066  l1_loss:0.016233 flow_loss:0.036919 ter_loss:0.012772 loss_sum:0.065924 0.282668s/batch\n",
            "2022-11-20 13:02:16,171 [INFO ]  epoch:132 step:0000  current_lr: 0.000066  l1_loss:0.016233 flow_loss:0.036919 ter_loss:0.012772 loss_sum:0.065924 0.282668s/batch\n",
            "INFO:root:epoch:133 step:0000  current_lr: 0.000067  l1_loss:0.015594 flow_loss:0.030536 ter_loss:0.012646 loss_sum:0.058775 0.280996s/batch\n",
            "2022-11-20 13:02:28,942 [INFO ]  epoch:133 step:0000  current_lr: 0.000067  l1_loss:0.015594 flow_loss:0.030536 ter_loss:0.012646 loss_sum:0.058775 0.280996s/batch\n",
            "INFO:root:epoch:134 step:0000  current_lr: 0.000067  l1_loss:0.016086 flow_loss:0.038127 ter_loss:0.012742 loss_sum:0.066955 0.283445s/batch\n",
            "2022-11-20 13:02:41,757 [INFO ]  epoch:134 step:0000  current_lr: 0.000067  l1_loss:0.016086 flow_loss:0.038127 ter_loss:0.012742 loss_sum:0.066955 0.283445s/batch\n",
            "INFO:root:epoch:135 step:0000  current_lr: 0.000068  l1_loss:0.014015 flow_loss:0.025556 ter_loss:0.012089 loss_sum:0.051660 0.281311s/batch\n",
            "2022-11-20 13:02:54,552 [INFO ]  epoch:135 step:0000  current_lr: 0.000068  l1_loss:0.014015 flow_loss:0.025556 ter_loss:0.012089 loss_sum:0.051660 0.281311s/batch\n",
            "INFO:root:epoch:136 step:0000  current_lr: 0.000068  l1_loss:0.016435 flow_loss:0.033563 ter_loss:0.013876 loss_sum:0.063874 0.294261s/batch\n",
            "2022-11-20 13:03:07,493 [INFO ]  epoch:136 step:0000  current_lr: 0.000068  l1_loss:0.016435 flow_loss:0.033563 ter_loss:0.013876 loss_sum:0.063874 0.294261s/batch\n",
            "INFO:root:epoch:137 step:0000  current_lr: 0.000069  l1_loss:0.015976 flow_loss:0.038052 ter_loss:0.012408 loss_sum:0.066436 0.280176s/batch\n",
            "2022-11-20 13:03:20,273 [INFO ]  epoch:137 step:0000  current_lr: 0.000069  l1_loss:0.015976 flow_loss:0.038052 ter_loss:0.012408 loss_sum:0.066436 0.280176s/batch\n",
            "INFO:root:epoch:138 step:0000  current_lr: 0.000069  l1_loss:0.015381 flow_loss:0.029263 ter_loss:0.012589 loss_sum:0.057233 0.285472s/batch\n",
            "2022-11-20 13:03:33,112 [INFO ]  epoch:138 step:0000  current_lr: 0.000069  l1_loss:0.015381 flow_loss:0.029263 ter_loss:0.012589 loss_sum:0.057233 0.285472s/batch\n",
            "INFO:root:epoch:139 step:0000  current_lr: 0.000069  l1_loss:0.015230 flow_loss:0.032026 ter_loss:0.013557 loss_sum:0.060813 0.280861s/batch\n",
            "2022-11-20 13:03:45,890 [INFO ]  epoch:139 step:0000  current_lr: 0.000069  l1_loss:0.015230 flow_loss:0.032026 ter_loss:0.013557 loss_sum:0.060813 0.280861s/batch\n",
            "INFO:root:epoch:140 step:0000  current_lr: 0.000070  l1_loss:0.016806 flow_loss:0.035954 ter_loss:0.013800 loss_sum:0.066559 0.283272s/batch\n",
            "2022-11-20 13:03:58,707 [INFO ]  epoch:140 step:0000  current_lr: 0.000070  l1_loss:0.016806 flow_loss:0.035954 ter_loss:0.013800 loss_sum:0.066559 0.283272s/batch\n",
            "INFO:root:Saving state, epoch: 140 iter:0\n",
            "2022-11-20 13:04:08,677 [INFO ]  Saving state, epoch: 140 iter:0\n",
            "INFO:root:epoch:141 step:0000  current_lr: 0.000071  l1_loss:0.014190 flow_loss:0.033224 ter_loss:0.011368 loss_sum:0.058781 0.281708s/batch\n",
            "2022-11-20 13:04:12,653 [INFO ]  epoch:141 step:0000  current_lr: 0.000071  l1_loss:0.014190 flow_loss:0.033224 ter_loss:0.011368 loss_sum:0.058781 0.281708s/batch\n",
            "INFO:root:epoch:142 step:0000  current_lr: 0.000071  l1_loss:0.015129 flow_loss:0.038664 ter_loss:0.011714 loss_sum:0.065507 0.287359s/batch\n",
            "2022-11-20 13:04:25,521 [INFO ]  epoch:142 step:0000  current_lr: 0.000071  l1_loss:0.015129 flow_loss:0.038664 ter_loss:0.011714 loss_sum:0.065507 0.287359s/batch\n",
            "INFO:root:epoch:143 step:0000  current_lr: 0.000072  l1_loss:0.015562 flow_loss:0.035045 ter_loss:0.011735 loss_sum:0.062342 0.281162s/batch\n",
            "2022-11-20 13:04:38,306 [INFO ]  epoch:143 step:0000  current_lr: 0.000072  l1_loss:0.015562 flow_loss:0.035045 ter_loss:0.011735 loss_sum:0.062342 0.281162s/batch\n",
            "INFO:root:epoch:144 step:0000  current_lr: 0.000072  l1_loss:0.014980 flow_loss:0.034562 ter_loss:0.011617 loss_sum:0.061159 0.284080s/batch\n",
            "2022-11-20 13:04:51,116 [INFO ]  epoch:144 step:0000  current_lr: 0.000072  l1_loss:0.014980 flow_loss:0.034562 ter_loss:0.011617 loss_sum:0.061159 0.284080s/batch\n",
            "INFO:root:epoch:145 step:0000  current_lr: 0.000073  l1_loss:0.014487 flow_loss:0.041690 ter_loss:0.010523 loss_sum:0.066700 0.280571s/batch\n",
            "2022-11-20 13:05:03,883 [INFO ]  epoch:145 step:0000  current_lr: 0.000073  l1_loss:0.014487 flow_loss:0.041690 ter_loss:0.010523 loss_sum:0.066700 0.280571s/batch\n",
            "INFO:root:epoch:146 step:0000  current_lr: 0.000073  l1_loss:0.015401 flow_loss:0.033647 ter_loss:0.012698 loss_sum:0.061746 0.290812s/batch\n",
            "2022-11-20 13:05:16,765 [INFO ]  epoch:146 step:0000  current_lr: 0.000073  l1_loss:0.015401 flow_loss:0.033647 ter_loss:0.012698 loss_sum:0.061746 0.290812s/batch\n",
            "INFO:root:epoch:147 step:0000  current_lr: 0.000073  l1_loss:0.015819 flow_loss:0.033563 ter_loss:0.011975 loss_sum:0.061357 0.285072s/batch\n",
            "2022-11-20 13:05:29,601 [INFO ]  epoch:147 step:0000  current_lr: 0.000073  l1_loss:0.015819 flow_loss:0.033563 ter_loss:0.011975 loss_sum:0.061357 0.285072s/batch\n",
            "INFO:root:epoch:148 step:0000  current_lr: 0.000074  l1_loss:0.016516 flow_loss:0.037060 ter_loss:0.013277 loss_sum:0.066853 0.285847s/batch\n",
            "2022-11-20 13:05:42,426 [INFO ]  epoch:148 step:0000  current_lr: 0.000074  l1_loss:0.016516 flow_loss:0.037060 ter_loss:0.013277 loss_sum:0.066853 0.285847s/batch\n",
            "INFO:root:epoch:149 step:0000  current_lr: 0.000075  l1_loss:0.015088 flow_loss:0.042096 ter_loss:0.011486 loss_sum:0.068670 0.282934s/batch\n",
            "2022-11-20 13:05:55,218 [INFO ]  epoch:149 step:0000  current_lr: 0.000075  l1_loss:0.015088 flow_loss:0.042096 ter_loss:0.011486 loss_sum:0.068670 0.282934s/batch\n",
            "INFO:root:epoch:150 step:0000  current_lr: 0.000075  l1_loss:0.015484 flow_loss:0.040181 ter_loss:0.011655 loss_sum:0.067320 0.280626s/batch\n",
            "2022-11-20 13:06:08,029 [INFO ]  epoch:150 step:0000  current_lr: 0.000075  l1_loss:0.015484 flow_loss:0.040181 ter_loss:0.011655 loss_sum:0.067320 0.280626s/batch\n",
            "INFO:root:epoch:151 step:0000  current_lr: 0.000076  l1_loss:0.014903 flow_loss:0.040079 ter_loss:0.011355 loss_sum:0.066338 0.280822s/batch\n",
            "2022-11-20 13:06:20,808 [INFO ]  epoch:151 step:0000  current_lr: 0.000076  l1_loss:0.014903 flow_loss:0.040079 ter_loss:0.011355 loss_sum:0.066338 0.280822s/batch\n",
            "INFO:root:epoch:152 step:0000  current_lr: 0.000076  l1_loss:0.013284 flow_loss:0.032737 ter_loss:0.010555 loss_sum:0.056575 0.286008s/batch\n",
            "2022-11-20 13:06:33,643 [INFO ]  epoch:152 step:0000  current_lr: 0.000076  l1_loss:0.013284 flow_loss:0.032737 ter_loss:0.010555 loss_sum:0.056575 0.286008s/batch\n",
            "INFO:root:epoch:153 step:0000  current_lr: 0.000077  l1_loss:0.015138 flow_loss:0.039684 ter_loss:0.011321 loss_sum:0.066143 0.281588s/batch\n",
            "2022-11-20 13:06:46,448 [INFO ]  epoch:153 step:0000  current_lr: 0.000077  l1_loss:0.015138 flow_loss:0.039684 ter_loss:0.011321 loss_sum:0.066143 0.281588s/batch\n",
            "INFO:root:epoch:154 step:0000  current_lr: 0.000077  l1_loss:0.015781 flow_loss:0.035110 ter_loss:0.013289 loss_sum:0.064181 0.282383s/batch\n",
            "2022-11-20 13:06:59,239 [INFO ]  epoch:154 step:0000  current_lr: 0.000077  l1_loss:0.015781 flow_loss:0.035110 ter_loss:0.013289 loss_sum:0.064181 0.282383s/batch\n",
            "INFO:root:epoch:155 step:0000  current_lr: 0.000077  l1_loss:0.014879 flow_loss:0.034396 ter_loss:0.012647 loss_sum:0.061922 0.282347s/batch\n",
            "2022-11-20 13:07:12,050 [INFO ]  epoch:155 step:0000  current_lr: 0.000077  l1_loss:0.014879 flow_loss:0.034396 ter_loss:0.012647 loss_sum:0.061922 0.282347s/batch\n",
            "INFO:root:epoch:156 step:0000  current_lr: 0.000078  l1_loss:0.015957 flow_loss:0.036043 ter_loss:0.012902 loss_sum:0.064902 0.282417s/batch\n",
            "2022-11-20 13:07:24,866 [INFO ]  epoch:156 step:0000  current_lr: 0.000078  l1_loss:0.015957 flow_loss:0.036043 ter_loss:0.012902 loss_sum:0.064902 0.282417s/batch\n",
            "INFO:root:epoch:157 step:0000  current_lr: 0.000079  l1_loss:0.015196 flow_loss:0.033661 ter_loss:0.012404 loss_sum:0.061260 0.280295s/batch\n",
            "2022-11-20 13:07:37,644 [INFO ]  epoch:157 step:0000  current_lr: 0.000079  l1_loss:0.015196 flow_loss:0.033661 ter_loss:0.012404 loss_sum:0.061260 0.280295s/batch\n",
            "INFO:root:epoch:158 step:0000  current_lr: 0.000079  l1_loss:0.014267 flow_loss:0.031420 ter_loss:0.011233 loss_sum:0.056920 0.281800s/batch\n",
            "2022-11-20 13:07:50,430 [INFO ]  epoch:158 step:0000  current_lr: 0.000079  l1_loss:0.014267 flow_loss:0.031420 ter_loss:0.011233 loss_sum:0.056920 0.281800s/batch\n",
            "INFO:root:epoch:159 step:0000  current_lr: 0.000080  l1_loss:0.012321 flow_loss:0.041864 ter_loss:0.009190 loss_sum:0.063374 0.281128s/batch\n",
            "2022-11-20 13:08:03,254 [INFO ]  epoch:159 step:0000  current_lr: 0.000080  l1_loss:0.012321 flow_loss:0.041864 ter_loss:0.009190 loss_sum:0.063374 0.281128s/batch\n",
            "INFO:root:epoch:160 step:0000  current_lr: 0.000080  l1_loss:0.014638 flow_loss:0.038727 ter_loss:0.011759 loss_sum:0.065123 0.280404s/batch\n",
            "2022-11-20 13:08:16,031 [INFO ]  epoch:160 step:0000  current_lr: 0.000080  l1_loss:0.014638 flow_loss:0.038727 ter_loss:0.011759 loss_sum:0.065123 0.280404s/batch\n",
            "INFO:root:Saving state, epoch: 160 iter:0\n",
            "2022-11-20 13:08:26,014 [INFO ]  Saving state, epoch: 160 iter:0\n",
            "INFO:root:epoch:161 step:0000  current_lr: 0.000081  l1_loss:0.012441 flow_loss:0.024252 ter_loss:0.010059 loss_sum:0.046752 0.281609s/batch\n",
            "2022-11-20 13:08:29,956 [INFO ]  epoch:161 step:0000  current_lr: 0.000081  l1_loss:0.012441 flow_loss:0.024252 ter_loss:0.010059 loss_sum:0.046752 0.281609s/batch\n",
            "INFO:root:epoch:162 step:0000  current_lr: 0.000081  l1_loss:0.014244 flow_loss:0.035061 ter_loss:0.011737 loss_sum:0.061042 0.283572s/batch\n",
            "2022-11-20 13:08:42,784 [INFO ]  epoch:162 step:0000  current_lr: 0.000081  l1_loss:0.014244 flow_loss:0.035061 ter_loss:0.011737 loss_sum:0.061042 0.283572s/batch\n",
            "INFO:root:epoch:163 step:0000  current_lr: 0.000082  l1_loss:0.013886 flow_loss:0.031090 ter_loss:0.010698 loss_sum:0.055674 0.286000s/batch\n",
            "2022-11-20 13:08:55,620 [INFO ]  epoch:163 step:0000  current_lr: 0.000082  l1_loss:0.013886 flow_loss:0.031090 ter_loss:0.010698 loss_sum:0.055674 0.286000s/batch\n",
            "INFO:root:epoch:164 step:0000  current_lr: 0.000082  l1_loss:0.015664 flow_loss:0.040156 ter_loss:0.012710 loss_sum:0.068530 0.284335s/batch\n",
            "2022-11-20 13:09:08,435 [INFO ]  epoch:164 step:0000  current_lr: 0.000082  l1_loss:0.015664 flow_loss:0.040156 ter_loss:0.012710 loss_sum:0.068530 0.284335s/batch\n",
            "INFO:root:epoch:165 step:0000  current_lr: 0.000082  l1_loss:0.015215 flow_loss:0.033505 ter_loss:0.011845 loss_sum:0.060565 0.282292s/batch\n",
            "2022-11-20 13:09:21,233 [INFO ]  epoch:165 step:0000  current_lr: 0.000082  l1_loss:0.015215 flow_loss:0.033505 ter_loss:0.011845 loss_sum:0.060565 0.282292s/batch\n",
            "INFO:root:epoch:166 step:0000  current_lr: 0.000083  l1_loss:0.015155 flow_loss:0.045406 ter_loss:0.011644 loss_sum:0.072205 0.281621s/batch\n",
            "2022-11-20 13:09:34,020 [INFO ]  epoch:166 step:0000  current_lr: 0.000083  l1_loss:0.015155 flow_loss:0.045406 ter_loss:0.011644 loss_sum:0.072205 0.281621s/batch\n",
            "INFO:root:epoch:167 step:0000  current_lr: 0.000083  l1_loss:0.015222 flow_loss:0.041077 ter_loss:0.011406 loss_sum:0.067705 0.282176s/batch\n",
            "2022-11-20 13:09:46,819 [INFO ]  epoch:167 step:0000  current_lr: 0.000083  l1_loss:0.015222 flow_loss:0.041077 ter_loss:0.011406 loss_sum:0.067705 0.282176s/batch\n",
            "INFO:root:epoch:168 step:0000  current_lr: 0.000084  l1_loss:0.015224 flow_loss:0.042651 ter_loss:0.011084 loss_sum:0.068959 0.280707s/batch\n",
            "2022-11-20 13:09:59,626 [INFO ]  epoch:168 step:0000  current_lr: 0.000084  l1_loss:0.015224 flow_loss:0.042651 ter_loss:0.011084 loss_sum:0.068959 0.280707s/batch\n",
            "INFO:root:epoch:169 step:0000  current_lr: 0.000085  l1_loss:0.014232 flow_loss:0.034645 ter_loss:0.010646 loss_sum:0.059524 0.282208s/batch\n",
            "2022-11-20 13:10:12,412 [INFO ]  epoch:169 step:0000  current_lr: 0.000085  l1_loss:0.014232 flow_loss:0.034645 ter_loss:0.010646 loss_sum:0.059524 0.282208s/batch\n",
            "INFO:root:epoch:170 step:0000  current_lr: 0.000085  l1_loss:0.014870 flow_loss:0.034012 ter_loss:0.011435 loss_sum:0.060317 0.282723s/batch\n",
            "2022-11-20 13:10:25,227 [INFO ]  epoch:170 step:0000  current_lr: 0.000085  l1_loss:0.014870 flow_loss:0.034012 ter_loss:0.011435 loss_sum:0.060317 0.282723s/batch\n",
            "INFO:root:epoch:171 step:0000  current_lr: 0.000086  l1_loss:0.014244 flow_loss:0.037605 ter_loss:0.010910 loss_sum:0.062758 0.283538s/batch\n",
            "2022-11-20 13:10:38,044 [INFO ]  epoch:171 step:0000  current_lr: 0.000086  l1_loss:0.014244 flow_loss:0.037605 ter_loss:0.010910 loss_sum:0.062758 0.283538s/batch\n",
            "INFO:root:epoch:172 step:0000  current_lr: 0.000086  l1_loss:0.015625 flow_loss:0.034393 ter_loss:0.013011 loss_sum:0.063029 0.283491s/batch\n",
            "2022-11-20 13:10:50,844 [INFO ]  epoch:172 step:0000  current_lr: 0.000086  l1_loss:0.015625 flow_loss:0.034393 ter_loss:0.013011 loss_sum:0.063029 0.283491s/batch\n",
            "INFO:root:epoch:173 step:0000  current_lr: 0.000087  l1_loss:0.015598 flow_loss:0.030737 ter_loss:0.012260 loss_sum:0.058596 0.284659s/batch\n",
            "2022-11-20 13:11:03,659 [INFO ]  epoch:173 step:0000  current_lr: 0.000087  l1_loss:0.015598 flow_loss:0.030737 ter_loss:0.012260 loss_sum:0.058596 0.284659s/batch\n",
            "INFO:root:epoch:174 step:0000  current_lr: 0.000087  l1_loss:0.016040 flow_loss:0.036556 ter_loss:0.013034 loss_sum:0.065629 0.283529s/batch\n",
            "2022-11-20 13:11:16,488 [INFO ]  epoch:174 step:0000  current_lr: 0.000087  l1_loss:0.016040 flow_loss:0.036556 ter_loss:0.013034 loss_sum:0.065629 0.283529s/batch\n",
            "INFO:root:epoch:175 step:0000  current_lr: 0.000087  l1_loss:0.013489 flow_loss:0.032134 ter_loss:0.009768 loss_sum:0.055392 0.282025s/batch\n",
            "2022-11-20 13:11:29,273 [INFO ]  epoch:175 step:0000  current_lr: 0.000087  l1_loss:0.013489 flow_loss:0.032134 ter_loss:0.009768 loss_sum:0.055392 0.282025s/batch\n",
            "INFO:root:epoch:176 step:0000  current_lr: 0.000088  l1_loss:0.014549 flow_loss:0.031458 ter_loss:0.011716 loss_sum:0.057723 0.282126s/batch\n",
            "2022-11-20 13:11:42,076 [INFO ]  epoch:176 step:0000  current_lr: 0.000088  l1_loss:0.014549 flow_loss:0.031458 ter_loss:0.011716 loss_sum:0.057723 0.282126s/batch\n",
            "INFO:root:epoch:177 step:0000  current_lr: 0.000089  l1_loss:0.013081 flow_loss:0.032107 ter_loss:0.010382 loss_sum:0.055570 0.282077s/batch\n",
            "2022-11-20 13:11:54,902 [INFO ]  epoch:177 step:0000  current_lr: 0.000089  l1_loss:0.013081 flow_loss:0.032107 ter_loss:0.010382 loss_sum:0.055570 0.282077s/batch\n",
            "INFO:root:epoch:178 step:0000  current_lr: 0.000089  l1_loss:0.013977 flow_loss:0.035222 ter_loss:0.011124 loss_sum:0.060323 0.279764s/batch\n",
            "2022-11-20 13:12:07,672 [INFO ]  epoch:178 step:0000  current_lr: 0.000089  l1_loss:0.013977 flow_loss:0.035222 ter_loss:0.011124 loss_sum:0.060323 0.279764s/batch\n",
            "INFO:root:epoch:179 step:0000  current_lr: 0.000090  l1_loss:0.013662 flow_loss:0.035570 ter_loss:0.010362 loss_sum:0.059594 0.279324s/batch\n",
            "2022-11-20 13:12:20,439 [INFO ]  epoch:179 step:0000  current_lr: 0.000090  l1_loss:0.013662 flow_loss:0.035570 ter_loss:0.010362 loss_sum:0.059594 0.279324s/batch\n",
            "INFO:root:epoch:180 step:0000  current_lr: 0.000090  l1_loss:0.015694 flow_loss:0.031331 ter_loss:0.013643 loss_sum:0.060669 0.280219s/batch\n",
            "2022-11-20 13:12:33,238 [INFO ]  epoch:180 step:0000  current_lr: 0.000090  l1_loss:0.015694 flow_loss:0.031331 ter_loss:0.013643 loss_sum:0.060669 0.280219s/batch\n",
            "INFO:root:Saving state, epoch: 180 iter:0\n",
            "2022-11-20 13:12:43,204 [INFO ]  Saving state, epoch: 180 iter:0\n",
            "INFO:root:epoch:181 step:0000  current_lr: 0.000091  l1_loss:0.015742 flow_loss:0.037506 ter_loss:0.012550 loss_sum:0.065797 0.280740s/batch\n",
            "2022-11-20 13:12:47,131 [INFO ]  epoch:181 step:0000  current_lr: 0.000091  l1_loss:0.015742 flow_loss:0.037506 ter_loss:0.012550 loss_sum:0.065797 0.280740s/batch\n",
            "INFO:root:epoch:182 step:0000  current_lr: 0.000091  l1_loss:0.015171 flow_loss:0.035678 ter_loss:0.012287 loss_sum:0.063136 0.288252s/batch\n",
            "2022-11-20 13:13:00,014 [INFO ]  epoch:182 step:0000  current_lr: 0.000091  l1_loss:0.015171 flow_loss:0.035678 ter_loss:0.012287 loss_sum:0.063136 0.288252s/batch\n",
            "INFO:root:epoch:183 step:0000  current_lr: 0.000092  l1_loss:0.013301 flow_loss:0.034956 ter_loss:0.010100 loss_sum:0.058357 0.282250s/batch\n",
            "2022-11-20 13:13:12,821 [INFO ]  epoch:183 step:0000  current_lr: 0.000092  l1_loss:0.013301 flow_loss:0.034956 ter_loss:0.010100 loss_sum:0.058357 0.282250s/batch\n",
            "INFO:root:epoch:184 step:0000  current_lr: 0.000092  l1_loss:0.014614 flow_loss:0.042077 ter_loss:0.011466 loss_sum:0.068156 0.283993s/batch\n",
            "2022-11-20 13:13:25,632 [INFO ]  epoch:184 step:0000  current_lr: 0.000092  l1_loss:0.014614 flow_loss:0.042077 ter_loss:0.011466 loss_sum:0.068156 0.283993s/batch\n",
            "INFO:root:epoch:185 step:0000  current_lr: 0.000093  l1_loss:0.012405 flow_loss:0.032056 ter_loss:0.008857 loss_sum:0.053318 0.281506s/batch\n",
            "2022-11-20 13:13:38,442 [INFO ]  epoch:185 step:0000  current_lr: 0.000093  l1_loss:0.012405 flow_loss:0.032056 ter_loss:0.008857 loss_sum:0.053318 0.281506s/batch\n",
            "INFO:root:epoch:186 step:0000  current_lr: 0.000093  l1_loss:0.013526 flow_loss:0.033894 ter_loss:0.010786 loss_sum:0.058206 0.280442s/batch\n",
            "2022-11-20 13:13:51,251 [INFO ]  epoch:186 step:0000  current_lr: 0.000093  l1_loss:0.013526 flow_loss:0.033894 ter_loss:0.010786 loss_sum:0.058206 0.280442s/batch\n",
            "INFO:root:epoch:187 step:0000  current_lr: 0.000094  l1_loss:0.013844 flow_loss:0.027213 ter_loss:0.011744 loss_sum:0.052801 0.282971s/batch\n",
            "2022-11-20 13:14:04,055 [INFO ]  epoch:187 step:0000  current_lr: 0.000094  l1_loss:0.013844 flow_loss:0.027213 ter_loss:0.011744 loss_sum:0.052801 0.282971s/batch\n",
            "INFO:root:epoch:188 step:0000  current_lr: 0.000094  l1_loss:0.013921 flow_loss:0.034911 ter_loss:0.010652 loss_sum:0.059484 0.279319s/batch\n",
            "2022-11-20 13:14:16,820 [INFO ]  epoch:188 step:0000  current_lr: 0.000094  l1_loss:0.013921 flow_loss:0.034911 ter_loss:0.010652 loss_sum:0.059484 0.279319s/batch\n",
            "INFO:root:epoch:189 step:0000  current_lr: 0.000094  l1_loss:0.013042 flow_loss:0.036524 ter_loss:0.009461 loss_sum:0.059027 0.282479s/batch\n",
            "2022-11-20 13:14:29,629 [INFO ]  epoch:189 step:0000  current_lr: 0.000094  l1_loss:0.013042 flow_loss:0.036524 ter_loss:0.009461 loss_sum:0.059027 0.282479s/batch\n",
            "INFO:root:epoch:190 step:0000  current_lr: 0.000095  l1_loss:0.013498 flow_loss:0.029542 ter_loss:0.011156 loss_sum:0.054196 0.282115s/batch\n",
            "2022-11-20 13:14:42,416 [INFO ]  epoch:190 step:0000  current_lr: 0.000095  l1_loss:0.013498 flow_loss:0.029542 ter_loss:0.011156 loss_sum:0.054196 0.282115s/batch\n",
            "INFO:root:epoch:191 step:0000  current_lr: 0.000096  l1_loss:0.014148 flow_loss:0.032178 ter_loss:0.011210 loss_sum:0.057535 0.281967s/batch\n",
            "2022-11-20 13:14:55,203 [INFO ]  epoch:191 step:0000  current_lr: 0.000096  l1_loss:0.014148 flow_loss:0.032178 ter_loss:0.011210 loss_sum:0.057535 0.281967s/batch\n",
            "INFO:root:epoch:192 step:0000  current_lr: 0.000096  l1_loss:0.017004 flow_loss:0.034562 ter_loss:0.014077 loss_sum:0.065643 0.281734s/batch\n",
            "2022-11-20 13:15:08,053 [INFO ]  epoch:192 step:0000  current_lr: 0.000096  l1_loss:0.017004 flow_loss:0.034562 ter_loss:0.014077 loss_sum:0.065643 0.281734s/batch\n",
            "INFO:root:epoch:193 step:0000  current_lr: 0.000097  l1_loss:0.014527 flow_loss:0.032332 ter_loss:0.011652 loss_sum:0.058511 0.278313s/batch\n",
            "2022-11-20 13:15:20,794 [INFO ]  epoch:193 step:0000  current_lr: 0.000097  l1_loss:0.014527 flow_loss:0.032332 ter_loss:0.011652 loss_sum:0.058511 0.278313s/batch\n",
            "INFO:root:epoch:194 step:0000  current_lr: 0.000097  l1_loss:0.012106 flow_loss:0.032938 ter_loss:0.008718 loss_sum:0.053762 0.286263s/batch\n",
            "2022-11-20 13:15:33,638 [INFO ]  epoch:194 step:0000  current_lr: 0.000097  l1_loss:0.012106 flow_loss:0.032938 ter_loss:0.008718 loss_sum:0.053762 0.286263s/batch\n",
            "INFO:root:epoch:195 step:0000  current_lr: 0.000097  l1_loss:0.014841 flow_loss:0.032775 ter_loss:0.011500 loss_sum:0.059117 0.284109s/batch\n",
            "2022-11-20 13:15:46,462 [INFO ]  epoch:195 step:0000  current_lr: 0.000097  l1_loss:0.014841 flow_loss:0.032775 ter_loss:0.011500 loss_sum:0.059117 0.284109s/batch\n",
            "INFO:root:epoch:196 step:0000  current_lr: 0.000098  l1_loss:0.014228 flow_loss:0.031803 ter_loss:0.011099 loss_sum:0.057131 0.283070s/batch\n",
            "2022-11-20 13:15:59,263 [INFO ]  epoch:196 step:0000  current_lr: 0.000098  l1_loss:0.014228 flow_loss:0.031803 ter_loss:0.011099 loss_sum:0.057131 0.283070s/batch\n",
            "INFO:root:epoch:197 step:0000  current_lr: 0.000099  l1_loss:0.012208 flow_loss:0.029542 ter_loss:0.009539 loss_sum:0.051289 0.279220s/batch\n",
            "2022-11-20 13:16:12,029 [INFO ]  epoch:197 step:0000  current_lr: 0.000099  l1_loss:0.012208 flow_loss:0.029542 ter_loss:0.009539 loss_sum:0.051289 0.279220s/batch\n",
            "INFO:root:epoch:198 step:0000  current_lr: 0.000099  l1_loss:0.014657 flow_loss:0.031846 ter_loss:0.011076 loss_sum:0.057578 0.280197s/batch\n",
            "2022-11-20 13:16:24,814 [INFO ]  epoch:198 step:0000  current_lr: 0.000099  l1_loss:0.014657 flow_loss:0.031846 ter_loss:0.011076 loss_sum:0.057578 0.280197s/batch\n",
            "INFO:root:epoch:199 step:0000  current_lr: 0.000100  l1_loss:0.012875 flow_loss:0.034259 ter_loss:0.009842 loss_sum:0.056976 0.282109s/batch\n",
            "2022-11-20 13:16:37,601 [INFO ]  epoch:199 step:0000  current_lr: 0.000100  l1_loss:0.012875 flow_loss:0.034259 ter_loss:0.009842 loss_sum:0.056976 0.282109s/batch\n",
            "INFO:root:epoch:200 step:0000  current_lr: 0.000100  l1_loss:0.013755 flow_loss:0.033054 ter_loss:0.010906 loss_sum:0.057715 0.279402s/batch\n",
            "2022-11-20 13:16:50,365 [INFO ]  epoch:200 step:0000  current_lr: 0.000100  l1_loss:0.013755 flow_loss:0.033054 ter_loss:0.010906 loss_sum:0.057715 0.279402s/batch\n",
            "INFO:root:Saving state, epoch: 200 iter:0\n",
            "2022-11-20 13:17:00,402 [INFO ]  Saving state, epoch: 200 iter:0\n",
            "INFO:root:epoch:201 step:0000  current_lr: 0.000100  l1_loss:0.012338 flow_loss:0.029186 ter_loss:0.008932 loss_sum:0.050456 0.282941s/batch\n",
            "2022-11-20 13:17:04,422 [INFO ]  epoch:201 step:0000  current_lr: 0.000100  l1_loss:0.012338 flow_loss:0.029186 ter_loss:0.008932 loss_sum:0.050456 0.282941s/batch\n",
            "INFO:root:epoch:202 step:0000  current_lr: 0.000100  l1_loss:0.013297 flow_loss:0.034828 ter_loss:0.010272 loss_sum:0.058397 0.285227s/batch\n",
            "2022-11-20 13:17:17,298 [INFO ]  epoch:202 step:0000  current_lr: 0.000100  l1_loss:0.013297 flow_loss:0.034828 ter_loss:0.010272 loss_sum:0.058397 0.285227s/batch\n",
            "INFO:root:epoch:203 step:0000  current_lr: 0.000100  l1_loss:0.013107 flow_loss:0.026722 ter_loss:0.010135 loss_sum:0.049964 0.283009s/batch\n",
            "2022-11-20 13:17:30,118 [INFO ]  epoch:203 step:0000  current_lr: 0.000100  l1_loss:0.013107 flow_loss:0.026722 ter_loss:0.010135 loss_sum:0.049964 0.283009s/batch\n",
            "INFO:root:epoch:204 step:0000  current_lr: 0.000100  l1_loss:0.015678 flow_loss:0.038021 ter_loss:0.012209 loss_sum:0.065909 0.279367s/batch\n",
            "2022-11-20 13:17:42,952 [INFO ]  epoch:204 step:0000  current_lr: 0.000100  l1_loss:0.015678 flow_loss:0.038021 ter_loss:0.012209 loss_sum:0.065909 0.279367s/batch\n",
            "INFO:root:epoch:205 step:0000  current_lr: 0.000099  l1_loss:0.011924 flow_loss:0.031619 ter_loss:0.008640 loss_sum:0.052183 0.282991s/batch\n",
            "2022-11-20 13:17:55,785 [INFO ]  epoch:205 step:0000  current_lr: 0.000099  l1_loss:0.011924 flow_loss:0.031619 ter_loss:0.008640 loss_sum:0.052183 0.282991s/batch\n",
            "INFO:root:epoch:206 step:0000  current_lr: 0.000099  l1_loss:0.012838 flow_loss:0.039995 ter_loss:0.009571 loss_sum:0.062404 0.279914s/batch\n",
            "2022-11-20 13:18:08,567 [INFO ]  epoch:206 step:0000  current_lr: 0.000099  l1_loss:0.012838 flow_loss:0.039995 ter_loss:0.009571 loss_sum:0.062404 0.279914s/batch\n",
            "INFO:root:epoch:207 step:0000  current_lr: 0.000099  l1_loss:0.012602 flow_loss:0.031523 ter_loss:0.009760 loss_sum:0.053886 0.280664s/batch\n",
            "2022-11-20 13:18:21,405 [INFO ]  epoch:207 step:0000  current_lr: 0.000099  l1_loss:0.012602 flow_loss:0.031523 ter_loss:0.009760 loss_sum:0.053886 0.280664s/batch\n",
            "INFO:root:epoch:208 step:0000  current_lr: 0.000098  l1_loss:0.012632 flow_loss:0.030471 ter_loss:0.009356 loss_sum:0.052460 0.281448s/batch\n",
            "2022-11-20 13:18:34,215 [INFO ]  epoch:208 step:0000  current_lr: 0.000098  l1_loss:0.012632 flow_loss:0.030471 ter_loss:0.009356 loss_sum:0.052460 0.281448s/batch\n",
            "INFO:root:epoch:209 step:0000  current_lr: 0.000098  l1_loss:0.013876 flow_loss:0.030022 ter_loss:0.011331 loss_sum:0.055228 0.285085s/batch\n",
            "2022-11-20 13:18:47,069 [INFO ]  epoch:209 step:0000  current_lr: 0.000098  l1_loss:0.013876 flow_loss:0.030022 ter_loss:0.011331 loss_sum:0.055228 0.285085s/batch\n",
            "INFO:root:epoch:210 step:0000  current_lr: 0.000098  l1_loss:0.013739 flow_loss:0.030107 ter_loss:0.011554 loss_sum:0.055400 0.281252s/batch\n",
            "2022-11-20 13:18:59,877 [INFO ]  epoch:210 step:0000  current_lr: 0.000098  l1_loss:0.013739 flow_loss:0.030107 ter_loss:0.011554 loss_sum:0.055400 0.281252s/batch\n",
            "INFO:root:epoch:211 step:0000  current_lr: 0.000097  l1_loss:0.010543 flow_loss:0.024133 ter_loss:0.007280 loss_sum:0.041956 0.282697s/batch\n",
            "2022-11-20 13:19:12,706 [INFO ]  epoch:211 step:0000  current_lr: 0.000097  l1_loss:0.010543 flow_loss:0.024133 ter_loss:0.007280 loss_sum:0.041956 0.282697s/batch\n",
            "INFO:root:epoch:212 step:0000  current_lr: 0.000096  l1_loss:0.013290 flow_loss:0.032427 ter_loss:0.009866 loss_sum:0.055583 0.281565s/batch\n",
            "2022-11-20 13:19:25,518 [INFO ]  epoch:212 step:0000  current_lr: 0.000096  l1_loss:0.013290 flow_loss:0.032427 ter_loss:0.009866 loss_sum:0.055583 0.281565s/batch\n",
            "INFO:root:epoch:213 step:0000  current_lr: 0.000096  l1_loss:0.013309 flow_loss:0.028946 ter_loss:0.010271 loss_sum:0.052526 0.282987s/batch\n",
            "2022-11-20 13:19:38,344 [INFO ]  epoch:213 step:0000  current_lr: 0.000096  l1_loss:0.013309 flow_loss:0.028946 ter_loss:0.010271 loss_sum:0.052526 0.282987s/batch\n",
            "INFO:root:epoch:214 step:0000  current_lr: 0.000095  l1_loss:0.015162 flow_loss:0.030581 ter_loss:0.012964 loss_sum:0.058708 0.281054s/batch\n",
            "2022-11-20 13:19:51,166 [INFO ]  epoch:214 step:0000  current_lr: 0.000095  l1_loss:0.015162 flow_loss:0.030581 ter_loss:0.012964 loss_sum:0.058708 0.281054s/batch\n",
            "INFO:root:epoch:215 step:0000  current_lr: 0.000095  l1_loss:0.012630 flow_loss:0.023290 ter_loss:0.009238 loss_sum:0.045158 0.285966s/batch\n",
            "2022-11-20 13:20:04,015 [INFO ]  epoch:215 step:0000  current_lr: 0.000095  l1_loss:0.012630 flow_loss:0.023290 ter_loss:0.009238 loss_sum:0.045158 0.285966s/batch\n",
            "INFO:root:epoch:216 step:0000  current_lr: 0.000094  l1_loss:0.012450 flow_loss:0.029361 ter_loss:0.009016 loss_sum:0.050828 0.284246s/batch\n",
            "2022-11-20 13:20:16,854 [INFO ]  epoch:216 step:0000  current_lr: 0.000094  l1_loss:0.012450 flow_loss:0.029361 ter_loss:0.009016 loss_sum:0.050828 0.284246s/batch\n",
            "INFO:root:epoch:217 step:0000  current_lr: 0.000093  l1_loss:0.011916 flow_loss:0.025685 ter_loss:0.008906 loss_sum:0.046507 0.283898s/batch\n",
            "2022-11-20 13:20:29,726 [INFO ]  epoch:217 step:0000  current_lr: 0.000093  l1_loss:0.011916 flow_loss:0.025685 ter_loss:0.008906 loss_sum:0.046507 0.283898s/batch\n",
            "INFO:root:epoch:218 step:0000  current_lr: 0.000092  l1_loss:0.012868 flow_loss:0.026783 ter_loss:0.009963 loss_sum:0.049614 0.282845s/batch\n",
            "2022-11-20 13:20:42,560 [INFO ]  epoch:218 step:0000  current_lr: 0.000092  l1_loss:0.012868 flow_loss:0.026783 ter_loss:0.009963 loss_sum:0.049614 0.282845s/batch\n",
            "INFO:root:epoch:219 step:0000  current_lr: 0.000091  l1_loss:0.012844 flow_loss:0.025367 ter_loss:0.009988 loss_sum:0.048200 0.283217s/batch\n",
            "2022-11-20 13:20:55,387 [INFO ]  epoch:219 step:0000  current_lr: 0.000091  l1_loss:0.012844 flow_loss:0.025367 ter_loss:0.009988 loss_sum:0.048200 0.283217s/batch\n",
            "INFO:root:epoch:220 step:0000  current_lr: 0.000090  l1_loss:0.012208 flow_loss:0.030676 ter_loss:0.008783 loss_sum:0.051666 0.280735s/batch\n",
            "2022-11-20 13:21:08,217 [INFO ]  epoch:220 step:0000  current_lr: 0.000090  l1_loss:0.012208 flow_loss:0.030676 ter_loss:0.008783 loss_sum:0.051666 0.280735s/batch\n",
            "INFO:root:Saving state, epoch: 220 iter:0\n",
            "2022-11-20 13:21:18,235 [INFO ]  Saving state, epoch: 220 iter:0\n",
            "INFO:root:epoch:221 step:0000  current_lr: 0.000090  l1_loss:0.011934 flow_loss:0.025615 ter_loss:0.009505 loss_sum:0.047054 0.307055s/batch\n",
            "2022-11-20 13:21:22,427 [INFO ]  epoch:221 step:0000  current_lr: 0.000090  l1_loss:0.011934 flow_loss:0.025615 ter_loss:0.009505 loss_sum:0.047054 0.307055s/batch\n",
            "INFO:root:epoch:222 step:0000  current_lr: 0.000089  l1_loss:0.014883 flow_loss:0.032144 ter_loss:0.011819 loss_sum:0.058846 0.281863s/batch\n",
            "2022-11-20 13:21:35,242 [INFO ]  epoch:222 step:0000  current_lr: 0.000089  l1_loss:0.014883 flow_loss:0.032144 ter_loss:0.011819 loss_sum:0.058846 0.281863s/batch\n",
            "INFO:root:epoch:223 step:0000  current_lr: 0.000088  l1_loss:0.013013 flow_loss:0.034202 ter_loss:0.009910 loss_sum:0.057126 0.287000s/batch\n",
            "2022-11-20 13:21:48,106 [INFO ]  epoch:223 step:0000  current_lr: 0.000088  l1_loss:0.013013 flow_loss:0.034202 ter_loss:0.009910 loss_sum:0.057126 0.287000s/batch\n",
            "INFO:root:epoch:224 step:0000  current_lr: 0.000086  l1_loss:0.015560 flow_loss:0.032398 ter_loss:0.013103 loss_sum:0.061061 0.281687s/batch\n",
            "2022-11-20 13:22:00,945 [INFO ]  epoch:224 step:0000  current_lr: 0.000086  l1_loss:0.015560 flow_loss:0.032398 ter_loss:0.013103 loss_sum:0.061061 0.281687s/batch\n",
            "INFO:root:epoch:225 step:0000  current_lr: 0.000085  l1_loss:0.010937 flow_loss:0.027370 ter_loss:0.008043 loss_sum:0.046350 0.283534s/batch\n",
            "2022-11-20 13:22:13,781 [INFO ]  epoch:225 step:0000  current_lr: 0.000085  l1_loss:0.010937 flow_loss:0.027370 ter_loss:0.008043 loss_sum:0.046350 0.283534s/batch\n",
            "INFO:root:epoch:226 step:0000  current_lr: 0.000084  l1_loss:0.011248 flow_loss:0.024476 ter_loss:0.008117 loss_sum:0.043841 0.283071s/batch\n",
            "2022-11-20 13:22:26,595 [INFO ]  epoch:226 step:0000  current_lr: 0.000084  l1_loss:0.011248 flow_loss:0.024476 ter_loss:0.008117 loss_sum:0.043841 0.283071s/batch\n",
            "INFO:root:epoch:227 step:0000  current_lr: 0.000083  l1_loss:0.010015 flow_loss:0.027211 ter_loss:0.006591 loss_sum:0.043816 0.284997s/batch\n",
            "2022-11-20 13:22:39,470 [INFO ]  epoch:227 step:0000  current_lr: 0.000083  l1_loss:0.010015 flow_loss:0.027211 ter_loss:0.006591 loss_sum:0.043816 0.284997s/batch\n",
            "INFO:root:epoch:228 step:0000  current_lr: 0.000082  l1_loss:0.012085 flow_loss:0.028857 ter_loss:0.008681 loss_sum:0.049623 0.284061s/batch\n",
            "2022-11-20 13:22:52,300 [INFO ]  epoch:228 step:0000  current_lr: 0.000082  l1_loss:0.012085 flow_loss:0.028857 ter_loss:0.008681 loss_sum:0.049623 0.284061s/batch\n",
            "INFO:root:epoch:229 step:0000  current_lr: 0.000081  l1_loss:0.014251 flow_loss:0.034508 ter_loss:0.010815 loss_sum:0.059573 0.282711s/batch\n",
            "2022-11-20 13:23:05,106 [INFO ]  epoch:229 step:0000  current_lr: 0.000081  l1_loss:0.014251 flow_loss:0.034508 ter_loss:0.010815 loss_sum:0.059573 0.282711s/batch\n",
            "INFO:root:epoch:230 step:0000  current_lr: 0.000079  l1_loss:0.012549 flow_loss:0.026703 ter_loss:0.009738 loss_sum:0.048989 0.300438s/batch\n",
            "2022-11-20 13:23:18,133 [INFO ]  epoch:230 step:0000  current_lr: 0.000079  l1_loss:0.012549 flow_loss:0.026703 ter_loss:0.009738 loss_sum:0.048989 0.300438s/batch\n",
            "INFO:root:epoch:231 step:0000  current_lr: 0.000078  l1_loss:0.012490 flow_loss:0.025242 ter_loss:0.009112 loss_sum:0.046845 0.280640s/batch\n",
            "2022-11-20 13:23:30,942 [INFO ]  epoch:231 step:0000  current_lr: 0.000078  l1_loss:0.012490 flow_loss:0.025242 ter_loss:0.009112 loss_sum:0.046845 0.280640s/batch\n",
            "INFO:root:epoch:232 step:0000  current_lr: 0.000077  l1_loss:0.010546 flow_loss:0.032805 ter_loss:0.007265 loss_sum:0.050616 0.279546s/batch\n",
            "2022-11-20 13:23:43,752 [INFO ]  epoch:232 step:0000  current_lr: 0.000077  l1_loss:0.010546 flow_loss:0.032805 ter_loss:0.007265 loss_sum:0.050616 0.279546s/batch\n",
            "INFO:root:epoch:233 step:0000  current_lr: 0.000075  l1_loss:0.011845 flow_loss:0.028890 ter_loss:0.009096 loss_sum:0.049831 0.284132s/batch\n",
            "2022-11-20 13:23:56,588 [INFO ]  epoch:233 step:0000  current_lr: 0.000075  l1_loss:0.011845 flow_loss:0.028890 ter_loss:0.009096 loss_sum:0.049831 0.284132s/batch\n",
            "INFO:root:epoch:234 step:0000  current_lr: 0.000074  l1_loss:0.011695 flow_loss:0.031955 ter_loss:0.008145 loss_sum:0.051794 0.280232s/batch\n",
            "2022-11-20 13:24:09,390 [INFO ]  epoch:234 step:0000  current_lr: 0.000074  l1_loss:0.011695 flow_loss:0.031955 ter_loss:0.008145 loss_sum:0.051794 0.280232s/batch\n",
            "INFO:root:epoch:235 step:0000  current_lr: 0.000073  l1_loss:0.011139 flow_loss:0.030962 ter_loss:0.007835 loss_sum:0.049936 0.283774s/batch\n",
            "2022-11-20 13:24:22,235 [INFO ]  epoch:235 step:0000  current_lr: 0.000073  l1_loss:0.011139 flow_loss:0.030962 ter_loss:0.007835 loss_sum:0.049936 0.283774s/batch\n",
            "INFO:root:epoch:236 step:0000  current_lr: 0.000071  l1_loss:0.011488 flow_loss:0.036454 ter_loss:0.008408 loss_sum:0.056350 0.278786s/batch\n",
            "2022-11-20 13:24:35,020 [INFO ]  epoch:236 step:0000  current_lr: 0.000071  l1_loss:0.011488 flow_loss:0.036454 ter_loss:0.008408 loss_sum:0.056350 0.278786s/batch\n",
            "INFO:root:epoch:237 step:0000  current_lr: 0.000070  l1_loss:0.012125 flow_loss:0.030194 ter_loss:0.008484 loss_sum:0.050803 0.281283s/batch\n",
            "2022-11-20 13:24:47,870 [INFO ]  epoch:237 step:0000  current_lr: 0.000070  l1_loss:0.012125 flow_loss:0.030194 ter_loss:0.008484 loss_sum:0.050803 0.281283s/batch\n",
            "INFO:root:epoch:238 step:0000  current_lr: 0.000068  l1_loss:0.012107 flow_loss:0.033359 ter_loss:0.008886 loss_sum:0.054352 0.282903s/batch\n",
            "2022-11-20 13:25:00,693 [INFO ]  epoch:238 step:0000  current_lr: 0.000068  l1_loss:0.012107 flow_loss:0.033359 ter_loss:0.008886 loss_sum:0.054352 0.282903s/batch\n",
            "INFO:root:epoch:239 step:0000  current_lr: 0.000067  l1_loss:0.011759 flow_loss:0.028069 ter_loss:0.008945 loss_sum:0.048773 0.281996s/batch\n",
            "2022-11-20 13:25:13,509 [INFO ]  epoch:239 step:0000  current_lr: 0.000067  l1_loss:0.011759 flow_loss:0.028069 ter_loss:0.008945 loss_sum:0.048773 0.281996s/batch\n",
            "INFO:root:epoch:240 step:0000  current_lr: 0.000065  l1_loss:0.012268 flow_loss:0.028823 ter_loss:0.008697 loss_sum:0.049788 0.280847s/batch\n",
            "2022-11-20 13:25:26,353 [INFO ]  epoch:240 step:0000  current_lr: 0.000065  l1_loss:0.012268 flow_loss:0.028823 ter_loss:0.008697 loss_sum:0.049788 0.280847s/batch\n",
            "INFO:root:Saving state, epoch: 240 iter:0\n",
            "2022-11-20 13:25:36,350 [INFO ]  Saving state, epoch: 240 iter:0\n",
            "INFO:root:epoch:241 step:0000  current_lr: 0.000064  l1_loss:0.011359 flow_loss:0.023768 ter_loss:0.008027 loss_sum:0.043154 0.282459s/batch\n",
            "2022-11-20 13:25:40,287 [INFO ]  epoch:241 step:0000  current_lr: 0.000064  l1_loss:0.011359 flow_loss:0.023768 ter_loss:0.008027 loss_sum:0.043154 0.282459s/batch\n",
            "INFO:root:epoch:242 step:0000  current_lr: 0.000062  l1_loss:0.013435 flow_loss:0.026841 ter_loss:0.010100 loss_sum:0.050376 0.281975s/batch\n",
            "2022-11-20 13:25:54,173 [INFO ]  epoch:242 step:0000  current_lr: 0.000062  l1_loss:0.013435 flow_loss:0.026841 ter_loss:0.010100 loss_sum:0.050376 0.281975s/batch\n",
            "INFO:root:epoch:243 step:0000  current_lr: 0.000061  l1_loss:0.010542 flow_loss:0.025887 ter_loss:0.006821 loss_sum:0.043250 0.283979s/batch\n",
            "2022-11-20 13:26:07,000 [INFO ]  epoch:243 step:0000  current_lr: 0.000061  l1_loss:0.010542 flow_loss:0.025887 ter_loss:0.006821 loss_sum:0.043250 0.283979s/batch\n",
            "INFO:root:epoch:244 step:0000  current_lr: 0.000059  l1_loss:0.011224 flow_loss:0.026007 ter_loss:0.007942 loss_sum:0.045172 0.282539s/batch\n",
            "2022-11-20 13:26:19,823 [INFO ]  epoch:244 step:0000  current_lr: 0.000059  l1_loss:0.011224 flow_loss:0.026007 ter_loss:0.007942 loss_sum:0.045172 0.282539s/batch\n",
            "INFO:root:epoch:245 step:0000  current_lr: 0.000058  l1_loss:0.012263 flow_loss:0.031528 ter_loss:0.008764 loss_sum:0.052555 0.283443s/batch\n",
            "2022-11-20 13:26:32,660 [INFO ]  epoch:245 step:0000  current_lr: 0.000058  l1_loss:0.012263 flow_loss:0.031528 ter_loss:0.008764 loss_sum:0.052555 0.283443s/batch\n",
            "INFO:root:epoch:246 step:0000  current_lr: 0.000056  l1_loss:0.009461 flow_loss:0.028621 ter_loss:0.006372 loss_sum:0.044454 0.280925s/batch\n",
            "2022-11-20 13:26:45,470 [INFO ]  epoch:246 step:0000  current_lr: 0.000056  l1_loss:0.009461 flow_loss:0.028621 ter_loss:0.006372 loss_sum:0.044454 0.280925s/batch\n",
            "INFO:root:epoch:247 step:0000  current_lr: 0.000055  l1_loss:0.010889 flow_loss:0.025235 ter_loss:0.007784 loss_sum:0.043908 0.280817s/batch\n",
            "2022-11-20 13:26:58,275 [INFO ]  epoch:247 step:0000  current_lr: 0.000055  l1_loss:0.010889 flow_loss:0.025235 ter_loss:0.007784 loss_sum:0.043908 0.280817s/batch\n",
            "INFO:root:epoch:248 step:0000  current_lr: 0.000053  l1_loss:0.009527 flow_loss:0.022700 ter_loss:0.006588 loss_sum:0.038815 0.280208s/batch\n",
            "2022-11-20 13:27:11,068 [INFO ]  epoch:248 step:0000  current_lr: 0.000053  l1_loss:0.009527 flow_loss:0.022700 ter_loss:0.006588 loss_sum:0.038815 0.280208s/batch\n",
            "INFO:root:epoch:249 step:0000  current_lr: 0.000052  l1_loss:0.011015 flow_loss:0.024087 ter_loss:0.008396 loss_sum:0.043499 0.283316s/batch\n",
            "2022-11-20 13:27:23,897 [INFO ]  epoch:249 step:0000  current_lr: 0.000052  l1_loss:0.011015 flow_loss:0.024087 ter_loss:0.008396 loss_sum:0.043499 0.283316s/batch\n",
            "INFO:root:epoch:250 step:0000  current_lr: 0.000050  l1_loss:0.011278 flow_loss:0.027239 ter_loss:0.007779 loss_sum:0.046296 0.281778s/batch\n",
            "2022-11-20 13:27:36,711 [INFO ]  epoch:250 step:0000  current_lr: 0.000050  l1_loss:0.011278 flow_loss:0.027239 ter_loss:0.007779 loss_sum:0.046296 0.281778s/batch\n",
            "INFO:root:epoch:251 step:0000  current_lr: 0.000048  l1_loss:0.010705 flow_loss:0.024106 ter_loss:0.007519 loss_sum:0.042330 0.281406s/batch\n",
            "2022-11-20 13:27:49,549 [INFO ]  epoch:251 step:0000  current_lr: 0.000048  l1_loss:0.010705 flow_loss:0.024106 ter_loss:0.007519 loss_sum:0.042330 0.281406s/batch\n",
            "INFO:root:epoch:252 step:0000  current_lr: 0.000047  l1_loss:0.010870 flow_loss:0.026263 ter_loss:0.007899 loss_sum:0.045031 0.281392s/batch\n",
            "2022-11-20 13:28:02,381 [INFO ]  epoch:252 step:0000  current_lr: 0.000047  l1_loss:0.010870 flow_loss:0.026263 ter_loss:0.007899 loss_sum:0.045031 0.281392s/batch\n",
            "INFO:root:epoch:253 step:0000  current_lr: 0.000045  l1_loss:0.010474 flow_loss:0.026423 ter_loss:0.006718 loss_sum:0.043615 0.283980s/batch\n",
            "2022-11-20 13:28:15,215 [INFO ]  epoch:253 step:0000  current_lr: 0.000045  l1_loss:0.010474 flow_loss:0.026423 ter_loss:0.006718 loss_sum:0.043615 0.283980s/batch\n",
            "INFO:root:epoch:254 step:0000  current_lr: 0.000044  l1_loss:0.010019 flow_loss:0.028481 ter_loss:0.006868 loss_sum:0.045368 0.284969s/batch\n",
            "2022-11-20 13:28:28,055 [INFO ]  epoch:254 step:0000  current_lr: 0.000044  l1_loss:0.010019 flow_loss:0.028481 ter_loss:0.006868 loss_sum:0.045368 0.284969s/batch\n",
            "INFO:root:epoch:255 step:0000  current_lr: 0.000042  l1_loss:0.011436 flow_loss:0.028807 ter_loss:0.007790 loss_sum:0.048033 0.278802s/batch\n",
            "2022-11-20 13:28:40,849 [INFO ]  epoch:255 step:0000  current_lr: 0.000042  l1_loss:0.011436 flow_loss:0.028807 ter_loss:0.007790 loss_sum:0.048033 0.278802s/batch\n",
            "INFO:root:epoch:256 step:0000  current_lr: 0.000041  l1_loss:0.011310 flow_loss:0.024864 ter_loss:0.007608 loss_sum:0.043782 0.281975s/batch\n",
            "2022-11-20 13:28:53,659 [INFO ]  epoch:256 step:0000  current_lr: 0.000041  l1_loss:0.011310 flow_loss:0.024864 ter_loss:0.007608 loss_sum:0.043782 0.281975s/batch\n",
            "INFO:root:epoch:257 step:0000  current_lr: 0.000039  l1_loss:0.011624 flow_loss:0.030596 ter_loss:0.008233 loss_sum:0.050453 0.289645s/batch\n",
            "2022-11-20 13:29:06,555 [INFO ]  epoch:257 step:0000  current_lr: 0.000039  l1_loss:0.011624 flow_loss:0.030596 ter_loss:0.008233 loss_sum:0.050453 0.289645s/batch\n",
            "INFO:root:epoch:258 step:0000  current_lr: 0.000038  l1_loss:0.011967 flow_loss:0.029653 ter_loss:0.008450 loss_sum:0.050071 0.278773s/batch\n",
            "2022-11-20 13:29:19,354 [INFO ]  epoch:258 step:0000  current_lr: 0.000038  l1_loss:0.011967 flow_loss:0.029653 ter_loss:0.008450 loss_sum:0.050071 0.278773s/batch\n",
            "INFO:root:epoch:259 step:0000  current_lr: 0.000036  l1_loss:0.010874 flow_loss:0.024650 ter_loss:0.008085 loss_sum:0.043609 0.287263s/batch\n",
            "2022-11-20 13:29:32,235 [INFO ]  epoch:259 step:0000  current_lr: 0.000036  l1_loss:0.010874 flow_loss:0.024650 ter_loss:0.008085 loss_sum:0.043609 0.287263s/batch\n",
            "INFO:root:epoch:260 step:0000  current_lr: 0.000035  l1_loss:0.011757 flow_loss:0.027871 ter_loss:0.008378 loss_sum:0.048006 0.282048s/batch\n",
            "2022-11-20 13:29:45,062 [INFO ]  epoch:260 step:0000  current_lr: 0.000035  l1_loss:0.011757 flow_loss:0.027871 ter_loss:0.008378 loss_sum:0.048006 0.282048s/batch\n",
            "INFO:root:Saving state, epoch: 260 iter:0\n",
            "2022-11-20 13:29:55,054 [INFO ]  Saving state, epoch: 260 iter:0\n",
            "INFO:root:epoch:261 step:0000  current_lr: 0.000033  l1_loss:0.010225 flow_loss:0.021152 ter_loss:0.007465 loss_sum:0.038842 0.283345s/batch\n",
            "2022-11-20 13:29:59,005 [INFO ]  epoch:261 step:0000  current_lr: 0.000033  l1_loss:0.010225 flow_loss:0.021152 ter_loss:0.007465 loss_sum:0.038842 0.283345s/batch\n",
            "INFO:root:epoch:262 step:0000  current_lr: 0.000032  l1_loss:0.012185 flow_loss:0.021588 ter_loss:0.009944 loss_sum:0.043717 0.281929s/batch\n",
            "2022-11-20 13:30:11,854 [INFO ]  epoch:262 step:0000  current_lr: 0.000032  l1_loss:0.012185 flow_loss:0.021588 ter_loss:0.009944 loss_sum:0.043717 0.281929s/batch\n",
            "INFO:root:epoch:263 step:0000  current_lr: 0.000030  l1_loss:0.010898 flow_loss:0.025357 ter_loss:0.007527 loss_sum:0.043782 0.283528s/batch\n",
            "2022-11-20 13:30:24,692 [INFO ]  epoch:263 step:0000  current_lr: 0.000030  l1_loss:0.010898 flow_loss:0.025357 ter_loss:0.007527 loss_sum:0.043782 0.283528s/batch\n",
            "INFO:root:epoch:264 step:0000  current_lr: 0.000029  l1_loss:0.009765 flow_loss:0.021345 ter_loss:0.006918 loss_sum:0.038028 0.282719s/batch\n",
            "2022-11-20 13:30:37,531 [INFO ]  epoch:264 step:0000  current_lr: 0.000029  l1_loss:0.009765 flow_loss:0.021345 ter_loss:0.006918 loss_sum:0.038028 0.282719s/batch\n",
            "INFO:root:epoch:265 step:0000  current_lr: 0.000027  l1_loss:0.009387 flow_loss:0.025402 ter_loss:0.006094 loss_sum:0.040883 0.283199s/batch\n",
            "2022-11-20 13:30:50,374 [INFO ]  epoch:265 step:0000  current_lr: 0.000027  l1_loss:0.009387 flow_loss:0.025402 ter_loss:0.006094 loss_sum:0.040883 0.283199s/batch\n",
            "INFO:root:epoch:266 step:0000  current_lr: 0.000026  l1_loss:0.010317 flow_loss:0.022288 ter_loss:0.007426 loss_sum:0.040031 0.280042s/batch\n",
            "2022-11-20 13:31:03,170 [INFO ]  epoch:266 step:0000  current_lr: 0.000026  l1_loss:0.010317 flow_loss:0.022288 ter_loss:0.007426 loss_sum:0.040031 0.280042s/batch\n",
            "INFO:root:epoch:268 step:0000  current_lr: 0.000023  l1_loss:0.010181 flow_loss:0.026969 ter_loss:0.007112 loss_sum:0.044263 0.284887s/batch\n",
            "2022-11-20 13:31:28,923 [INFO ]  epoch:268 step:0000  current_lr: 0.000023  l1_loss:0.010181 flow_loss:0.026969 ter_loss:0.007112 loss_sum:0.044263 0.284887s/batch\n",
            "INFO:root:epoch:269 step:0000  current_lr: 0.000022  l1_loss:0.010455 flow_loss:0.021647 ter_loss:0.007446 loss_sum:0.039548 0.280932s/batch\n",
            "2022-11-20 13:31:41,730 [INFO ]  epoch:269 step:0000  current_lr: 0.000022  l1_loss:0.010455 flow_loss:0.021647 ter_loss:0.007446 loss_sum:0.039548 0.280932s/batch\n",
            "INFO:root:epoch:270 step:0000  current_lr: 0.000021  l1_loss:0.010016 flow_loss:0.025826 ter_loss:0.006875 loss_sum:0.042717 0.276524s/batch\n",
            "2022-11-20 13:31:54,527 [INFO ]  epoch:270 step:0000  current_lr: 0.000021  l1_loss:0.010016 flow_loss:0.025826 ter_loss:0.006875 loss_sum:0.042717 0.276524s/batch\n",
            "INFO:root:epoch:271 step:0000  current_lr: 0.000019  l1_loss:0.010262 flow_loss:0.021323 ter_loss:0.007085 loss_sum:0.038671 0.284956s/batch\n",
            "2022-11-20 13:32:07,397 [INFO ]  epoch:271 step:0000  current_lr: 0.000019  l1_loss:0.010262 flow_loss:0.021323 ter_loss:0.007085 loss_sum:0.038671 0.284956s/batch\n",
            "INFO:root:epoch:272 step:0000  current_lr: 0.000018  l1_loss:0.009927 flow_loss:0.027792 ter_loss:0.006783 loss_sum:0.044502 0.282796s/batch\n",
            "2022-11-20 13:32:20,211 [INFO ]  epoch:272 step:0000  current_lr: 0.000018  l1_loss:0.009927 flow_loss:0.027792 ter_loss:0.006783 loss_sum:0.044502 0.282796s/batch\n",
            "INFO:root:epoch:273 step:0000  current_lr: 0.000017  l1_loss:0.011426 flow_loss:0.031965 ter_loss:0.008053 loss_sum:0.051445 0.280959s/batch\n",
            "2022-11-20 13:32:33,016 [INFO ]  epoch:273 step:0000  current_lr: 0.000017  l1_loss:0.011426 flow_loss:0.031965 ter_loss:0.008053 loss_sum:0.051445 0.280959s/batch\n",
            "INFO:root:epoch:274 step:0000  current_lr: 0.000016  l1_loss:0.011106 flow_loss:0.024024 ter_loss:0.007941 loss_sum:0.043071 0.282787s/batch\n",
            "2022-11-20 13:32:45,839 [INFO ]  epoch:274 step:0000  current_lr: 0.000016  l1_loss:0.011106 flow_loss:0.024024 ter_loss:0.007941 loss_sum:0.043071 0.282787s/batch\n",
            "INFO:root:epoch:275 step:0000  current_lr: 0.000015  l1_loss:0.011532 flow_loss:0.023813 ter_loss:0.008336 loss_sum:0.043681 0.285126s/batch\n",
            "2022-11-20 13:32:58,677 [INFO ]  epoch:275 step:0000  current_lr: 0.000015  l1_loss:0.011532 flow_loss:0.023813 ter_loss:0.008336 loss_sum:0.043681 0.285126s/batch\n",
            "INFO:root:epoch:276 step:0000  current_lr: 0.000014  l1_loss:0.009789 flow_loss:0.020215 ter_loss:0.006617 loss_sum:0.036622 0.281688s/batch\n",
            "2022-11-20 13:33:11,491 [INFO ]  epoch:276 step:0000  current_lr: 0.000014  l1_loss:0.009789 flow_loss:0.020215 ter_loss:0.006617 loss_sum:0.036622 0.281688s/batch\n",
            "INFO:root:epoch:277 step:0000  current_lr: 0.000012  l1_loss:0.010907 flow_loss:0.025648 ter_loss:0.007735 loss_sum:0.044290 0.285484s/batch\n",
            "2022-11-20 13:33:24,344 [INFO ]  epoch:277 step:0000  current_lr: 0.000012  l1_loss:0.010907 flow_loss:0.025648 ter_loss:0.007735 loss_sum:0.044290 0.285484s/batch\n",
            "INFO:root:epoch:278 step:0000  current_lr: 0.000011  l1_loss:0.010665 flow_loss:0.027541 ter_loss:0.007117 loss_sum:0.045323 0.283381s/batch\n",
            "2022-11-20 13:33:37,289 [INFO ]  epoch:278 step:0000  current_lr: 0.000011  l1_loss:0.010665 flow_loss:0.027541 ter_loss:0.007117 loss_sum:0.045323 0.283381s/batch\n",
            "INFO:root:epoch:279 step:0000  current_lr: 0.000010  l1_loss:0.010432 flow_loss:0.032486 ter_loss:0.006934 loss_sum:0.049852 0.282450s/batch\n",
            "2022-11-20 13:33:50,108 [INFO ]  epoch:279 step:0000  current_lr: 0.000010  l1_loss:0.010432 flow_loss:0.032486 ter_loss:0.006934 loss_sum:0.049852 0.282450s/batch\n",
            "INFO:root:epoch:280 step:0000  current_lr: 0.000010  l1_loss:0.010449 flow_loss:0.027267 ter_loss:0.007059 loss_sum:0.044776 0.284638s/batch\n",
            "2022-11-20 13:34:02,968 [INFO ]  epoch:280 step:0000  current_lr: 0.000010  l1_loss:0.010449 flow_loss:0.027267 ter_loss:0.007059 loss_sum:0.044776 0.284638s/batch\n",
            "INFO:root:Saving state, epoch: 280 iter:0\n",
            "2022-11-20 13:34:12,968 [INFO ]  Saving state, epoch: 280 iter:0\n",
            "INFO:root:epoch:281 step:0000  current_lr: 0.000009  l1_loss:0.009218 flow_loss:0.024400 ter_loss:0.006033 loss_sum:0.039651 0.283634s/batch\n",
            "2022-11-20 13:34:16,920 [INFO ]  epoch:281 step:0000  current_lr: 0.000009  l1_loss:0.009218 flow_loss:0.024400 ter_loss:0.006033 loss_sum:0.039651 0.283634s/batch\n",
            "INFO:root:epoch:282 step:0000  current_lr: 0.000008  l1_loss:0.010589 flow_loss:0.024761 ter_loss:0.007582 loss_sum:0.042933 0.285061s/batch\n",
            "2022-11-20 13:34:29,762 [INFO ]  epoch:282 step:0000  current_lr: 0.000008  l1_loss:0.010589 flow_loss:0.024761 ter_loss:0.007582 loss_sum:0.042933 0.285061s/batch\n",
            "INFO:root:epoch:283 step:0000  current_lr: 0.000007  l1_loss:0.010758 flow_loss:0.024551 ter_loss:0.007038 loss_sum:0.042346 0.284098s/batch\n",
            "2022-11-20 13:34:42,616 [INFO ]  epoch:283 step:0000  current_lr: 0.000007  l1_loss:0.010758 flow_loss:0.024551 ter_loss:0.007038 loss_sum:0.042346 0.284098s/batch\n",
            "INFO:root:epoch:284 step:0000  current_lr: 0.000006  l1_loss:0.011824 flow_loss:0.027865 ter_loss:0.008885 loss_sum:0.048574 0.282631s/batch\n",
            "2022-11-20 13:34:55,454 [INFO ]  epoch:284 step:0000  current_lr: 0.000006  l1_loss:0.011824 flow_loss:0.027865 ter_loss:0.008885 loss_sum:0.048574 0.282631s/batch\n",
            "INFO:root:epoch:285 step:0000  current_lr: 0.000005  l1_loss:0.009658 flow_loss:0.029991 ter_loss:0.005756 loss_sum:0.045404 0.280697s/batch\n",
            "2022-11-20 13:35:08,276 [INFO ]  epoch:285 step:0000  current_lr: 0.000005  l1_loss:0.009658 flow_loss:0.029991 ter_loss:0.005756 loss_sum:0.045404 0.280697s/batch\n",
            "INFO:root:epoch:286 step:0000  current_lr: 0.000005  l1_loss:0.010876 flow_loss:0.020265 ter_loss:0.008355 loss_sum:0.039496 0.285397s/batch\n",
            "2022-11-20 13:35:21,123 [INFO ]  epoch:286 step:0000  current_lr: 0.000005  l1_loss:0.010876 flow_loss:0.020265 ter_loss:0.008355 loss_sum:0.039496 0.285397s/batch\n",
            "INFO:root:epoch:287 step:0000  current_lr: 0.000004  l1_loss:0.010963 flow_loss:0.022368 ter_loss:0.008365 loss_sum:0.041696 0.283985s/batch\n",
            "2022-11-20 13:35:33,964 [INFO ]  epoch:287 step:0000  current_lr: 0.000004  l1_loss:0.010963 flow_loss:0.022368 ter_loss:0.008365 loss_sum:0.041696 0.283985s/batch\n",
            "INFO:root:epoch:288 step:0000  current_lr: 0.000004  l1_loss:0.010642 flow_loss:0.026766 ter_loss:0.007563 loss_sum:0.044970 0.281119s/batch\n",
            "2022-11-20 13:35:46,780 [INFO ]  epoch:288 step:0000  current_lr: 0.000004  l1_loss:0.010642 flow_loss:0.026766 ter_loss:0.007563 loss_sum:0.044970 0.281119s/batch\n",
            "INFO:root:epoch:289 step:0000  current_lr: 0.000003  l1_loss:0.010989 flow_loss:0.021685 ter_loss:0.007789 loss_sum:0.040463 0.285736s/batch\n",
            "2022-11-20 13:35:59,620 [INFO ]  epoch:289 step:0000  current_lr: 0.000003  l1_loss:0.010989 flow_loss:0.021685 ter_loss:0.007789 loss_sum:0.040463 0.285736s/batch\n",
            "INFO:root:epoch:290 step:0000  current_lr: 0.000002  l1_loss:0.011175 flow_loss:0.029137 ter_loss:0.007599 loss_sum:0.047911 0.285628s/batch\n",
            "2022-11-20 13:36:12,482 [INFO ]  epoch:290 step:0000  current_lr: 0.000002  l1_loss:0.011175 flow_loss:0.029137 ter_loss:0.007599 loss_sum:0.047911 0.285628s/batch\n",
            "INFO:root:epoch:291 step:0000  current_lr: 0.000002  l1_loss:0.011175 flow_loss:0.026539 ter_loss:0.007790 loss_sum:0.045504 0.283150s/batch\n",
            "2022-11-20 13:36:25,306 [INFO ]  epoch:291 step:0000  current_lr: 0.000002  l1_loss:0.011175 flow_loss:0.026539 ter_loss:0.007790 loss_sum:0.045504 0.283150s/batch\n",
            "INFO:root:epoch:292 step:0000  current_lr: 0.000002  l1_loss:0.010582 flow_loss:0.023005 ter_loss:0.007551 loss_sum:0.041138 0.289004s/batch\n",
            "2022-11-20 13:36:38,196 [INFO ]  epoch:292 step:0000  current_lr: 0.000002  l1_loss:0.010582 flow_loss:0.023005 ter_loss:0.007551 loss_sum:0.041138 0.289004s/batch\n",
            "INFO:root:epoch:293 step:0000  current_lr: 0.000001  l1_loss:0.009828 flow_loss:0.028434 ter_loss:0.006157 loss_sum:0.044419 0.284226s/batch\n",
            "2022-11-20 13:36:51,041 [INFO ]  epoch:293 step:0000  current_lr: 0.000001  l1_loss:0.009828 flow_loss:0.028434 ter_loss:0.006157 loss_sum:0.044419 0.284226s/batch\n",
            "INFO:root:epoch:294 step:0000  current_lr: 0.000001  l1_loss:0.009940 flow_loss:0.022854 ter_loss:0.007075 loss_sum:0.039869 0.283925s/batch\n",
            "2022-11-20 13:37:03,881 [INFO ]  epoch:294 step:0000  current_lr: 0.000001  l1_loss:0.009940 flow_loss:0.022854 ter_loss:0.007075 loss_sum:0.039869 0.283925s/batch\n",
            "INFO:root:epoch:295 step:0000  current_lr: 0.000001  l1_loss:0.010846 flow_loss:0.021287 ter_loss:0.008338 loss_sum:0.040472 0.284755s/batch\n",
            "2022-11-20 13:37:16,727 [INFO ]  epoch:295 step:0000  current_lr: 0.000001  l1_loss:0.010846 flow_loss:0.021287 ter_loss:0.008338 loss_sum:0.040472 0.284755s/batch\n",
            "INFO:root:epoch:296 step:0000  current_lr: 0.000000  l1_loss:0.011403 flow_loss:0.025670 ter_loss:0.008424 loss_sum:0.045497 0.285432s/batch\n",
            "2022-11-20 13:37:29,582 [INFO ]  epoch:296 step:0000  current_lr: 0.000000  l1_loss:0.011403 flow_loss:0.025670 ter_loss:0.008424 loss_sum:0.045497 0.285432s/batch\n",
            "INFO:root:epoch:297 step:0000  current_lr: 0.000000  l1_loss:0.010701 flow_loss:0.024712 ter_loss:0.007888 loss_sum:0.043300 0.281634s/batch\n",
            "2022-11-20 13:37:42,426 [INFO ]  epoch:297 step:0000  current_lr: 0.000000  l1_loss:0.010701 flow_loss:0.024712 ter_loss:0.007888 loss_sum:0.043300 0.281634s/batch\n",
            "INFO:root:epoch:298 step:0000  current_lr: 0.000000  l1_loss:0.010226 flow_loss:0.027032 ter_loss:0.007126 loss_sum:0.044384 0.288660s/batch\n",
            "2022-11-20 13:37:55,315 [INFO ]  epoch:298 step:0000  current_lr: 0.000000  l1_loss:0.010226 flow_loss:0.027032 ter_loss:0.007126 loss_sum:0.044384 0.288660s/batch\n",
            "INFO:root:epoch:299 step:0000  current_lr: 0.000000  l1_loss:0.009670 flow_loss:0.026359 ter_loss:0.005928 loss_sum:0.041958 0.285738s/batch\n",
            "2022-11-20 13:38:08,160 [INFO ]  epoch:299 step:0000  current_lr: 0.000000  l1_loss:0.009670 flow_loss:0.026359 ter_loss:0.005928 loss_sum:0.041958 0.285738s/batch\n",
            "INFO:root:The training stage on VimeoDataset is over!!!\n",
            "2022-11-20 13:38:18,700 [INFO ]  The training stage on VimeoDataset is over!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch --nproc_per_node=1 --master_port=4175 train.py --launcher pytorch --gpu_ids 0 \\\n",
        "            --loss_l1 --loss_ter --loss_flow --use_tb_logger --batch_size 24 --net_name VFIformerSmall --name train_VFIformerSmall --max_iter 300 \\\n",
        "            --crop_size 192 --save_epoch_freq 5 --data_root \"/content/drive/My Drive/Duong/datasets/smallest\" \\\n",
        "            --num_workers 2"
      ],
      "metadata": {
        "id": "j4UXMw7nzJ-s",
        "outputId": "664fdbe0-0ff5-45cf-d755-2386d245c1ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Logging file is ./weights/train_VFIformerSmall/20221120_115145.log\n",
            "2022-11-20 11:51:45,654 [INFO ]  Logging file is ./weights/train_VFIformerSmall/20221120_115145.log\n",
            "INFO:root:random_seed:0\n",
            "2022-11-20 11:51:45,654 [INFO ]  random_seed:0\n",
            "INFO:root:name:train_VFIformerSmall\n",
            "2022-11-20 11:51:45,654 [INFO ]  name:train_VFIformerSmall\n",
            "INFO:root:phase:train\n",
            "2022-11-20 11:51:45,655 [INFO ]  phase:train\n",
            "INFO:root:gpu_ids:[0]\n",
            "2022-11-20 11:51:45,655 [INFO ]  gpu_ids:[0]\n",
            "INFO:root:launcher:pytorch\n",
            "2022-11-20 11:51:45,655 [INFO ]  launcher:pytorch\n",
            "INFO:root:local_rank:0\n",
            "2022-11-20 11:51:45,655 [INFO ]  local_rank:0\n",
            "INFO:root:net_name:VFIformerSmall\n",
            "2022-11-20 11:51:45,655 [INFO ]  net_name:VFIformerSmall\n",
            "INFO:root:window_size:8\n",
            "2022-11-20 11:51:45,655 [INFO ]  window_size:8\n",
            "INFO:root:data_root:/content/drive/My Drive/Duong/datasets/smallest\n",
            "2022-11-20 11:51:45,655 [INFO ]  data_root:/content/drive/My Drive/Duong/datasets/smallest\n",
            "INFO:root:trainset:VimeoDataset\n",
            "2022-11-20 11:51:45,655 [INFO ]  trainset:VimeoDataset\n",
            "INFO:root:testset:VimeoDataset\n",
            "2022-11-20 11:51:45,655 [INFO ]  testset:VimeoDataset\n",
            "INFO:root:save_test_root:generated\n",
            "2022-11-20 11:51:45,655 [INFO ]  save_test_root:generated\n",
            "INFO:root:crop_size:192\n",
            "2022-11-20 11:51:45,656 [INFO ]  crop_size:192\n",
            "INFO:root:batch_size:24\n",
            "2022-11-20 11:51:45,656 [INFO ]  batch_size:24\n",
            "INFO:root:num_workers:2\n",
            "2022-11-20 11:51:45,656 [INFO ]  num_workers:2\n",
            "INFO:root:multi_scale:False\n",
            "2022-11-20 11:51:45,656 [INFO ]  multi_scale:False\n",
            "INFO:root:data_augmentation:False\n",
            "2022-11-20 11:51:45,656 [INFO ]  data_augmentation:False\n",
            "INFO:root:lr:0.0001\n",
            "2022-11-20 11:51:45,656 [INFO ]  lr:0.0001\n",
            "INFO:root:lr_D:0.0001\n",
            "2022-11-20 11:51:45,656 [INFO ]  lr_D:0.0001\n",
            "INFO:root:weight_decay:0.0001\n",
            "2022-11-20 11:51:45,656 [INFO ]  weight_decay:0.0001\n",
            "INFO:root:start_iter:0\n",
            "2022-11-20 11:51:45,656 [INFO ]  start_iter:0\n",
            "INFO:root:max_iter:300\n",
            "2022-11-20 11:51:45,656 [INFO ]  max_iter:300\n",
            "INFO:root:loss_l1:True\n",
            "2022-11-20 11:51:45,657 [INFO ]  loss_l1:True\n",
            "INFO:root:loss_ter:True\n",
            "2022-11-20 11:51:45,657 [INFO ]  loss_ter:True\n",
            "INFO:root:loss_flow:True\n",
            "2022-11-20 11:51:45,657 [INFO ]  loss_flow:True\n",
            "INFO:root:loss_perceptual:False\n",
            "2022-11-20 11:51:45,657 [INFO ]  loss_perceptual:False\n",
            "INFO:root:loss_adv:False\n",
            "2022-11-20 11:51:45,657 [INFO ]  loss_adv:False\n",
            "INFO:root:gan_type:WGAN_GP\n",
            "2022-11-20 11:51:45,657 [INFO ]  gan_type:WGAN_GP\n",
            "INFO:root:lambda_l1:1\n",
            "2022-11-20 11:51:45,657 [INFO ]  lambda_l1:1\n",
            "INFO:root:lambda_ter:1\n",
            "2022-11-20 11:51:45,657 [INFO ]  lambda_ter:1\n",
            "INFO:root:lambda_flow:0.01\n",
            "2022-11-20 11:51:45,657 [INFO ]  lambda_flow:0.01\n",
            "INFO:root:lambda_perceptual:1\n",
            "2022-11-20 11:51:45,657 [INFO ]  lambda_perceptual:1\n",
            "INFO:root:lambda_adv:0.005\n",
            "2022-11-20 11:51:45,658 [INFO ]  lambda_adv:0.005\n",
            "INFO:root:resume:\n",
            "2022-11-20 11:51:45,658 [INFO ]  resume:\n",
            "INFO:root:resume_optim:\n",
            "2022-11-20 11:51:45,658 [INFO ]  resume_optim:\n",
            "INFO:root:resume_scheduler:\n",
            "2022-11-20 11:51:45,658 [INFO ]  resume_scheduler:\n",
            "INFO:root:resume_flownet:\n",
            "2022-11-20 11:51:45,658 [INFO ]  resume_flownet:\n",
            "INFO:root:log_freq:10\n",
            "2022-11-20 11:51:45,658 [INFO ]  log_freq:10\n",
            "INFO:root:vis_freq:50000\n",
            "2022-11-20 11:51:45,658 [INFO ]  vis_freq:50000\n",
            "INFO:root:save_epoch_freq:5\n",
            "2022-11-20 11:51:45,658 [INFO ]  save_epoch_freq:5\n",
            "INFO:root:test_freq:100\n",
            "2022-11-20 11:51:45,658 [INFO ]  test_freq:100\n",
            "INFO:root:save_folder:./weights/train_VFIformerSmall\n",
            "2022-11-20 11:51:45,658 [INFO ]  save_folder:./weights/train_VFIformerSmall\n",
            "INFO:root:vis_step_freq:100\n",
            "2022-11-20 11:51:45,659 [INFO ]  vis_step_freq:100\n",
            "INFO:root:use_tb_logger:True\n",
            "2022-11-20 11:51:45,659 [INFO ]  use_tb_logger:True\n",
            "INFO:root:save_test_results:False\n",
            "2022-11-20 11:51:45,659 [INFO ]  save_test_results:False\n",
            "INFO:root:ref_level:1\n",
            "2022-11-20 11:51:45,659 [INFO ]  ref_level:1\n",
            "INFO:root:dist:True\n",
            "2022-11-20 11:51:45,659 [INFO ]  dist:True\n",
            "INFO:root:world_size:1\n",
            "2022-11-20 11:51:45,659 [INFO ]  world_size:1\n",
            "INFO:root:rank:0\n",
            "2022-11-20 11:51:45,659 [INFO ]  rank:0\n",
            "INFO:root:vis_save_dir:./weights/train_VFIformerSmall/vis\n",
            "2022-11-20 11:51:45,659 [INFO ]  vis_save_dir:./weights/train_VFIformerSmall/vis\n",
            "INFO:root:snapshot_save_dir:./weights/train_VFIformerSmall/snapshot\n",
            "2022-11-20 11:51:45,659 [INFO ]  snapshot_save_dir:./weights/train_VFIformerSmall/snapshot\n",
            "INFO:root:----- generator parameters: 17.024426 -----\n",
            "2022-11-20 11:51:49,892 [INFO ]  ----- generator parameters: 17.024426 -----\n",
            "INFO:root:init criterion and optimizer...\n",
            "2022-11-20 11:51:49,892 [INFO ]  init criterion and optimizer...\n",
            "INFO:root:  using l1 loss...\n",
            "2022-11-20 11:51:49,894 [INFO ]    using l1 loss...\n",
            "INFO:root:  using flow loss...\n",
            "2022-11-20 11:51:49,894 [INFO ]    using flow loss...\n",
            "INFO:root:  using ter loss...\n",
            "2022-11-20 11:51:49,894 [INFO ]    using ter loss...\n",
            "INFO:root:training on  ...VimeoDataset\n",
            "2022-11-20 11:51:49,895 [INFO ]  training on  ...VimeoDataset\n",
            "INFO:root:173 training samples\n",
            "2022-11-20 11:51:49,895 [INFO ]  173 training samples\n",
            "INFO:root:the init lr: 0.000100\n",
            "2022-11-20 11:51:49,895 [INFO ]  the init lr: 0.000100\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 152, in <module>\n",
            "    main()\n",
            "  File \"train.py\", line 149, in main\n",
            "    trainer.train()\n",
            "  File \"/content/drive/MyDrive/Duong/VFIformer/code/models/trainer.py\", line 194, in train\n",
            "    output, flow_list = self.net(img0, img1, None)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/parallel/distributed.py\", line 705, in forward\n",
            "    output = self.module(*inputs[0], **kwargs[0])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Duong/VFIformer/code/models/archs/VFIformer_arch.py\", line 542, in forward\n",
            "    refine_output = self.transformer(x, c0, c1)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Duong/VFIformer/code/models/archs/transformer_layers.py\", line 1001, in forward\n",
            "    fea0 = self.forward_features(s0, self.layers0)\n",
            "  File \"/content/drive/MyDrive/Duong/VFIformer/code/models/archs/transformer_layers.py\", line 989, in forward_features\n",
            "    x = layer(x, x_size)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Duong/VFIformer/code/models/archs/transformer_layers.py\", line 656, in forward\n",
            "    return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))) + x\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Duong/VFIformer/code/models/archs/transformer_layers.py\", line 595, in forward\n",
            "    x = blk(x, x_size)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Duong/VFIformer/code/models/archs/transformer_layers.py\", line 449, in forward\n",
            "    attn_windows = self.attn(x_windows, x_windows_down, mask_x=self.attn_mask_x, mask_y=self.attn_mask_y)  # nW*B, window_size*window_size, C\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Duong/VFIformer/code/models/archs/transformer_layers.py\", line 251, in forward\n",
            "    x = (attn @ v).transpose(1, 2).reshape(B_, N, C).contiguous()\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 460.00 MiB (GPU 0; 39.59 GiB total capacity; 37.11 GiB already allocated; 398.19 MiB free; 37.31 GiB reserved in total by PyTorch)\n",
            "Killing subprocess 7152\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 340, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 326, in main\n",
            "    sigkill_handler(signal.SIGTERM, None)  # not coming back\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/distributed/launch.py\", line 301, in sigkill_handler\n",
            "    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)\n",
            "subprocess.CalledProcessError: Command '['/usr/bin/python3', '-u', 'train.py', '--local_rank=0', '--launcher', 'pytorch', '--gpu_ids', '0', '--loss_l1', '--loss_ter', '--loss_flow', '--use_tb_logger', '--batch_size', '24', '--net_name', 'VFIformerSmall', '--name', 'train_VFIformerSmall', '--max_iter', '300', '--crop_size', '192', '--save_epoch_freq', '5', '--data_root', '/content/drive/My Drive/Duong/datasets/smallest', '--num_workers', '2']' returned non-zero exit status 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py --data_root \"/content/drive/My Drive/datasets/vimeo_triplet\" --testset VimeoDataset --net_name VFIformerSmall \\\n",
        "  --resume \"/content/drive/My Drive/VFIformer/weights/train_VFIformerSmall/snapshot/net_105.pth\" \\\n",
        "  --save_result --save_folder \"/content/drive/My Drive/datasets/vimeo_triplet/test_results\""
      ],
      "metadata": {
        "id": "1XN_-G6gHhHk",
        "outputId": "25b5369d-9055-499e-d6ab-651c839e0d64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disabled distributed training.\n",
            "2022-08-15 16:15:12,339 [INFO ]  Logging file is /content/drive/My Drive/datasets/vimeo_triplet/test_results/test_vfiformer/20220815_161512.log\n",
            "2022-08-15 16:15:12,339 [INFO ]  random_seed:0\n",
            "2022-08-15 16:15:12,340 [INFO ]  name:test_vfiformer\n",
            "2022-08-15 16:15:12,340 [INFO ]  phase:test\n",
            "2022-08-15 16:15:12,340 [INFO ]  gpu_ids:[0]\n",
            "2022-08-15 16:15:12,340 [INFO ]  launcher:none\n",
            "2022-08-15 16:15:12,340 [INFO ]  local_rank:0\n",
            "2022-08-15 16:15:12,340 [INFO ]  net_name:VFIformerSmall\n",
            "2022-08-15 16:15:12,340 [INFO ]  data_root:/content/drive/My Drive/datasets/vimeo_triplet\n",
            "2022-08-15 16:15:12,340 [INFO ]  trainset:VimeoDataset\n",
            "2022-08-15 16:15:12,341 [INFO ]  testset:VimeoDataset\n",
            "2022-08-15 16:15:12,341 [INFO ]  crop_size:192\n",
            "2022-08-15 16:15:12,341 [INFO ]  batch_size:1\n",
            "2022-08-15 16:15:12,341 [INFO ]  num_workers:4\n",
            "2022-08-15 16:15:12,341 [INFO ]  data_augmentation:False\n",
            "2022-08-15 16:15:12,341 [INFO ]  resume:/content/drive/My Drive/VFIformer/weights/train_VFIformerSmall/snapshot/net_105.pth\n",
            "2022-08-15 16:15:12,341 [INFO ]  resume_flownet:\n",
            "2022-08-15 16:15:12,341 [INFO ]  save_folder:/content/drive/My Drive/datasets/vimeo_triplet/test_results/test_vfiformer\n",
            "2022-08-15 16:15:12,341 [INFO ]  save_result:True\n",
            "2022-08-15 16:15:12,342 [INFO ]  dist:False\n",
            "2022-08-15 16:15:12,342 [INFO ]  rank:-1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3503: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
            "2022-08-15 16:15:16,511 [INFO ]  ----- generator parameters: 17.024426 -----\n",
            "2022-08-15 16:15:16,517 [INFO ]  start testing...\n",
            "2022-08-15 16:15:16,517 [INFO ]  20 testing samples\n",
            "2022-08-15 16:15:19,690 [INFO ]  testing on: 00001/0001    psnr: 20.373328    ssim: 0.697932\n",
            "2022-08-15 16:15:20,428 [INFO ]  testing on: 00001/0002    psnr: 20.429682    ssim: 0.700806\n",
            "2022-08-15 16:15:21,188 [INFO ]  testing on: 00001/0003    psnr: 20.484634    ssim: 0.704268\n",
            "2022-08-15 16:15:21,905 [INFO ]  testing on: 00001/0004    psnr: 19.075170    ssim: 0.568258\n",
            "2022-08-15 16:15:22,625 [INFO ]  testing on: 00001/0005    psnr: 20.149826    ssim: 0.630112\n",
            "2022-08-15 16:15:23,353 [INFO ]  testing on: 00001/0006    psnr: 20.811068    ssim: 0.661224\n",
            "2022-08-15 16:15:24,072 [INFO ]  testing on: 00001/0007    psnr: 20.052586    ssim: 0.610983\n",
            "2022-08-15 16:15:24,791 [INFO ]  testing on: 00001/0008    psnr: 19.442653    ssim: 0.575973\n",
            "2022-08-15 16:15:25,515 [INFO ]  testing on: 00001/0009    psnr: 19.007357    ssim: 0.548806\n",
            "2022-08-15 16:15:26,262 [INFO ]  testing on: 00001/0010    psnr: 18.629936    ssim: 0.526344\n",
            "2022-08-15 16:15:26,983 [INFO ]  testing on: 00001/0011    psnr: 18.440923    ssim: 0.515653\n",
            "2022-08-15 16:15:27,707 [INFO ]  testing on: 00001/0012    psnr: 18.301578    ssim: 0.507408\n",
            "2022-08-15 16:15:28,423 [INFO ]  testing on: 00001/0013    psnr: 18.276738    ssim: 0.503915\n",
            "2022-08-15 16:15:29,132 [INFO ]  testing on: 00001/0014    psnr: 18.598399    ssim: 0.520811\n",
            "2022-08-15 16:15:29,847 [INFO ]  testing on: 00001/0015    psnr: 19.017759    ssim: 0.541669\n",
            "2022-08-15 16:15:30,581 [INFO ]  testing on: 00001/0016    psnr: 19.466106    ssim: 0.563200\n",
            "2022-08-15 16:15:31,303 [INFO ]  testing on: 00001/0017    psnr: 19.005264    ssim: 0.572232\n",
            "2022-08-15 16:15:32,021 [INFO ]  testing on: 00001/0018    psnr: 18.823476    ssim: 0.557886\n",
            "2022-08-15 16:15:32,733 [INFO ]  testing on: 00001/0019    psnr: 18.918367    ssim: 0.553478\n",
            "2022-08-15 16:15:33,458 [INFO ]  testing on: 00001/0020    psnr: 19.158276    ssim: 0.556573\n",
            "2022-08-15 16:15:33,742 [INFO ]  --------- average PSNR: 19.323156,  SSIM: 0.580877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xX_jMGHVObxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "data_root = '/content/drive/My Drive/datasets/vimeo_triplet' \n",
        "data_list = open(os.path.join(data_root, 'tri_testlist.txt'), 'r')\n",
        "flow_data = []\n",
        "for item in data_list:\n",
        "  name = str(item).strip()\n",
        "  flow = sorted(glob.glob(os.path.join(data_root.replace('vimeo_triplet', ''), 'flows', name, '*.npy')))\n",
        "  print(glob.glob(os.path.join(data_root.replace('vimeo_triplet', ''), 'flows',name, '*.npy')))\n",
        "print(flow_data)\n",
        "\n",
        "# print(glob.glob(f'/content/drive/My Drive/datasets/**/*'))"
      ],
      "metadata": {
        "id": "xA_Y_oDnNbif",
        "outputId": "abcafec7-df2f-4cb3-c477-79254ad35dd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/My Drive/datasets/flows/00001/0001/flo21.npy', '/content/drive/My Drive/datasets/flows/00001/0001/flo23.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0002/flo23.npy', '/content/drive/My Drive/datasets/flows/00001/0002/flo21.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0003/flo21.npy', '/content/drive/My Drive/datasets/flows/00001/0003/flo23.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0004/flo23.npy', '/content/drive/My Drive/datasets/flows/00001/0004/flo21.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0005/flo21.npy', '/content/drive/My Drive/datasets/flows/00001/0005/flo23.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0006/flo23.npy', '/content/drive/My Drive/datasets/flows/00001/0006/flo21.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0007/flo23.npy', '/content/drive/My Drive/datasets/flows/00001/0007/flo21.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0008/flo23.npy', '/content/drive/My Drive/datasets/flows/00001/0008/flo21.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0009/flo21.npy', '/content/drive/My Drive/datasets/flows/00001/0009/flo23.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0010/flo23.npy', '/content/drive/My Drive/datasets/flows/00001/0010/flo21.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0011/flo23.npy', '/content/drive/My Drive/datasets/flows/00001/0011/flo21.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0012/flo23.npy', '/content/drive/My Drive/datasets/flows/00001/0012/flo21.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0013/flo23.npy', '/content/drive/My Drive/datasets/flows/00001/0013/flo21.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0014/flo21.npy', '/content/drive/My Drive/datasets/flows/00001/0014/flo23.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0015/flo21.npy', '/content/drive/My Drive/datasets/flows/00001/0015/flo23.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0016/flo21.npy', '/content/drive/My Drive/datasets/flows/00001/0016/flo23.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0017/flo23.npy', '/content/drive/My Drive/datasets/flows/00001/0017/flo21.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0018/flo23.npy', '/content/drive/My Drive/datasets/flows/00001/0018/flo21.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0019/flo21.npy', '/content/drive/My Drive/datasets/flows/00001/0019/flo23.npy']\n",
            "['/content/drive/My Drive/datasets/flows/00001/0020/flo23.npy', '/content/drive/My Drive/datasets/flows/00001/0020/flo21.npy']\n",
            "[]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "VFIFormer.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}